{"config":{"lang":["en"],"separator":"[\\s\\-,:!=\\[\\]()\"`/]+|\\.(?!\\d)|&[lg]t;|(?!\\b)(?=[A-Z][a-z])","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Traffik","text":""},{"location":"#stop-the-flood-let-the-right-traffic-through","title":"Stop the flood. Let the right traffic through","text":"<p>Traffik is async-first, distributed rate limiting for FastAPI and Starlette, built for correctness under pressure, not just greenfield demos.</p> <p>Get Started Quick Start GitHub</p>"},{"location":"#what-is-traffik","title":"What is Traffik?","text":"<p>Rate limiting is one of those things that looks simple until it isn't. A naive counter in a dictionary works fine locally, then falls apart the moment you add a second server, a Redis timeout, or a burst of 5,000 simultaneous requests.</p> <p>Traffik is built to handle all of that, correctly. It's an async, distributed rate limiter for FastAPI and Starlette that gives you atomic operations, 10+ proven strategies, HTTP and WebSocket support, circuit breakers, and backend failover, all with an API that stays out of your way.</p> <p>Zero ceremony. Maximum protection.</p>"},{"location":"#features-at-a-glance","title":"Features at a Glance","text":"<ul> <li> <p> Fully Async</p> <p>Built for <code>async</code>/<code>await</code> from the ground up. Non-blocking by default, with minimal latency overhead on the hot path.</p> </li> <li> <p> Distributed-First</p> <p>Atomic operations with distributed locks (Redis, Memcached). Correct counts even under high concurrency across multiple app instances.</p> </li> <li> <p> 10+ Strategies</p> <p>Fixed Window, Sliding Window (Log &amp; Counter), Token Bucket, Leaky Bucket, GCRA, Adaptive, Tiered, Priority Queue, and more.</p> </li> <li> <p> HTTP &amp; WebSocket</p> <p>Full-featured throttling for both protocols. Per-connection throttling on HTTP, per-message throttling on WebSocket.</p> </li> <li> <p> Production-Ready</p> <p>Circuit breakers, automatic retries, backend failover. When Redis goes down, your API stays up.</p> </li> <li> <p> Flexible Integration</p> <p>Dependencies, decorators, middleware, or direct calls. Use whatever fits your architecture.</p> </li> <li> <p> Highly Extensible</p> <p>Clean, well-documented APIs for custom backends, strategies, identifiers, and error handlers.</p> </li> <li> <p> Observable</p> <p>Rich error context, strategy statistics, and quota introspection for monitoring and debugging.</p> </li> <li> <p> Performance-Optimized</p> <p>Lock striping, smart short-circuiting, script caching, and a minimal memory footprint.</p> </li> </ul>"},{"location":"#a-quick-look","title":"A Quick Look","text":"<p>Add rate limiting to a FastAPI endpoint in five lines:</p> <pre><code>from fastapi import FastAPI, Depends\nfrom traffik import HTTPThrottle\nfrom traffik.backends.inmemory import InMemoryBackend\n\nbackend = InMemoryBackend(namespace=\"myapp\")  # (1)!\napp = FastAPI(lifespan=backend.lifespan)       # (2)!\n\nthrottle = HTTPThrottle(\"api:items\", rate=\"100/min\")  # (3)!\n\n@app.get(\"/items\", dependencies=[Depends(throttle)])  # (4)!\nasync def list_items():\n    return {\"items\": [\"widget\", \"gizmo\", \"thingamajig\"]}\n</code></pre> <ol> <li>Start with the in-memory backend for local development \u2014 no external services needed.</li> <li>Pass the backend's lifespan to FastAPI so it initializes and cleans up properly.</li> <li>Define your throttle with a unique ID and a human-readable rate string.</li> <li>Attach it as a FastAPI dependency. That's it. Exceeded limits return <code>HTTP 429</code> automatically.</li> </ol> <p>That's it. No middleware to wire up, no decorators to memorize, no configuration files to manage. When you're ready to go distributed, swap <code>InMemoryBackend</code> for <code>RedisBackend</code> and you're done.</p>"},{"location":"#why-traffik","title":"Why Traffik?","text":"<p>There are other rate limiting libraries out there. Here's what makes Traffik different.</p>"},{"location":"#correctness-under-concurrency","title":"Correctness under concurrency","text":"<p>Simple in-process rate limiters use a plain Python dict. That works until two requests hit simultaneously and both read the same counter before either writes back. Traffik uses atomic operations at the backend level, Lua scripts for Redis, distributed locks for Memcached, so your counts are always correct, even at high concurrency.</p>"},{"location":"#async-all-the-way-down","title":"Async all the way down","text":"<p>Traffik is not a sync library with an <code>async</code> wrapper bolted on. Every backend operation, identifier resolution, and error handler is a coroutine. No <code>run_in_executor</code>, no thread pools, no hidden blocking calls.</p>"},{"location":"#websocket-support-that-actually-works","title":"WebSocket support that actually works","text":"<p>Most rate limiters only think about HTTP. Traffik ships a <code>WebSocketThrottle</code> that handles both connection-level throttling (gate the handshake) and message-level throttling (rate-limit individual frames). By default, when a WebSocket client sends too many messages, Traffik sends them a structured JSON notification rather than slamming the connection shut.</p>"},{"location":"#fail-gracefully","title":"Fail gracefully","text":"<p>Backend unavailable? You choose what happens: allow the request through (<code>on_error=\"allow\"</code>), treat it as throttled (<code>on_error=\"throttle\"</code>), raise the error (<code>on_error=\"raise\"</code>), or use a full circuit breaker with automatic failover to a secondary backend. No silent data loss, no surprise outages.</p>"},{"location":"#identifiers-as-a-natively-supported-concept","title":"Identifiers as a natively supported concept","text":"<p>Who counts as \"one client\"? An IP address? A user ID? An API key? A tenant? In Traffik, this is a natively supported concept. Pass any async function as <code>identifier</code> and Traffik uses it everywhere. Returning the special <code>EXEMPTED</code> sentinel from your identifier function bypasses rate limiting entirely for that connection, no special cases needed.</p>"},{"location":"#plays-nicely-with-fastapi","title":"Plays nicely with FastAPI","text":"<p>Throttles are callable objects that implement <code>__call__</code> with a clean, FastAPI-compatible signature. They work as <code>Depends(...)</code> arguments, <code>@throttled(...)</code> decorators, or direct <code>await throttle(request)</code> calls. FastAPI's OpenAPI schema generation never sees the throttle parameters.</p>"},{"location":"#backends-at-a-glance","title":"Backends at a Glance","text":"Backend Best For Distributed Persistence <code>InMemoryBackend</code> Development, testing, single-process No No <code>RedisBackend</code> Production, multi-instance deployments Yes Yes <code>MemcachedBackend</code> High-throughput, cache-friendly workloads Yes Best-effort"},{"location":"#ready-to-go","title":"Ready to go?","text":"<ul> <li> <p> Installation</p> <p>Get Traffik installed and pick your backend.</p> <p>Installation</p> </li> <li> <p> Quick Start</p> <p>From zero to protected endpoint in minutes. With a real production setup at the end.</p> <p>Quick Start</p> </li> <li> <p> Core Concepts</p> <p>Understand rates, backends, strategies, and identifiers before diving into advanced features.</p> <p>Core Concepts</p> </li> </ul>"},{"location":"api-reference/","title":"API Reference","text":"<p>Quick reference for every public class and function in Traffik. Each entry lists the import path, constructor parameters, and key methods. Follow the guide links for full usage examples.</p>"},{"location":"api-reference/#throttles","title":"Throttles","text":""},{"location":"api-reference/#httpthrottle","title":"<code>HTTPThrottle</code>","text":"<p>Rate limiter for HTTP requests. The most common entry point.</p> <pre><code>from traffik import HTTPThrottle\n</code></pre> <p>Guide: Dependencies \u00b7 Decorators \u00b7 Direct Usage</p> Parameter Type Default Description <code>uid</code> <code>str</code> required Unique identifier for this throttle. Used as the registry key and storage namespace. <code>rate</code> <code>str \\| Rate \\| async callable</code> required Rate limit. A string like <code>\"100/min\"</code>, a <code>Rate</code> object, or an async callable <code>(request, context) -&gt; Rate</code>. <code>cost</code> <code>int \\| async callable</code> <code>1</code> Quota consumed per hit. An integer or an async callable <code>(request, context) -&gt; int</code>. <code>backend</code> <code>ThrottleBackend</code> <code>None</code> Storage backend. Falls back to the backend set on the active context if not specified. <code>identifier</code> <code>async callable</code> <code>None</code> Async callable <code>(request) -&gt; str</code> that returns the throttle key. Defaults to the backend's identifier. Return <code>EXEMPTED</code> to skip throttling. <code>strategy</code> <code>ThrottleStrategy</code> <code>None</code> Throttling algorithm. Defaults to the backend's default strategy. <code>handle_throttled</code> <code>async callable</code> <code>None</code> Custom handler called when the client is throttled. Receives <code>(request, wait_ms, throttle, context)</code>. <code>headers</code> <code>Headers \\| dict</code> <code>None</code> Response headers to resolve on each hit. See Response Headers. <code>on_error</code> <code>\"allow\" \\| \"throttle\" \\| \"raise\" \\| callable</code> <code>None</code> Behaviour when the backend raises. <code>\"allow\"</code> passes the request through; <code>\"throttle\"</code> rejects it; <code>\"raise\"</code> re-raises. <code>registry</code> <code>ThrottleRegistry</code> <code>GLOBAL_REGISTRY</code> Registry to register this throttle in. <code>rules</code> <code>Iterable[ThrottleRule]</code> <code>None</code> Gating rules. All must pass for the throttle to fire. <code>context</code> <code>dict</code> <code>None</code> Static context dict passed to rate/cost callables and handlers. <code>dynamic_backend</code> <code>bool</code> <code>False</code> Resolve the backend from request context on each hit instead of using a fixed instance. <code>min_wait_period</code> <code>int</code> <code>None</code> Minimum wait floor in milliseconds for throttled responses. <code>cache_ids</code> <code>bool</code> <code>True</code> Cache connection identifiers within a single request for performance. <code>dynamic_rules</code> <code>bool</code> <code>False</code> Re-evaluate registry rules on every hit instead of caching them at startup. <p>Key methods:</p> Method Description <code>await hit(request, cost=None, context=None)</code> Check and consume quota. Returns the request on success; raises <code>ConnectionThrottled</code> when over limit. <code>await check(request, cost=None, context=None)</code> Check quota without consuming it. Returns <code>True</code> if the request would pass. <code>await stat(request, cost=None, context=None)</code> Return a <code>StrategyStat</code> snapshot for the current connection. <code>await get_headers(request, headers=None)</code> Resolve rate limit headers for the current connection. <code>await disable()</code> Disable this throttle. Subsequent <code>hit()</code> calls return immediately. <code>await enable()</code> Re-enable a disabled throttle. <code>is_disabled</code> <code>True</code> if the throttle is currently disabled. <code>await update_rate(rate)</code> Swap the rate limit atomically. <code>await update_backend(backend)</code> Swap the backend atomically. <code>await update_strategy(strategy)</code> Swap the strategy atomically. <code>await update_cost(cost)</code> Update the cost atomically. <code>await update_identifier(fn)</code> Swap the identifier function atomically. <code>await update_headers(headers)</code> Replace the header collection atomically. <code>await update_min_wait_period(ms)</code> Update the minimum wait floor atomically. <code>await update_handle_throttled(handler)</code> Swap the throttled-response handler atomically."},{"location":"api-reference/#websocketthrottle","title":"<code>WebSocketThrottle</code>","text":"<p>Rate limiter for WebSocket connections.</p> <pre><code>from traffik import WebSocketThrottle\n</code></pre> <p>Guide: Dependencies \u00b7 Direct Usage</p> <p>Same parameters as <code>HTTPThrottle</code>, but scoped to <code>WebSocket</code> connections. Use inside a WebSocket handler to throttle per-message.</p>"},{"location":"api-reference/#middlewarethrottle","title":"<code>MiddlewareThrottle</code>","text":"<p>Throttle applied at the ASGI middleware layer before route handlers execute.</p> <pre><code>from traffik.throttles import MiddlewareThrottle\n</code></pre> <p>Guide: Middleware</p> Parameter Type Default Description <code>throttle</code> <code>HTTPThrottle</code> required The underlying <code>HTTPThrottle</code> to wrap. <code>cost</code> <code>int \\| async callable \\| None</code> <code>None</code> Override cost for middleware context. <code>None</code> uses the wrapped throttle's cost. <code>predicate</code> <code>async callable \\| None</code> <code>None</code> Async callable <code>(request) -&gt; bool</code>. If provided, the throttle only applies when it returns <code>True</code>."},{"location":"api-reference/#backends","title":"Backends","text":""},{"location":"api-reference/#inmemorybackend","title":"<code>InMemoryBackend</code>","text":"<p>In-process storage backend. No external dependencies. Not shared across processes.</p> <pre><code>from traffik.backends.inmemory import InMemoryBackend\n</code></pre> <p>Guide: Backends</p> Parameter Type Default Description <code>namespace</code> <code>str</code> <code>\"inmemory\"</code> Key prefix for storage isolation. <code>persistent</code> <code>bool</code> <code>False</code> Keep data in memory across <code>lifespan</code> restarts within the same process. <code>on_error</code> <code>\"allow\" \\| \"throttle\" \\| \"raise\" \\| callable</code> <code>\"throttle\"</code> Default error handling for throttles using this backend. <code>identifier</code> <code>async callable</code> <code>None</code> Default identifier for throttles that don't specify one. <code>handle_throttled</code> <code>async callable</code> <code>None</code> Default throttled handler for throttles that don't specify one. <code>number_of_shards</code> <code>int</code> <code>3</code> Number of internal storage shards. Higher values reduce lock contention under concurrent load. <code>cleanup_frequency</code> <code>float \\| None</code> <code>10.0</code> Seconds between expired-key cleanup sweeps. <code>None</code> disables automatic cleanup. <code>lock_kind</code> <code>\"fair\" \\| \"unfair\"</code> <code>\"unfair\"</code> Lock fairness. <code>\"fair\"</code> gives waiting coroutines FIFO ordering; <code>\"unfair\"</code> is faster. <code>lock_blocking</code> <code>bool \\| None</code> <code>None</code> Override default lock blocking. Falls back to <code>TRAFFIK_DEFAULT_BLOCKING</code> env var. <code>lock_ttl</code> <code>float \\| None</code> <code>None</code> Override default lock TTL in seconds. <code>lock_blocking_timeout</code> <code>float \\| None</code> <code>None</code> Override default lock timeout. <p>Integration with FastAPI:</p> <pre><code>backend = InMemoryBackend()\napp = FastAPI(lifespan=backend.lifespan)\n</code></pre>"},{"location":"api-reference/#redisbackend","title":"<code>RedisBackend</code>","text":"<p>Distributed backend backed by Redis. Shares counters across all app instances.</p> <pre><code>from traffik.backends.redis import RedisBackend\n</code></pre> <p>Guide: Backends</p> Parameter Type Default Description <code>url</code> <code>str</code> required Redis connection URL, e.g. <code>\"redis://localhost:6379/0\"</code>. <code>namespace</code> <code>str</code> <code>\"redis\"</code> Key prefix for storage isolation. <code>persistent</code> <code>bool</code> <code>False</code> Keep data across restarts. <code>on_error</code> <code>\"allow\" \\| \"throttle\" \\| \"raise\" \\| callable</code> <code>\"throttle\"</code> Default error handling. <code>identifier</code> <code>async callable</code> <code>None</code> Default identifier function. <code>handle_throttled</code> <code>async callable</code> <code>None</code> Default throttled handler. <code>lock_blocking</code> <code>bool \\| None</code> <code>None</code> Lock blocking setting. <code>lock_ttl</code> <code>float \\| None</code> <code>None</code> Lock TTL in seconds. <code>lock_blocking_timeout</code> <code>float \\| None</code> <code>None</code> Lock timeout in seconds."},{"location":"api-reference/#memcachedbackend","title":"<code>MemcachedBackend</code>","text":"<p>Distributed backend backed by Memcached.</p> <pre><code>from traffik.backends.memcached import MemcachedBackend\n</code></pre> <p>Guide: Backends</p> Parameter Type Default Description <code>servers</code> <code>list[tuple[str, int]]</code> required List of <code>(host, port)</code> tuples. <code>namespace</code> <code>str</code> <code>\"memcached\"</code> Key prefix for storage isolation. <code>persistent</code> <code>bool</code> <code>False</code> Keep data across restarts. <code>on_error</code> <code>\"allow\" \\| \"throttle\" \\| \"raise\" \\| callable</code> <code>\"throttle\"</code> Default error handling."},{"location":"api-reference/#rate","title":"Rate","text":""},{"location":"api-reference/#rate_1","title":"<code>Rate</code>","text":"<p>Immutable rate limit definition. Holds the request limit and window duration.</p> <pre><code>from traffik import Rate\n# or\nfrom traffik.rates import Rate\n</code></pre> <p>Guide: Rate Format</p> Parameter Type Default Description <code>limit</code> <code>int</code> <code>0</code> Maximum requests allowed in the window. <code>0</code> with no duration means unlimited. <code>milliseconds</code> <code>int</code> <code>0</code> Window duration in milliseconds. <code>seconds</code> <code>int</code> <code>0</code> Window duration in seconds. <code>minutes</code> <code>int</code> <code>0</code> Window duration in minutes. <code>hours</code> <code>int</code> <code>0</code> Window duration in hours. <p>Key attributes: <code>limit</code>, <code>expire</code> (total ms), <code>rps</code>, <code>rpm</code>, <code>rph</code>, <code>rpd</code>, <code>is_subsecond</code>, <code>unlimited</code></p> <p>Parse a rate string:</p> <pre><code>rate = Rate.parse(\"100/min\")   # 100 per minute\nrate = Rate.parse(\"10/5s\")     # 10 per 5 seconds\nrate = Rate.parse(\"5 per hour\")\n</code></pre> <p>Supported units: <code>ms</code>, <code>s</code> / <code>sec</code> / <code>second</code>, <code>m</code> / <code>min</code> / <code>minute</code>, <code>h</code> / <code>hr</code> / <code>hour</code>, <code>d</code> / <code>day</code>.</p>"},{"location":"api-reference/#registry","title":"Registry","text":""},{"location":"api-reference/#throttleregistry","title":"<code>ThrottleRegistry</code>","text":"<p>Manages throttle registration, rule attachment, and group disable/enable.</p> <pre><code>from traffik.registry import ThrottleRegistry\n</code></pre> <p>Guide: Throttle Registry</p> <p>Throttles register themselves automatically on construction. You rarely need to create a registry manually \u2014 the <code>GLOBAL_REGISTRY</code> is used by default.</p> Method Description <code>exist(uid)</code> Check if a UID is registered. <code>add_rules(uid, *rules)</code> Attach gating rules to a throttle. Raises <code>ConfigurationError</code> if UID not registered. <code>get_rules(uid)</code> Return all rules attached to a throttle. <code>get_throttle(uid)</code> Return the live throttle instance, or <code>None</code> if garbage-collected. <code>await disable(uid)</code> Disable the throttle. Returns <code>True</code> if found. <code>await enable(uid)</code> Re-enable the throttle. Returns <code>True</code> if found. <code>await disable_all()</code> Disable every live throttle in this registry. <code>await enable_all()</code> Re-enable every live throttle in this registry. <code>clear()</code> Unregister everything and wipe all rules and refs."},{"location":"api-reference/#global_registry","title":"<code>GLOBAL_REGISTRY</code>","text":"<p>The default <code>ThrottleRegistry</code> used when no registry is specified.</p> <pre><code>from traffik.registry import GLOBAL_REGISTRY\n</code></pre>"},{"location":"api-reference/#rules","title":"Rules","text":""},{"location":"api-reference/#throttlerule","title":"<code>ThrottleRule</code>","text":"<p>A gating rule: the throttle only fires when all attached rules pass.</p> <pre><code>from traffik.registry import ThrottleRule\n</code></pre> <p>Guide: Throttle Rules &amp; Wildcards</p> Parameter Type Default Description <code>path</code> <code>str \\| Pattern \\| None</code> <code>None</code> Path pattern to match. Supports <code>*</code> (single segment) and <code>**</code> (any segments) glob syntax, or a compiled regex. <code>None</code> matches all paths. <code>methods</code> <code>Iterable[str] \\| None</code> <code>None</code> HTTP methods to match (e.g. <code>{\"POST\", \"PUT\"}</code>). <code>None</code> matches all methods. <code>predicate</code> <code>async callable \\| None</code> <code>None</code> Async callable <code>(connection, [context]) -&gt; bool</code>. Throttle fires when it returns <code>True</code>. <p>All specified conditions are combined with AND: path AND method AND predicate must all match.</p>"},{"location":"api-reference/#bypassthrottlerule","title":"<code>BypassThrottleRule</code>","text":"<p>Skips throttling when the rule matches (inverse logic compared to <code>ThrottleRule</code>).</p> <pre><code>from traffik.registry import BypassThrottleRule\n</code></pre> <p>Guide: Throttle Rules &amp; Wildcards</p> <p>Same parameters as <code>ThrottleRule</code>. When all conditions match, the throttle is bypassed for that request.</p>"},{"location":"api-reference/#headers","title":"Headers","text":""},{"location":"api-reference/#header","title":"<code>Header</code>","text":"<p>A single rate limit response header with a name, value resolver, and inclusion condition.</p> <pre><code>from traffik.headers import Header\n</code></pre> <p>Guide: Response Headers</p> Parameter Type Default Description <code>v</code> <code>str \\| callable</code> required Static string value or a resolver <code>(connection, stat, context) -&gt; str</code>. <code>when</code> <code>\"always\" \\| \"throttled\" \\| callable</code> <code>\"throttled\"</code> Inclusion condition. <code>\"always\"</code> includes on every response. <code>\"throttled\"</code> only on 429s. A callable <code>(connection, stat, context) -&gt; bool</code> for custom logic. <p>Built-in builders (all require <code>when=</code>):</p> Builder Header value <code>Header.LIMIT(when=...)</code> <code>X-RateLimit-Limit</code>: rate window limit <code>Header.REMAINING(when=...)</code> <code>X-RateLimit-Remaining</code>: hits left this window <code>Header.RESET_SECONDS(when=...)</code> <code>Retry-After</code>: seconds until window resets <code>Header.RESET_MILLISECONDS(when=...)</code> <code>X-RateLimit-Reset-Ms</code>: milliseconds until reset <p>Sentinels:</p> <ul> <li><code>Header.DISABLE</code> \u2014 Pass as a value in <code>get_headers()</code> overrides to suppress a specific header for that call.</li> </ul>"},{"location":"api-reference/#headers_1","title":"<code>Headers</code>","text":"<p>A collection of rate limit response headers resolved per request.</p> <pre><code>from traffik.headers import Headers\n</code></pre> <pre><code>from traffik.headers import Headers, Header\n\nmy_headers = Headers({\n    \"X-RateLimit-Limit\": Header.LIMIT(when=\"always\"),\n    \"X-RateLimit-Remaining\": Header.REMAINING(when=\"always\"),\n    \"Retry-After\": Header.RESET_SECONDS(when=\"throttled\"),\n})\n</code></pre> <p>Supports <code>|</code> for non-mutating merge and <code>.copy()</code> for shallow duplication.</p>"},{"location":"api-reference/#default_headers_always","title":"<code>DEFAULT_HEADERS_ALWAYS</code>","text":"<p>Preset <code>Headers</code> collection: <code>X-RateLimit-Limit</code>, <code>X-RateLimit-Remaining</code>, and <code>Retry-After</code> resolved on every response.</p> <pre><code>from traffik.headers import DEFAULT_HEADERS_ALWAYS\n</code></pre>"},{"location":"api-reference/#default_headers_throttled","title":"<code>DEFAULT_HEADERS_THROTTLED</code>","text":"<p>Preset <code>Headers</code> collection: same three headers resolved only on throttled (<code>429</code>) responses.</p> <pre><code>from traffik.headers import DEFAULT_HEADERS_THROTTLED\n</code></pre>"},{"location":"api-reference/#types-and-sentinels","title":"Types and Sentinels","text":""},{"location":"api-reference/#exempted","title":"<code>EXEMPTED</code>","text":"<p>Sentinel returned from an identifier function to skip throttling for a connection entirely. No quota is consumed and no backend call is made.</p> <pre><code>from traffik import EXEMPTED\n</code></pre> <pre><code>async def my_identifier(request):\n    if request.headers.get(\"x-admin-key\") == ADMIN_KEY:\n        return EXEMPTED  # skip throttling\n    return request.client.host\n</code></pre>"},{"location":"api-reference/#strategystat","title":"<code>StrategyStat</code>","text":"<p>Immutable statistics snapshot returned by <code>throttle.stat()</code>.</p> <pre><code>from traffik.types import StrategyStat\n</code></pre> Attribute Type Description <code>key</code> <code>Stringable</code> The throttle key for this connection. <code>rate</code> <code>Rate</code> The active rate limit. <code>hits_remaining</code> <code>float</code> Hits left in the current window. <code>wait_ms</code> <code>float</code> Milliseconds until the window resets (or until a slot is available). <code>metadata</code> <code>Mapping \\| None</code> Additional strategy-specific data."},{"location":"api-reference/#utilities","title":"Utilities","text":""},{"location":"api-reference/#get_remote_address","title":"<code>get_remote_address</code>","text":"<p>Default connection identifier: returns the client IP address as a string.</p> <pre><code>from traffik import get_remote_address\n</code></pre> <pre><code>throttle = HTTPThrottle(\"api:v1\", rate=\"100/min\", identifier=get_remote_address)\n</code></pre>"},{"location":"api-reference/#configuration","title":"Configuration","text":"<p>Global lock defaults, settable via environment variables or these functions. Throttle-level <code>lock_blocking</code>, <code>lock_ttl</code>, and <code>lock_blocking_timeout</code> parameters override these globals.</p> <pre><code>from traffik.config import (\n    get_lock_ttl, set_lock_ttl,\n    get_lock_blocking, set_lock_blocking,\n    get_lock_blocking_timeout, set_lock_blocking_timeout,\n)\n</code></pre> Function Env var Default Description <code>get_lock_ttl()</code> / <code>set_lock_ttl(v)</code> <code>TRAFFIK_DEFAULT_LOCK_TTL</code> <code>None</code> Lock TTL in seconds. <code>None</code> means no timeout. <code>get_lock_blocking()</code> / <code>set_lock_blocking(v)</code> <code>TRAFFIK_DEFAULT_BLOCKING</code> <code>True</code> Whether to block when acquiring a lock. <code>get_lock_blocking_timeout()</code> / <code>set_lock_blocking_timeout(v)</code> <code>TRAFFIK_DEFAULT_BLOCKING_TIMEOUT</code> <code>None</code> Max seconds to wait for a lock. <code>None</code> means wait indefinitely."},{"location":"benchmarks/","title":"Benchmarks","text":"<p>Numbers, glorious numbers. This page documents benchmark results for Traffik across a wide range of scenarios \u2014 HTTP dependencies, middleware, WebSocket, and the overhead of specific features like response headers and throttle rules.</p> <p>The headline: Traffik is occasionally slower in raw throughput (by design), but it wins decisively where it counts: correctness under concurrency.</p> <p>Run them yourself</p> <p>All benchmark code lives in the <code>benchmarks/</code> directory. Every table and chart here was produced by running those scripts. Numbers will differ on your hardware \u2014 run your own suite to get figures that reflect your setup.</p>"},{"location":"benchmarks/#test-environment","title":"Test Environment","text":"<p>Benchmarks were run on:</p> <ul> <li>Machine: 8-core CPU, 16 GB RAM</li> <li>Python: 3.9.22</li> <li>Backend versions: Redis v6.2.0, aiomcache v0.8.2</li> <li>Comparison: SlowAPI \u2014 a popular FastAPI rate limiter</li> <li>Test client: <code>httpx.AsyncClient</code> with <code>ASGITransport</code> (in-process, no real network)</li> <li>Concurrency: batches of 50 concurrent requests unless noted otherwise</li> </ul>"},{"location":"benchmarks/#http-dependency-mode","title":"HTTP Dependency Mode","text":"<p>Throttle applied via <code>Depends(throttle)</code> on individual endpoints \u2014 the most common integration pattern.</p>"},{"location":"benchmarks/#throughput-reqs-higher-is-better","title":"Throughput (req/s) \u2014 Higher is better","text":"<pre><code>%%{init: {\"theme\": \"base\", \"themeVariables\": {\"xyChart\": {\"plotColorPalette\": \"#ff5722,#78909c\"}}}}%%\nxychart-beta\n    title \"HTTP Throughput by Backend (50 concurrent, FixedWindow)\"\n    x-axis [\"InMemory\", \"Redis\", \"Memcached\"]\n    y-axis \"req/s\" 0 --&gt; 3000\n    line [2740, 1860, 1810]\n    line [2120, 1530, 1490]</code></pre> <p>First line = Traffik. Second line = SlowAPI.</p> Backend Traffik (req/s) SlowAPI (req/s) Difference InMemory 2,740 2,120 +29% Redis 1,860 1,530 +22% Memcached 1,810 1,490 +21%"},{"location":"benchmarks/#latency-percentiles-lower-is-better","title":"Latency Percentiles \u2014 Lower is better","text":""},{"location":"benchmarks/#inmemory-50-concurrent-clients","title":"InMemory (50 concurrent clients)","text":"Percentile Traffik SlowAPI P50 0.37ms 0.46ms P95 1.1ms 2.0ms P99 3.2ms 7.4ms"},{"location":"benchmarks/#redis-50-concurrent-clients","title":"Redis (50 concurrent clients)","text":"Percentile Traffik SlowAPI P50 0.59ms 0.72ms P95 2.0ms 4.1ms P99 5.6ms 11.9ms <p>The P99 gap is the important number. Under sustained load, Traffik's atomic operations reduce tail latency substantially because there's no retry-on-conflict \u2014 the lock serialises, computes, and returns.</p>"},{"location":"benchmarks/#middleware-mode","title":"Middleware Mode","text":"<p>Throttle applied via <code>ThrottleMiddleware</code> with <code>MiddlewareThrottle</code> entries \u2014 the pattern used when you want to rate-limit without modifying route handlers.</p>"},{"location":"benchmarks/#throughput-reqs","title":"Throughput (req/s)","text":"Scenario Traffik SlowAPI Low load (50 req, within limit) 2,650 2,090 High load (200 req, over limit) 2,580 2,020 Sustained load (500 req, 50 concurrent) 2,710 2,100 Burst (100 req, 2\u00d7 limit) 2,620 2,050 <p>Middleware overhead is nearly identical to dependency mode. The extra layer of <code>ThrottleMiddleware</code> adds less than 0.1ms per request \u2014 it's all the same code path underneath.</p>"},{"location":"benchmarks/#selective-throttling","title":"Selective throttling","text":"<p>One benefit of middleware: you can exempt entire paths from throttle evaluation at zero cost. Requests to <code>/unthrottled</code> paths in the benchmark received <code>200 OK</code> with no throttle overhead at all:</p> Path Traffik SlowAPI <code>/test</code> (throttled) 100% correct 100% correct <code>/health</code> (unthrottled) 0ms overhead ~0.1ms overhead <p>Middleware path patterns</p> <p><code>MiddlewareThrottle</code> uses <code>ThrottleRule</code> underneath, so its <code>path</code> argument supports the same wildcard patterns: <code>*</code> for a single segment, <code>**</code> for multiple. See Throttle Rules &amp; Wildcards for details.</p>"},{"location":"benchmarks/#correctness-under-concurrency","title":"Correctness Under Concurrency","text":"<p>This is the table that matters most. A rate limiter that lets through double the allowed traffic isn't a rate limiter \u2014 it's a polite suggestion.</p> <p>Each test sends 150 fully concurrent requests against a limit of 100. The expected outcome: exactly 100 allowed, 50 blocked.</p> <pre><code>%%{init: {\"theme\": \"base\", \"themeVariables\": {\"xyChart\": {\"plotColorPalette\": \"#ff5722,#78909c\"}}}}%%\nxychart-beta\n    title \"Correctness: Allowed requests (limit=100, 150 concurrent)\"\n    x-axis [\"InMemory\", \"Redis\", \"Memcached\"]\n    y-axis \"Requests allowed\" 0 --&gt; 160\n    line [100, 100, 100]\n    line [99, 150, 63]</code></pre> <p>First line = Traffik (target: 100). Second line = SlowAPI.</p> Backend Traffik allowed SlowAPI allowed Expected InMemory 100 99 100 Redis 100 150 100 Memcached 100 63\u201397 100 <p>SlowAPI Redis race conditions</p> <p>When 150 requests arrive simultaneously, SlowAPI's Redis path uses a non-atomic read-increment pattern. All 150 requests read the same counter value (0) before any of them write, so all 150 are allowed. Traffik uses atomic Lua scripts and distributed locks, achieving perfect accuracy regardless of concurrency.</p>"},{"location":"benchmarks/#sustained-load-accuracy","title":"Sustained load accuracy","text":"<p>Over a 60-second run at high concurrency, Traffik's throughput is slightly lower than SlowAPI's when using Redis. The reason: Traffik is acquiring locks to enforce correctness; SlowAPI is skipping them.</p> Duration Backend Traffik SlowAPI Traffik accuracy SlowAPI accuracy 60s sustained Redis 1,860 req/s 2,340 req/s 99.97% 61% 60s sustained InMemory 2,740 req/s 2,870 req/s 99.99% 98% <p>SlowAPI processes more requests per second because it's allowing requests it shouldn't. That's not \"faster\" \u2014 it's wrong faster. Lock serialization is an intentional tradeoff: a small throughput cost for dramatically better accuracy under distributed load.</p>"},{"location":"benchmarks/#websocket-benchmarks","title":"WebSocket Benchmarks","text":"<p>WebSocket rate limiting has a different performance profile. Connections are long-lived; messages arrive in bursts. Traffik's per-message throttle check is extremely lightweight.</p>"},{"location":"benchmarks/#sustained-message-throughput","title":"Sustained message throughput","text":"Scenario Messages/s P50 latency P95 latency P99 latency Low load (50 msg, within limit) 18,200 0.05ms 0.19ms 0.44ms High load (200 msg, over limit) 17,800 0.05ms 0.22ms 0.51ms Sustained (500 msg, 1000/min limit) 18,100 0.05ms 0.21ms 0.48ms Burst (100 msg, 50/min limit) 17,600 0.06ms 0.24ms 0.55ms 10 concurrent connections 16,900 0.07ms 0.31ms 0.82ms <p>WebSocket throttle checks are sub-millisecond at P99. The default throttled handler (which sends a JSON <code>rate_limit</code> message back to the client and keeps the connection alive) is faster than raising an exception, because exception propagation carries Python interpreter overhead. See Custom Throttled Handlers for the send-message pattern.</p>"},{"location":"benchmarks/#strategy-comparison","title":"Strategy Comparison","text":"<p>Different strategies have different CPU and memory profiles. All figures are InMemory backend, 50 concurrent clients, 500 requests.</p> <pre><code>%%{init: {\"theme\": \"base\", \"themeVariables\": {\"xyChart\": {\"plotColorPalette\": \"#ff5722\"}}, \"xychart\": {\"chartOrientation\": \"horizontal\"}}}%%\nxychart-beta\n    title \"Strategy Throughput (InMemory, 50 concurrent, req/s)\"\n    x-axis [\"FixedWindow\", \"SlidingCounter\", \"SlidingLog\", \"TokenBucket\", \"TokenBucketDebt\", \"LeakyBucket\", \"GCRA\"]\n    y-axis \"req/s\" 0 --&gt; 3000\n    bar [2740, 2590, 2280, 2670, 2620, 2580, 2710]</code></pre> Strategy req/s P50 P95 P99 Correctness FixedWindow 2,740 0.37ms 1.1ms 3.2ms ~100% SlidingWindowCounter 2,590 0.39ms 1.2ms 3.5ms ~100% SlidingWindowLog 2,280 0.44ms 1.4ms 4.1ms 100% TokenBucket 2,670 0.38ms 1.1ms 3.3ms ~100% TokenBucketWithDebt 2,620 0.38ms 1.2ms 3.4ms ~100% LeakyBucket 2,580 0.39ms 1.2ms 3.4ms ~100% GCRA 2,710 0.37ms 1.1ms 3.2ms ~100% <p><code>SlidingWindowLog</code> is the most accurate (100% \u2014 it stores every request timestamp), but it's also the most memory-hungry and the slowest due to the log scan. <code>SlidingWindowCounter</code> hits ~100% correctness in practice with much lower overhead by using a weighted counter approximation instead of a full log.</p>"},{"location":"benchmarks/#feature-overhead-benchmarks","title":"Feature Overhead Benchmarks","text":"<p>These benchmarks isolate the cost of specific Traffik features on top of a baseline (no-headers, no-rules, InMemory, FixedWindow).</p>"},{"location":"benchmarks/#response-headers-overhead","title":"Response headers overhead","text":"<p>Adding response headers has a measurable but small cost, primarily from calling resolver functions and building the response header dict.</p> Configuration req/s vs. baseline P50 P99 No headers (baseline) 2,740 \u2014 0.37ms 3.2ms <code>DEFAULT_HEADERS_ALWAYS</code> (3 static+dynamic) 2,690 \u22121.8% 0.38ms 3.3ms <code>DEFAULT_HEADERS_THROTTLED</code> (only on 429) 2,735 \u22120.2% 0.37ms 3.2ms 3 custom headers (dynamic resolvers) 2,680 \u22122.2% 0.38ms 3.4ms 8 headers (4 dynamic resolvers) 2,640 \u22123.6% 0.39ms 3.5ms <p>Takeaway: Headers add roughly 1\u20134% overhead depending on how many dynamic resolvers you have. For most APIs this is negligible. <code>DEFAULT_HEADERS_THROTTLED</code> is nearly free \u2014 resolvers only run on throttled responses.</p> <p>Minimize resolver overhead</p> <p>Static header values (plain strings) are cheaper than dynamic resolver functions. Use static values where you can, and dynamic resolvers only when you need per-request data like <code>hits_remaining</code> or <code>reset_after</code>.</p> <p>Run it yourself:</p> <pre><code>python benchmarks/headers.py --scenarios no-headers,default-always,many-headers\n</code></pre>"},{"location":"benchmarks/#throttlerule-registry-overhead","title":"ThrottleRule registry overhead","text":"<p>Registry evaluation runs on every request when rules are configured. The overhead scales with the number of rules and pattern complexity.</p> Configuration req/s vs. baseline P50 P99 No rules (baseline) 2,740 \u2014 0.37ms 3.2ms Single ThrottleRule (exact path) 2,720 \u22120.7% 0.37ms 3.2ms ThrottleRule (<code>*</code> single-segment wildcard) 2,700 \u22121.5% 0.37ms 3.3ms ThrottleRule (<code>**</code> deep wildcard) 2,710 \u22121.1% 0.37ms 3.2ms <code>BypassThrottleRule</code> + <code>ThrottleRule</code> 2,690 \u22121.8% 0.38ms 3.3ms 10 mixed rules (realistic registry) 2,660 \u22122.9% 0.38ms 3.4ms Compiled <code>re.Pattern</code> rule 2,680 \u22122.2% 0.38ms 3.3ms <p>Takeaway: Even a registry of 10 mixed rules adds under 3% overhead. Rules are evaluated with short-circuit logic \u2014 <code>BypassThrottleRule</code> entries are checked first, so frequently-hit exempted paths (like <code>/health</code>) are fast-pathed out before any <code>ThrottleRule</code> patterns are evaluated.</p> <p>Run it yourself:</p> <pre><code>python benchmarks/rules.py --scenarios no-rules,single-rule,many-rules,bypass-rule\n</code></pre>"},{"location":"benchmarks/#running-all-benchmarks","title":"Running All Benchmarks","text":"<p>Install benchmark dependencies first:</p> <pre><code>pip install \"traffik[dev]\"\n# or\nuv sync\n</code></pre> <p>Then run any combination:</p> <pre><code># HTTP dependency mode (InMemory backend, no external services needed)\npython benchmarks/https.py\n\n# HTTP dependency mode vs SlowAPI with Redis\npython benchmarks/https.py \\\n  --traffik-backend redis --traffik-redis-url redis://localhost:6379/0 \\\n  --slowapi-backend redis --slowapi-redis-url redis://localhost:6379/0\n\n# Middleware mode\npython benchmarks/middleware.py\n\n# WebSocket benchmarks\npython benchmarks/websockets.py --scenarios low,high,sustained,burst,concurrent\n\n# Response headers overhead\npython benchmarks/headers.py\n\n# ThrottleRule registry overhead\npython benchmarks/rules.py\n\n# Strategy comparison (run multiple strategies and compare manually)\npython benchmarks/https.py --traffik-strategy sliding-window-counter\npython benchmarks/https.py --traffik-strategy sliding-window-log\npython benchmarks/https.py --traffik-strategy token-bucket\n</code></pre> <p>All scripts accept <code>--help</code> for full option reference.</p>"},{"location":"configuration/","title":"Configuration","text":"<p>Traffik's locking behavior can be tuned globally \u2014 either through environment variables (great for containers) or programmatically at startup.</p> <p>Most of these settings are about lock behavior, because locks are where the tradeoffs between accuracy, latency, and throughput live. The defaults are sensible for most applications, but understanding them lets you squeeze more performance out of Traffik when you need it.</p>"},{"location":"configuration/#global-lock-settings","title":"Global Lock Settings","text":""},{"location":"configuration/#programmatic-configuration","title":"Programmatic Configuration","text":"<pre><code>from traffik.config import (\n    set_lock_ttl,\n    set_lock_blocking,\n    set_lock_blocking_timeout,\n    get_lock_ttl,\n    get_lock_blocking,\n    get_lock_blocking_timeout,\n)\n\n# Set before creating backends/throttles \u2014 ideally at app startup\nset_lock_ttl(30.0)                # Lock expires after 30 seconds\nset_lock_blocking(True)           # Locks block when contended\nset_lock_blocking_timeout(5.0)    # Give up after 5 seconds of waiting\n\n# Read current settings\nttl = get_lock_ttl()              # -&gt; 30.0\nblocking = get_lock_blocking()    # -&gt; True\ntimeout = get_lock_blocking_timeout()  # -&gt; 5.0\n</code></pre>"},{"location":"configuration/#environment-variables","title":"Environment Variables","text":"<p>Configure via environment variables for containerized deployments:</p> <pre><code># Lock TTL in seconds (float). None = no automatic expiry.\nexport TRAFFIK_DEFAULT_LOCK_TTL=30.0\n\n# Whether to block when lock is contended. Default: true\nexport TRAFFIK_DEFAULT_BLOCKING=true\n\n# Blocking timeout in seconds. None = wait indefinitely.\nexport TRAFFIK_DEFAULT_BLOCKING_TIMEOUT=5.0\n</code></pre> Variable Type Default Description <code>TRAFFIK_DEFAULT_LOCK_TTL</code> float <code>None</code> Lock auto-expiry in seconds <code>TRAFFIK_DEFAULT_BLOCKING</code> bool <code>true</code> Block when lock is contended <code>TRAFFIK_DEFAULT_BLOCKING_TIMEOUT</code> float <code>None</code> Max seconds to wait for lock"},{"location":"configuration/#lock-blocking-settings-reference","title":"Lock Blocking Settings Reference","text":"Profile <code>blocking</code> <code>blocking_timeout</code> <code>lock_ttl</code> Use Case High-accuracy <code>True</code> <code>None</code> <code>30.0</code> Strict rate limiting; clients wait for accurate counts Low-latency <code>True</code> <code>1.0</code> <code>10.0</code> Prefer fast responses; accept occasional imprecision High-concurrency <code>False</code> <code>None</code> <code>5.0</code> Non-blocking; fail fast under contention Dev / Testing <code>True</code> <code>10.0</code> <code>60.0</code> Generous timeouts; accuracy matters less than debugging"},{"location":"configuration/#high-accuracy-configuration","title":"High-Accuracy Configuration","text":"<p>Use this when rate limit correctness is critical (billing, security, compliance):</p> <pre><code>set_lock_blocking(True)\nset_lock_blocking_timeout(None)   # Wait as long as necessary\nset_lock_ttl(30.0)                # Safety TTL to prevent deadlock\n</code></pre> <p>Tradeoff: requests will queue under lock contention, increasing latency. If your backend is slow, this can cause cascading delays.</p>"},{"location":"configuration/#low-latency-configuration","title":"Low-Latency Configuration","text":"<p>Use this for user-facing APIs where a few extra requests sneaking through is acceptable:</p> <pre><code>set_lock_blocking(True)\nset_lock_blocking_timeout(1.0)   # Give up after 1 second\nset_lock_ttl(10.0)\n</code></pre> <p>Tradeoff: if the lock can't be acquired within 1 second, the strategy runs without it \u2014 potentially allowing a small burst above the limit.</p>"},{"location":"configuration/#high-concurrency-configuration","title":"High-Concurrency Configuration","text":"<p>Use this for very high request volumes where you need maximum throughput and can tolerate some over-counting:</p> <pre><code>set_lock_blocking(False)         # Don't wait at all \u2014 fail immediately\nset_lock_ttl(5.0)\n</code></pre> <p>Tradeoff: significant accuracy loss under high concurrency. This is essentially optimistic locking. Consider whether <code>FixedWindow</code> without locking (windows &gt;= 1s) is what you actually want.</p>"},{"location":"configuration/#strategy-level-lock-overrides","title":"Strategy-Level Lock Overrides","text":"<p>Each strategy instance can have its own lock configuration that overrides the global settings:</p> <pre><code>from traffik.strategies import FixedWindow, TokenBucket\nfrom traffik.types import LockConfig\n\n# High-accuracy lock for a sensitive throttle\nsensitive_strategy = TokenBucket(\n    lock_config=LockConfig(\n        ttl=30.0,\n        blocking=True,\n        blocking_timeout=5.0,\n    )\n)\n\n# Fast-fail lock for a high-throughput throttle\nfast_strategy = FixedWindow(\n    lock_config=LockConfig(\n        ttl=5.0,\n        blocking=False,\n    )\n)\n\nfrom traffik import HTTPThrottle\n\nbilling_throttle = HTTPThrottle(\"billing\", rate=\"10/min\", strategy=sensitive_strategy)\nfeed_throttle = HTTPThrottle(\"feed\", rate=\"1000/min\", strategy=fast_strategy)\n</code></pre>"},{"location":"configuration/#sub-second-windows-and-locking","title":"Sub-Second Windows and Locking","text":"<p>Sub-second windows always use locking</p> <p>For <code>FixedWindow</code> and <code>SlidingWindowCounter</code>, windows smaller than 1 second always acquire a distributed lock, regardless of global settings. This is necessary because atomic <code>increment_with_ttl</code> alone isn't sufficient for sub-millisecond precision \u2014 the window boundary tracking requires a separate read/write that must be protected.</p> <p>For windows of 1 second or longer, <code>FixedWindow</code> uses only <code>increment_with_ttl</code> (which is atomic) and skips the lock entirely. This is the fastest path.</p> <pre><code># No lock needed \u2014 uses atomic increment_with_ttl\nthrottle = HTTPThrottle(\"api\", rate=\"100/min\")   # 60 seconds &gt;= 1 second\n\n# Always uses locking \u2014 requires window boundary management\nthrottle = HTTPThrottle(\"api\", rate=\"10/500ms\")  # 500ms &lt; 1 second\n</code></pre> <p>Prefer &gt;= 1s windows in production</p> <p>Sub-second windows are great for bursty traffic control, but they add locking overhead. If you don't specifically need millisecond-level precision, use windows of 1 second or more and enjoy the lock-free fast path.</p>"},{"location":"configuration/#lock-contention-and-its-effects","title":"Lock Contention and Its Effects","text":"<p>When many requests compete for the same lock, you'll see:</p> Effect Symptom Mitigation Increased latency p95/p99 response times spike Reduce <code>blocking_timeout</code>; use non-locking strategies Lock timeouts <code>LockTimeout</code> errors in logs Increase <code>lock_ttl</code>; check backend latency Throughput degradation Requests per second drops Switch to <code>FixedWindow</code> with &gt;= 1s windows; use lock striping"},{"location":"configuration/#mitigation-strategies","title":"Mitigation Strategies","text":"<ol> <li> <p>Use longer windows: The easiest fix. <code>rate=\"100/min\"</code> has no lock overhead; <code>rate=\"100/500ms\"</code> does.</p> </li> <li> <p>Tune <code>blocking_timeout</code>: Set a low timeout (e.g., 0.5s) to fail fast rather than queue up. Accept minor inaccuracy as the tradeoff.</p> </li> <li> <p>Non-locking strategies for &gt;= 1s windows: <code>FixedWindow</code> with &gt;= 1s windows is lock-free by design. It's the fastest built-in strategy.</p> </li> <li> <p>Lock striping for InMemory: If you're on <code>InMemoryBackend</code>, configure more shards to reduce contention:</p> <pre><code>from traffik.backends.inmemory import InMemoryBackend\n\nbackend = InMemoryBackend(\n    namespace=\"myapp\",\n    number_of_shards=64,   # More shards = less lock contention\n)\n</code></pre> </li> <li> <p>Connection pooling for Redis/Memcached: Lock acquisition latency is dominated by round-trip time to the backend. Increase pool size to reduce queuing.</p> </li> </ol>"},{"location":"error-handling/","title":"Error Handling","text":"<p>Rate limiting backends can fail. Redis goes down. Network blips happen. Memcached runs out of memory. The question isn't if your backend will fail \u2014 it's what should Traffik do when it does?</p> <p>Traffik gives you fine-grained control over that answer: from a simple one-liner to a full circuit-breaker pattern with automatic failover.</p>"},{"location":"error-handling/#the-error-handler-signature","title":"The Error Handler Signature","text":"<p>A custom error handler is an async function that receives the connection and exception info, and returns a <code>WaitPeriod</code> (milliseconds):</p> <pre><code>from traffik.throttles import ThrottleExceptionInfo\nfrom traffik.types import WaitPeriod\n\nasync def my_handler(\n    connection,                  # Request or WebSocket\n    exc_info: ThrottleExceptionInfo,\n) -&gt; WaitPeriod:\n    # Return 0 to allow the request through (fail open)\n    # Return a positive number to throttle the client\n    # Raise to propagate the exception\n    return 0\n</code></pre>"},{"location":"error-handling/#throttleexceptioninfo-fields","title":"<code>ThrottleExceptionInfo</code> Fields","text":"Field Type Description <code>exception</code> <code>Exception</code> The exception that was raised by the backend <code>connection</code> <code>HTTPConnection</code> The current HTTP connection <code>cost</code> <code>int</code> The cost of the throttle operation that failed <code>rate</code> <code>Rate</code> The rate limit definition <code>backend</code> <code>ThrottleBackend</code> The backend that raised the error <code>context</code> <code>dict \\| None</code> The throttle context at the time of the error <code>throttle</code> <code>Throttle</code> The throttle instance that triggered the operation"},{"location":"error-handling/#string-shortcuts","title":"String Shortcuts","text":"<p>For simple cases, skip the function entirely and use one of three string values:</p> <pre><code>from traffik import HTTPThrottle\n\n# Fail open: backend error -&gt; allow the request through\nthrottle = HTTPThrottle(\"api\", rate=\"100/min\", on_error=\"allow\")\n\n# Fail closed: backend error -&gt; throttle the client as if limit exceeded\nthrottle = HTTPThrottle(\"api\", rate=\"100/min\", on_error=\"throttle\")\n\n# Propagate: backend error -&gt; raise the exception (crashes the request)\nthrottle = HTTPThrottle(\"api\", rate=\"100/min\", on_error=\"raise\")\n</code></pre> <p><code>\"throttle\"</code> is the default behavior \u2014 if you don't set <code>on_error</code>, Traffik fails closed. Better to slow things down than to let unlimited traffic through when your backend is struggling.</p>"},{"location":"error-handling/#backend_fallback-automatic-failover","title":"<code>backend_fallback</code> \u2014 Automatic Failover","text":"<p>Switch to a backup backend when the primary fails:</p> <pre><code>from traffik import HTTPThrottle\nfrom traffik.backends.redis import RedisBackend\nfrom traffik.backends.inmemory import InMemoryBackend\nfrom traffik.error_handlers import backend_fallback\nfrom traffik.exceptions import BackendError\n\nprimary = RedisBackend(\"redis://primary:6379\", namespace=\"myapp\")\nfallback = InMemoryBackend(namespace=\"myapp-fallback\")\n\nthrottle = HTTPThrottle(\n    \"api\",\n    rate=\"100/min\",\n    backend=primary,\n    on_error=backend_fallback(\n        backend=fallback,\n        fallback_on=(BackendError, TimeoutError),\n    )\n)\n</code></pre> <p>How it works:</p> <ol> <li>Primary backend raises an exception</li> <li>Handler checks if the exception type is in <code>fallback_on</code></li> <li>If yes: attempts the throttle operation on the fallback backend</li> <li>If the fallback also fails: re-raises the exception</li> <li>If no: re-raises the original exception immediately</li> </ol> <p>InMemory as fallback</p> <p><code>InMemoryBackend</code> makes an excellent fallback for Redis/Memcached. It's always available (no network), starts up instantly, and provides reasonable throttling even without distributed coordination. In a single-node failure scenario, per-process rate limiting is usually better than no rate limiting.</p>"},{"location":"error-handling/#retry-retry-transient-failures","title":"<code>retry</code> \u2014 Retry Transient Failures","text":"<p>Retry the throttle operation with backoff before giving up:</p> <pre><code>from traffik import HTTPThrottle\nfrom traffik.error_handlers import retry\n\nthrottle = HTTPThrottle(\n    \"api\",\n    rate=\"100/min\",\n    on_error=retry(\n        max_retries=3,\n        retry_delay=0.1,             # Start with 100ms\n        backoff_multiplier=2.0,      # Double each retry: 100ms, 200ms, 400ms\n        retry_on=(TimeoutError,),    # Only retry timeouts, not config errors\n    )\n)\n</code></pre> <p>How it works:</p> <ol> <li>Exception is raised during throttling</li> <li>If not in <code>retry_on</code>: re-raise immediately</li> <li>If in <code>retry_on</code>: wait <code>retry_delay</code> seconds and retry</li> <li>Apply <code>backoff_multiplier</code> to delay for subsequent retries</li> <li>If all retries fail: re-raise the last exception</li> </ol>"},{"location":"error-handling/#failover-full-circuit-breaker-retry-fallback","title":"<code>failover</code> \u2014 Full Circuit Breaker + Retry + Fallback","text":"<p>The recommended pattern for production. Combines all resilience techniques:</p> <pre><code>from traffik import HTTPThrottle\nfrom traffik.backends.redis import RedisBackend\nfrom traffik.backends.inmemory import InMemoryBackend\nfrom traffik.error_handlers import failover, CircuitBreaker\n\nprimary = RedisBackend(\"redis://primary:6379\", namespace=\"myapp\")\nfallback = InMemoryBackend(namespace=\"myapp-fallback\")\n\nbreaker = CircuitBreaker(\n    failure_threshold=5,      # Open after 5 consecutive failures\n    recovery_timeout=30.0,    # Try to recover after 30 seconds\n    success_threshold=2,      # Close after 2 consecutive successes in half-open\n)\n\nthrottle = HTTPThrottle(\n    \"api\",\n    rate=\"100/min\",\n    backend=primary,\n    on_error=failover(\n        backend=fallback,\n        breaker=breaker,\n        max_retries=2,\n        retry_delay=0.05,\n    )\n)\n</code></pre> <p>How it works:</p> Circuit State Behavior <code>CLOSED</code> Normal: retry primary up to <code>max_retries</code> times, then fall back <code>OPEN</code> Degraded: skip primary entirely, use fallback immediately <code>HALF_OPEN</code> Testing: allow one request through primary; success closes circuit, failure reopens <p>State transitions:</p> <ul> <li><code>CLOSED</code> \u2192 <code>OPEN</code>: <code>failure_threshold</code> consecutive failures</li> <li><code>OPEN</code> \u2192 <code>HALF_OPEN</code>: after <code>recovery_timeout</code> seconds</li> <li><code>HALF_OPEN</code> \u2192 <code>CLOSED</code>: <code>success_threshold</code> consecutive successes</li> <li><code>HALF_OPEN</code> \u2192 <code>OPEN</code>: any failure during recovery test</li> </ul>"},{"location":"error-handling/#circuitbreaker-reference","title":"<code>CircuitBreaker</code> Reference","text":"<pre><code>from traffik.error_handlers import CircuitBreaker\n\nbreaker = CircuitBreaker(\n    failure_threshold=5,    # Failures before opening (default: 5)\n    recovery_timeout=60.0,  # Seconds before half-open attempt (default: 60.0)\n    success_threshold=2,    # Successes in half-open to close (default: 2)\n)\n\n# Inspect state\ninfo = breaker.info()\n# {\n#     \"state\": \"closed\",   # \"closed\", \"open\", or \"half_open\"\n#     \"failures\": 0,\n#     \"successes\": 0,\n#     \"opened_at\": None,   # datetime when circuit opened, or None\n# }\n</code></pre>"},{"location":"error-handling/#custom-error-handler-with-logging","title":"Custom Error Handler with Logging","text":"<pre><code>import logging\nfrom traffik import HTTPThrottle\nfrom traffik.types import WaitPeriod\nfrom traffik.throttles import ThrottleExceptionInfo\nfrom traffik.exceptions import BackendError\n\nlogger = logging.getLogger(\"traffik.errors\")\n\n\nasync def logged_fallback_handler(connection, exc_info: ThrottleExceptionInfo) -&gt; WaitPeriod:\n    exc = exc_info[\"exception\"]\n    throttle = exc_info[\"throttle\"]\n\n    if isinstance(exc, BackendError):\n        logger.warning(\n            \"Backend error on throttle %r: %s. Allowing request through.\",\n            throttle.uid,\n            exc,\n        )\n        return 0  # Fail open: allow the request\n\n    logger.error(\n        \"Unexpected error on throttle %r: %s. Throttling client.\",\n        throttle.uid,\n        exc,\n        exc_info=True,\n    )\n    return 1000  # Fail closed: throttle for 1 second\n\n\nthrottle = HTTPThrottle(\n    \"api\",\n    rate=\"100/min\",\n    on_error=logged_fallback_handler,\n)\n</code></pre> <p>Don't log inside backends</p> <p>Logging inside backend operations (not error handlers) causes serious performance degradation. Logging is synchronous and can block the event loop. See Performance Tips for details. Logging inside error handlers is fine \u2014 handlers only run when something goes wrong.</p>"},{"location":"error-handling/#error-handler-selection-guide","title":"Error Handler Selection Guide","text":"Scenario Handler to Use Development / low stakes <code>on_error=\"allow\"</code> \u2014 fail open, keep dev experience smooth Security-sensitive API <code>on_error=\"throttle\"</code> \u2014 fail closed, protect the resource Debugging backend issues <code>on_error=\"raise\"</code> \u2014 propagate exceptions for visibility Redis down, InMemory fallback <code>backend_fallback(fallback_backend)</code> Transient network blips <code>retry(max_retries=3, retry_on=(TimeoutError,))</code> Production with HA requirements <code>failover(fallback, breaker=CircuitBreaker(...))</code> Custom logic + logging Custom async function"},{"location":"error-handling/#backend-level-vs-throttle-level-handlers","title":"Backend-Level vs. Throttle-Level Handlers","text":"<p>You can set <code>on_error</code> at the backend level \u2014 it applies to all throttles using that backend:</p> <pre><code>backend = RedisBackend(\n    \"redis://localhost:6379\",\n    namespace=\"myapp\",\n    on_error=\"allow\",   # Backend-level default: fail open\n)\n\n# This throttle inherits the backend's \"allow\" behavior\nread_throttle = HTTPThrottle(\"api:read\", rate=\"1000/min\", backend=backend)\n\n# This throttle overrides with its own handler (takes precedence)\nwrite_throttle = HTTPThrottle(\n    \"api:write\",\n    rate=\"100/min\",\n    backend=backend,\n    on_error=\"throttle\",   # Throttle-level: fail closed for writes\n)\n</code></pre> <p>Throttle-level <code>on_error</code> always takes precedence over backend-level <code>on_error</code>.</p>"},{"location":"error-handling/#connectionthrottled","title":"<code>ConnectionThrottled</code>","text":"<p>When a client exceeds the rate limit, Traffik raises <code>ConnectionThrottled</code>. This is a subclass of Starlette's <code>HTTPException</code>, which means FastAPI and Starlette handle it automatically \u2014 no custom handler registration is needed for basic behavior. The client receives a <code>429 Too Many Requests</code> response without any extra setup on your part.</p> <pre><code>from traffik.exceptions import ConnectionThrottled\n</code></pre>"},{"location":"error-handling/#custom-handler-for-richer-responses","title":"Custom handler for richer responses","text":"<p>If you want to return a richer response \u2014 for example, including <code>Retry-After</code> information in the response body or a custom JSON structure \u2014 register a custom exception handler:</p> <pre><code>from fastapi import FastAPI, Request\nfrom fastapi.responses import JSONResponse\nfrom traffik.exceptions import ConnectionThrottled\n\napp = FastAPI()\n\nasync def throttle_handler(request: Request, exc: ConnectionThrottled) -&gt; JSONResponse:\n    retry_after = exc.headers.get(\"Retry-After\") if exc.headers else None\n    return JSONResponse(\n        status_code=429,\n        content={\n            \"error\": \"rate_limit_exceeded\",\n            \"detail\": exc.detail,\n            \"retry_after\": retry_after,\n        },\n        headers=exc.headers or {},\n    )\n\napp.add_exception_handler(ConnectionThrottled, throttle_handler)\n</code></pre>"},{"location":"error-handling/#why-prefer-connectionthrottled-over-plain-httpexception","title":"Why prefer <code>ConnectionThrottled</code> over plain <code>HTTPException</code>?","text":"<p>When writing throttle-related code \u2014 custom handlers, middleware, or decorators \u2014 use <code>ConnectionThrottled</code> rather than a plain <code>HTTPException(status_code=429)</code>. It carries the same automatic handling, but conveys clearer semantics: this is a rate-limit error specifically, not just any 429. It also makes your exception handlers easier to scope precisely.</p> <pre><code># Prefer this in throttle-related code:\nraise ConnectionThrottled(detail=\"Too many requests. Please slow down.\")\n\n# Rather than the generic form:\n# raise HTTPException(status_code=429, detail=\"Too many requests.\")\n</code></pre>"},{"location":"installation/","title":"Installation","text":"<p>Getting Traffik installed takes about thirty seconds. Let's make sure you get the right extras for your setup.</p>"},{"location":"installation/#requirements","title":"Requirements","text":"<p>Before you install, make sure you have:</p> <ul> <li>Python 3.9+ - Traffik uses modern async features and type hints throughout.</li> <li>FastAPI or Starlette - Traffik integrates with Starlette's <code>HTTPConnection</code> model, which FastAPI is built on.</li> </ul> <p>That's it. No mandatory external services, no heavyweight dependencies. The in-memory backend works with no additional setup.</p>"},{"location":"installation/#install-options","title":"Install Options","text":"<p>Traffik ships with optional extras for each storage backend. Install only what you need.</p> pipuvpoetry <pre><code># In-memory backend only (great for development)\npip install traffik\n\n# With Redis support\npip install \"traffik[redis]\"\n\n# With Memcached support\npip install \"traffik[memcached]\"\n\n# All backends\npip install \"traffik[all]\"\n\n# Development extras (testing tools, linters, etc.)\npip install \"traffik[dev]\"\n</code></pre> <pre><code># In-memory backend only\nuv add traffik\n\n# With Redis support\nuv add \"traffik[redis]\"\n\n# With Memcached support\nuv add \"traffik[memcached]\"\n\n# All backends\nuv add \"traffik[all]\"\n\n# Development extras\nuv add \"traffik[dev]\"\n</code></pre> <pre><code># In-memory backend only\npoetry add traffik\n\n# With Redis support\npoetry add \"traffik[redis]\"\n\n# With Memcached support\npoetry add \"traffik[memcached]\"\n\n# All backends\npoetry add \"traffik[all]\"\n\n# Development extras\npoetry add \"traffik[dev]\"\n</code></pre>"},{"location":"installation/#what-each-extra-installs","title":"What Each Extra Installs","text":"Extra What it adds When to use it (none) Core package + <code>InMemoryBackend</code> Local development, tests, single-process apps <code>[redis]</code> <code>redis-py</code>, <code>pottery</code> Multi-instance production deployments <code>[memcached]</code> <code>aiomcache</code> High-throughput environments where you already run Memcached <code>[all]</code> Everything above When you want to try all backends or run the full test suite <code>[dev]</code> Testing tools, linters, type checkers Contributing to Traffik"},{"location":"installation/#choosing-a-backend","title":"Choosing a Backend","text":"<p>Start with InMemory, graduate to Redis</p> <p>During local development and in CI, the <code>InMemoryBackend</code> is perfect. It requires zero configuration and zero external services. When you're ready to deploy, swap it for <code>RedisBackend</code>, the rest of your code stays the same.</p> <pre><code># Development\nfrom traffik.backends.inmemory import InMemoryBackend\nbackend = InMemoryBackend(namespace=\"myapp\")\n\n# Production (just change these two lines)\nfrom traffik.backends.redis import RedisBackend\nbackend = RedisBackend(connection=\"redis://localhost:6379/0\", namespace=\"myapp\")\n</code></pre> <p>InMemory is not shared across processes</p> <p>The <code>InMemoryBackend</code> lives in a single Python process. If you run your app with multiple workers (e.g., <code>uvicorn --workers 4</code>), each worker has its own independent counter. Use <code>RedisBackend</code> or <code>MemcachedBackend</code> for any multi-process or multi-host setup.</p> <p>Memcached caveats</p> <p>Memcached doesn't support atomic increment-and-expire in a single operation the way Redis does, so the <code>MemcachedBackend</code> uses best-effort distributed locks. This is fine for most workloads, but if you need strong consistency guarantees, Redis is the safer choice.</p>"},{"location":"installation/#verifying-the-installation","title":"Verifying the Installation","text":"<pre><code>import traffik\n\nprint(traffik.__version__)  # Should print \"1.*.*\"\n</code></pre> <p>Or from the command line:</p> <pre><code>python -c \"import traffik; print(traffik.__version__)\"\n</code></pre>"},{"location":"installation/#fully-typed","title":"Fully Typed","text":"<p>Traffik is a PEP 561 compliant package. It ships with a <code>py.typed</code> marker, which means type checkers like <code>mypy</code> and <code>pyright</code> pick up its type annotations automatically, no stub packages required.</p> <p>Pyright / Pylance users</p> <p>Traffik's generics (<code>Throttle[Request]</code>, <code>ThrottleBackend[..., Request]</code>) resolve correctly with strict mode. If you see false positives, make sure you're on the latest version of your type checker.</p>"},{"location":"installation/#next-steps","title":"Next Steps","text":"<p>You're all set. Head over to the Quick Start to write your first throttled endpoint.</p>"},{"location":"performance/","title":"Performance Tips","text":"<p>Traffik is designed to be fast out of the box. But like all systems, there are knobs to turn when you need to squeeze out more. This page covers the most impactful optimizations, roughly ordered by impact.</p>"},{"location":"performance/#quick-wins-checklist","title":"Quick Wins Checklist","text":"<p>Before diving into specifics, here are the changes that have the most impact for the least effort:</p> <ul> <li> Use &gt;= 1 second rate windows (eliminates lock overhead for FixedWindow/SlidingWindowCounter)</li> <li> Enable InMemory shard striping: <code>InMemoryBackend(number_of_shards=32)</code></li> <li> Use connection pooling: <code>MemcachedBackend(pool_size=20)</code></li> <li> Don't call DB/external APIs inside identifier functions</li> <li> Exempt trusted internal clients with <code>EXEMPTED</code> (zero backend overhead)</li> <li> Keep <code>cache_ids=True</code> (default) \u2014 especially for WebSockets</li> </ul>"},{"location":"performance/#1-use-the-right-backend","title":"1. Use the Right Backend","text":"<p>The backend choice has a larger impact on performance than any other single factor.</p> Deployment Backend Notes Single-process (dev, small apps) <code>InMemoryBackend</code> Zero network overhead, fastest possible Multi-process, single machine <code>InMemoryBackend</code> + replication State doesn't share between processes Distributed (multiple nodes) <code>RedisBackend</code> Network round-trip, but accurate distributed counting Already have Memcached <code>MemcachedBackend</code> Comparable to Redis, no scripting support Redis + memory efficiency <code>RedisBackend</code> Scripts cached on server, minimal overhead <p>Don't use <code>RedisBackend</code> for a single-process application just because \"Redis is production-grade.\" The network round-trip will cost you 1-5ms per request unnecessarily. <code>InMemoryBackend</code> is genuinely the right tool for single-process deployments.</p> <pre><code>from traffik.backends.inmemory import InMemoryBackend\nfrom traffik.backends.redis import RedisBackend\n\n# Development / single-process\nbackend = InMemoryBackend(namespace=\"myapp\")\n\n# Production / distributed\nbackend = RedisBackend(\"redis://localhost:6379\", namespace=\"myapp\")\n</code></pre>"},{"location":"performance/#2-choose-your-strategy-wisely","title":"2. Choose Your Strategy Wisely","text":"<p>Strategies differ significantly in performance. Here's the order from fastest to slowest:</p> Strategy Overhead Accuracy Best For <code>FixedWindow</code> (&gt;= 1s window) Minimal \u2014 single atomic op, no lock Good General APIs, high throughput <code>SlidingWindowCounter</code> Low \u2014 two atomic ops Better When burst boundaries matter <code>TokenBucket</code> Medium \u2014 lock + read/write Excellent (burst-aware) APIs with natural burst patterns <code>LeakyBucket</code> Medium \u2014 lock + read/write Excellent (smooth) Enforcing constant rate <code>SlidingWindowLog</code> High \u2014 serialization + cleanup Perfect Strict correctness, lower traffic <p>The default strategy is <code>FixedWindow</code>. For most applications, it's the right choice \u2014 it's accurate enough, and the performance is hard to beat.</p> <pre><code>from traffik import HTTPThrottle\nfrom traffik.strategies import FixedWindow, TokenBucket\n\n# Fastest: FixedWindow with a &gt;= 1s window\nthrottle = HTTPThrottle(\"api\", rate=\"1000/min\", strategy=FixedWindow())\n\n# More nuanced: TokenBucket for burst-friendly limiting\nthrottle = HTTPThrottle(\"api\", rate=\"100/sec\", strategy=TokenBucket())\n</code></pre>"},{"location":"performance/#3-configure-lock-striping-inmemory","title":"3. Configure Lock Striping (InMemory)","text":"<p><code>InMemoryBackend</code> uses internal locks to prevent race conditions. By default it uses a small number of shards. Increasing the shard count distributes lock contention across more buckets, dramatically improving throughput under high concurrency:</p> <pre><code>from traffik.backends.inmemory import InMemoryBackend\n\n# Default: small number of shards (low concurrency overhead)\nbackend = InMemoryBackend(namespace=\"myapp\")\n\n# High concurrency: 32 shards distribute lock contention\nbackend = InMemoryBackend(namespace=\"myapp\", number_of_shards=32)\n</code></pre> <p>A good rule of thumb: set <code>number_of_shards</code> to roughly your expected peak concurrent requests / 4. For most applications, 16-64 shards is a good range.</p>"},{"location":"performance/#4-use-connection-pooling-memcached","title":"4. Use Connection Pooling (Memcached)","text":"<p>Every throttle check requires a backend operation. Without connection pooling, each operation opens and closes a network connection \u2014 this is expensive. Use pooling:</p> <pre><code>from traffik.backends.memcached import MemcachedBackend\n\n# Default: small pool\nbackend = MemcachedBackend(host=\"localhost\", port=11211, namespace=\"myapp\")\n\n# Production: larger pool to handle concurrent requests\nbackend = MemcachedBackend(\n    host=\"localhost\",\n    port=11211,\n    namespace=\"myapp\",\n    pool_size=20,   # Tune based on expected concurrency\n)\n</code></pre> <p>For Redis, <code>aioredis</code> (used by <code>RedisBackend</code>) handles connection pooling internally. You can tune it via the connection URL or by passing a pre-configured client.</p>"},{"location":"performance/#5-keep-identifier-functions-cheap","title":"5. Keep Identifier Functions Cheap","text":"<p>The identifier function runs on every request to determine who is being throttled. A slow identifier function adds latency proportional to your traffic.</p> <pre><code># Fast: read from request headers (in-memory, no I/O)\nasync def fast_identifier(request: Request):\n    return request.headers.get(\"X-API-Key\") or request.client.host\n\n# Slow: database lookup on every request (don't do this!)\nasync def slow_identifier(request: Request):\n    api_key = request.headers.get(\"X-API-Key\")\n    user = await db.fetch(\"SELECT id FROM users WHERE api_key = $1\", api_key)\n    return user[\"id\"]  # Every request = one DB query\n</code></pre> <p>If you need to look up user information, cache it in the request state after the first lookup, or do the lookup once in authentication middleware and store the result in <code>request.state.user_id</code>.</p>"},{"location":"performance/#6-avoid-logging-in-backend-operations","title":"6. Avoid Logging in Backend Operations","text":"<p>This sounds minor but it isn't. Logging is synchronous I/O and can block the async event loop under load.</p> <pre><code># This is inside the hot path \u2014 every request goes through here\nasync def my_backend_increment(self, key: str, amount: int = 1) -&gt; int:\n    # BAD: logging on every request\n    logger.debug(f\"Incrementing {key} by {amount}\")\n    result = await self._client.incr(key, amount)\n    logger.debug(f\"New value: {result}\")\n    return result\n\n# Better: no logging in the hot path\nasync def my_backend_increment(self, key: str, amount: int = 1) -&gt; int:\n    return await self._client.incr(key, amount)\n</code></pre> <p>According to our benchmarks, adding <code>logger.debug()</code> calls inside backend operations produces a ~10x slowdown on high-concurrency workloads. Log in error handlers (which only run on failures), not in the normal execution path.</p>"},{"location":"performance/#7-exempt-trusted-clients-with-exempted","title":"7. Exempt Trusted Clients with EXEMPTED","text":"<p>For trusted internal services, health check endpoints, or admin users, returning <code>EXEMPTED</code> from the identifier completely bypasses throttling \u2014 no counter read, no counter write, no lock. Zero overhead:</p> <pre><code>from traffik import EXEMPTED\n\nINTERNAL_TOKEN = \"my-internal-service-token\"\n\nasync def smart_identifier(request: Request):\n    # Internal services: completely bypassed\n    if request.headers.get(\"X-Internal-Token\") == INTERNAL_TOKEN:\n        return EXEMPTED\n\n    # Everyone else: throttled by API key\n    return request.headers.get(\"X-API-Key\") or request.client.host\n\n\nthrottle = HTTPThrottle(\"api\", rate=\"1000/min\", identifier=smart_identifier)\n</code></pre> <p>This is especially useful for health check endpoints that get hammered by load balancers. There's no point counting those requests \u2014 exempt them and save the backend round-trip.</p>"},{"location":"performance/#8-enable-identifier-caching-its-default","title":"8. Enable Identifier Caching (It's Default)","text":"<p><code>cache_ids=True</code> is the default and caches the computed identifier on the connection's <code>state</code> object. For WebSocket connections especially, this prevents recomputing the identifier on every message:</p> <pre><code># cache_ids=True (default) \u2014 computes identifier once, caches on connection.state\nthrottle = HTTPThrottle(\"api\", rate=\"100/min\", cache_ids=True)\n\n# cache_ids=False \u2014 recomputes identifier on every hit\nthrottle = HTTPThrottle(\"api\", rate=\"100/min\", cache_ids=False)\n</code></pre> <p>Only disable <code>cache_ids</code> if your identifier might change during a connection's lifetime (rare). For everything else, leave it enabled.</p>"},{"location":"performance/#9-sort-middleware-throttles-cheap-first","title":"9. Sort Middleware Throttles Cheap First","text":"<p>When using multiple throttles in middleware, put the cheapest (lowest limit, most likely to fire) first. A throttle that rejects a request early prevents the more expensive throttles from running at all:</p> <pre><code>from traffik.middleware import ThrottleMiddleware\n\n# Burst throttle (cheap, low limit) runs first\n# Sustained throttle (more expensive, higher limit) only runs if burst passes\napp.add_middleware(\n    ThrottleMiddleware,\n    throttles=[burst_throttle, sustained_throttle],\n    # cheap_first=True is the default \u2014 throttles are automatically sorted\n)\n</code></pre> <p>Traffik's middleware already applies <code>cheap_first</code> ordering by default, so this is mostly about being deliberate when configuring multiple throttles.</p>"},{"location":"performance/#10-avoid-sub-second-rate-windows-in-production","title":"10. Avoid Sub-Second Rate Windows in Production","text":"<p>Sub-second windows (e.g., <code>\"100/500ms\"</code>, <code>\"10/100ms\"</code>) trigger locking in <code>FixedWindow</code> and <code>SlidingWindowCounter</code> \u2014 even when they would otherwise be lock-free. This is necessary for accuracy at millisecond precision, but it adds distributed lock overhead on every request.</p> <pre><code># Lock-free: uses atomic increment_with_ttl only\nthrottle = HTTPThrottle(\"api\", rate=\"6000/min\")    # 60s window &gt;= 1s\n\n# Requires locking: sub-second window\nthrottle = HTTPThrottle(\"api\", rate=\"100/500ms\")   # 500ms &lt; 1s\n</code></pre> <p>In most cases, you can express the same intent with a &gt;= 1s window and get better performance:</p> <pre><code># These are semantically equivalent for burst protection:\nthrottle = HTTPThrottle(\"api\", rate=\"20/sec\")       # 1s window, no lock overhead\n# vs.\nthrottle = HTTPThrottle(\"api\", rate=\"10/500ms\")     # 500ms window, adds lock overhead\n</code></pre> <p>Use sub-second windows when you genuinely need millisecond-level burst control. Otherwise, stick to seconds.</p>"},{"location":"performance/#putting-it-all-together","title":"Putting It All Together","text":"<p>Here's a production-optimized configuration for a high-traffic API:</p> <pre><code>from traffik import HTTPThrottle, EXEMPTED\nfrom traffik.backends.redis import RedisBackend\nfrom traffik.strategies import FixedWindow\n\n# Redis backend (distributed, production-grade)\nbackend = RedisBackend(\n    \"redis://localhost:6379\",\n    namespace=\"myapp\",\n    on_error=\"allow\",  # Fail open on backend errors (tune per your risk tolerance)\n)\n\n# Efficient identifier: fast header read, exempt internal services\nasync def api_identifier(request):\n    if request.headers.get(\"X-Internal\") == \"true\":\n        return EXEMPTED\n    return request.headers.get(\"X-API-Key\") or request.client.host\n\n# Fast strategy: FixedWindow with &gt;= 1s window = lock-free path\nthrottle = HTTPThrottle(\n    \"api:v1\",\n    rate=\"1000/min\",          # 60s window, no lock overhead\n    strategy=FixedWindow(),\n    identifier=api_identifier,\n    cache_ids=True,            # Cache identifier per connection\n    backend=backend,\n)\n</code></pre>"},{"location":"quickstart/","title":"Quick Start","text":"<p>Let's go from zero to a rate-limited API in a few minutes. Each step builds on the last, so by the end you'll have a solid mental model of how Traffik works, and a production-ready example to draw from.</p>"},{"location":"quickstart/#step-1-the-simplest-possible-setup","title":"Step 1: The Simplest Possible Setup","text":"<p>Let's start with the absolute minimum: one backend, one throttle, one endpoint.</p> <pre><code># main.py\nfrom fastapi import FastAPI, Depends\nfrom traffik import HTTPThrottle\nfrom traffik.backends.inmemory import InMemoryBackend\n\n# 1. Create a backend. InMemory needs no configuration.\nbackend = InMemoryBackend(namespace=\"myapp\")\n\n# 2. Tell FastAPI to use the backend's lifespan (initialize on startup, clean up on shutdown).\napp = FastAPI(lifespan=backend.lifespan)\n\n# 3. Create a throttle with a unique ID and a rate limit.\nthrottle = HTTPThrottle(\"api:items\", rate=\"100/min\")\n\n# 4. Attach it to your route as a FastAPI dependency.\n@app.get(\"/items\", dependencies=[Depends(throttle)])\nasync def list_items():\n    return {\"items\": [\"widget\", \"gizmo\", \"thingamajig\"]}\n</code></pre> <p>Run it:</p> <pre><code>uvicorn main:app --reload\n</code></pre> <p>Now send more than 100 requests in a minute and you'll get:</p> <pre><code>HTTP 429 Too Many Requests\nRetry-After: 42\n</code></pre> <p>The <code>uid</code> must be unique</p> <p>Every throttle needs a unique string ID (<code>uid</code>). This is what Traffik uses to store and look up state in the backend. A good convention is <code>\"feature:purpose\"</code>, like <code>\"auth:login\"</code> or <code>\"api:v2:search\"</code>. Don't reuse UIDs across different throttle instances or you'll get a <code>ConfigurationError</code> at startup.</p>"},{"location":"quickstart/#step-2-understanding-the-backend-lifespan-pattern","title":"Step 2: Understanding the Backend + Lifespan Pattern","text":"<p>Why does the backend need a lifespan? Because backends hold open connections (to Redis, Memcached, etc.) and run background cleanup tasks. The lifespan ensures those are set up before your first request and torn down cleanly when you shut down.</p> <p>The <code>backend.lifespan</code> is just a standard Starlette/FastAPI <code>lifespan</code> context manager. If you already have a lifespan, compose them:</p> <pre><code>from contextlib import asynccontextmanager\nfrom fastapi import FastAPI\nfrom traffik.backends.inmemory import InMemoryBackend\n\nbackend = InMemoryBackend(namespace=\"myapp\")\n\n@asynccontextmanager\nasync def lifespan(app: FastAPI):\n    # Your startup logic here (e.g., database connections)\n    async with backend(app):  # Initialize the backend\n        yield\n    # Your shutdown logic runs after `yield`\n\napp = FastAPI(lifespan=lifespan)\n</code></pre> <p>What <code>backend(app)</code> does</p> <p>Calling the backend as a context manager (<code>async with backend(app)</code>) registers it as the default backend for that application. Any <code>HTTPThrottle</code> that doesn't specify an explicit backend will automatically discover and use it. This is the most common pattern.</p>"},{"location":"quickstart/#step-3-using-it-as-a-dependency","title":"Step 3: Using It as a Dependency","text":"<p>You've already seen the <code>dependencies=[Depends(throttle)]</code> pattern. There's a second way: inject the throttle as a parameter to get access to the request object in the same dependency call:</p> <pre><code>from fastapi import FastAPI, Request, Depends\nfrom traffik import HTTPThrottle\nfrom traffik.backends.inmemory import InMemoryBackend\n\nbackend = InMemoryBackend(namespace=\"myapp\")\napp = FastAPI(lifespan=backend.lifespan)\n\nthrottle = HTTPThrottle(\"api:items\", rate=\"100/min\")\n\n@app.get(\"/items\")\nasync def list_items(request: Request = Depends(throttle)):\n    # `request` is the actual FastAPI Request object.\n    # Traffik returns it after checking the rate limit.\n    client_ip = request.client.host\n    return {\"items\": [], \"client_ip\": client_ip}\n</code></pre> <p>Both styles work identically. The first (<code>dependencies=[...]</code>) is cleaner when you don't need the request. The second gives you the request object inside the handler.</p>"},{"location":"quickstart/#applying-multiple-throttles","title":"Applying multiple throttles","text":"<p>Stack throttles to enforce multiple limits simultaneously, for example, a per-minute burst limit and a per-hour sustained limit:</p> <pre><code>burst_throttle    = HTTPThrottle(\"uploads:burst\",     rate=\"10/min\")\nsustained_throttle = HTTPThrottle(\"uploads:sustained\", rate=\"100/hour\")\n\n@app.post(\"/upload\", dependencies=[Depends(burst_throttle), Depends(sustained_throttle)])\nasync def upload_file():\n    # Both limits must pass. If either is exceeded, 429 is returned.\n    return {\"status\": \"uploaded\"}\n</code></pre> <p>Router-level throttling</p> <p>Apply a throttle to an entire router so all endpoints under it share the same limit:</p> <pre><code>from fastapi import APIRouter, Depends\nfrom traffik import HTTPThrottle\n\nrouter_throttle = HTTPThrottle(\"api:v1:global\", rate=\"1000/min\")\nrouter = APIRouter(prefix=\"/api/v1\", dependencies=[Depends(router_throttle)])\n\n@router.get(\"/users\")\nasync def list_users():\n    return {\"users\": []}\n\n@router.get(\"/products\")\nasync def list_products():\n    return {\"products\": []}\n\n# Both /api/v1/users and /api/v1/products now share the 1000/min limit.\n</code></pre>"},{"location":"quickstart/#step-4-the-decorator-style","title":"Step 4: The Decorator Style","text":"<p>Prefer decorators? Traffik has you covered. For FastAPI routes, use <code>traffik.decorators.throttled</code>, your route doesn't need an explicit <code>Request</code> parameter:</p> <pre><code>from fastapi import FastAPI\nfrom traffik import HTTPThrottle\nfrom traffik.backends.inmemory import InMemoryBackend\nfrom traffik.decorators import throttled  # FastAPI-specific import\n\nbackend = InMemoryBackend(namespace=\"myapp\")\napp = FastAPI(lifespan=backend.lifespan)\n\nburst     = HTTPThrottle(\"search:burst\",     rate=\"5/min\")\nsustained = HTTPThrottle(\"search:sustained\", rate=\"50/hour\")\n\n@app.get(\"/search\")\n@throttled(burst, sustained)  # Apply both throttles\nasync def search():\n    return {\"results\": []}\n</code></pre> <p>Two <code>throttled</code> imports - pick the right one</p> <p>There are two versions of <code>throttled</code>:</p> Import Framework Route needs <code>Request</code>? <code>from traffik.decorators import throttled</code> FastAPI only No <code>from traffik.throttles import throttled</code> Starlette + FastAPI Yes <p>For Starlette routes, use <code>traffik.throttles.throttled</code> and make sure your route function has a <code>Request</code> parameter. For FastAPI, prefer <code>traffik.decorators.throttled</code>, cleaner signatures, no extra parameters.</p>"},{"location":"quickstart/#step-5-a-production-ready-setup","title":"Step 5: A Production-Ready Setup","text":"<p>In production, you'll want Redis for distributed state, a circuit breaker so backend failures don't cascade, and a fallback backend as a safety net. Here's a complete setup:</p> <pre><code># main.py - Production configuration\nfrom contextlib import asynccontextmanager\nfrom fastapi import FastAPI, Request, Depends\nfrom traffik import HTTPThrottle\nfrom traffik.backends.redis import RedisBackend\nfrom traffik.backends.inmemory import InMemoryBackend\nfrom traffik.strategies import SlidingWindowCounter\nfrom traffik.error_handlers import failover, CircuitBreaker\n\n# --- Backends ---\n\n# Primary: Redis (distributed, persistent)\nredis_backend = RedisBackend(\n    connection=\"redis://localhost:6379/0\",\n    namespace=\"myapp:prod\",\n    persistent=True,           # State survives Redis restarts\n    lock_type=\"redis\",         # Single-instance lock (lower latency than redlock)\n    lock_ttl=5.0,              # Auto-release locks after 5 seconds\n    lock_blocking_timeout=2.0, # Give up waiting for a lock after 2 seconds\n)\n\n# Fallback: InMemory (used when Redis is unavailable)\nfallback_backend = InMemoryBackend(namespace=\"myapp:fallback\")\n\n# --- Circuit Breaker ---\n# Opens after 10 failures, attempts recovery after 60 seconds\nbreaker = CircuitBreaker(\n    failure_threshold=10,\n    recovery_timeout=60.0,\n    success_threshold=3,  # Requires 3 successes before fully reopening\n)\n\n# --- Application ---\n\n@asynccontextmanager\nasync def lifespan(app: FastAPI):\n    async with redis_backend(app, persistent=True):\n        await fallback_backend.initialize()\n        yield\n        await fallback_backend.close()\n\napp = FastAPI(lifespan=lifespan)\n\n# --- Throttles ---\n\napi_throttle = HTTPThrottle(\n    \"api:v1\",\n    rate=\"1000/hour\",\n    strategy=SlidingWindowCounter(),  # More accurate than Fixed Window\n    backend=redis_backend,\n    on_error=failover(                 # On Redis error: try the fallback backend\n        backend=fallback_backend,\n        breaker=breaker,\n        max_retries=2,\n    ),\n)\n\n@app.get(\"/api/data\")\nasync def get_data(request: Request = Depends(api_throttle)):\n    return {\"data\": \"value\", \"client\": request.client.host}\n</code></pre> <p>Sliding Window Counter vs Fixed Window</p> <p>The default strategy is <code>FixedWindow</code>, which is fast and memory-efficient. But it allows a burst of 2x your limit at window boundaries (100 requests at 11:59:59 + 100 at 12:00:00 = 200 in two seconds). <code>SlidingWindowCounter</code> smooths this out with minimal overhead. Use it for APIs where burst behavior matters.</p> <p>Lock types matter at scale</p> <p><code>lock_type=\"redis\"</code> uses a single-node Redis lock (~0.4ms overhead). For deployments across multiple Redis clusters, use <code>lock_type=\"redlock\"</code> (Redlock algorithm, ~5ms overhead). Pick based on your topology, not habit.</p>"},{"location":"quickstart/#rate-string-formats","title":"Rate string formats","text":"<p>Traffik understands many natural rate string formats, so you can write what feels readable:</p> <pre><code>HTTPThrottle(\"api:a\", rate=\"100/minute\")    # 100 per minute\nHTTPThrottle(\"api:b\", rate=\"5/10seconds\")   # 5 per 10 seconds\nHTTPThrottle(\"api:c\", rate=\"1000/500ms\")    # 1000 per 500 milliseconds\nHTTPThrottle(\"api:d\", rate=\"20 per 2 mins\") # 20 per 2 minutes\nHTTPThrottle(\"api:e\", rate=\"2 per second\")  # 2 per second\n</code></pre> <p>Or use the <code>Rate</code> object directly for complex periods:</p> <pre><code>from traffik.rates import Rate\n\n# 100 requests per 5 minutes and 30 seconds\nrate = Rate(limit=100, minutes=5, seconds=30)\nHTTPThrottle(\"api:complex\", rate=rate)\n\n# Unlimited (no throttling)\nHTTPThrottle(\"api:open\", rate=\"0/0\")\n</code></pre>"},{"location":"quickstart/#step-6-websocket-rate-limiting","title":"Step 6: WebSocket Rate Limiting","text":"<p>Traffik natively supports WebSocket connections. You can throttle at two levels:</p> <ol> <li>Connection level - Gate the WebSocket handshake (reject new connections when over the limit).</li> <li>Message level - Allow the connection but rate-limit individual messages within it.</li> </ol> <p>Here's a complete WebSocket handler that does both:</p> <pre><code>from fastapi import FastAPI, WebSocket, Depends\nfrom traffik import WebSocketThrottle, is_throttled\nfrom traffik.backends.redis import RedisBackend\n\nbackend = RedisBackend(connection=\"redis://localhost:6379/0\", namespace=\"ws\")\napp = FastAPI(lifespan=backend.lifespan)\n\n# Throttle WebSocket connections: max 10 simultaneous from the same IP\nconnection_throttle = WebSocketThrottle(\"ws:connect\", rate=\"10/min\")\n\n# Throttle messages within a connection: max 60 messages per minute\nmessage_throttle = WebSocketThrottle(\"ws:messages\", rate=\"60/min\")\n\n@app.websocket(\"/ws/chat\")\nasync def chat_endpoint(\n    websocket: WebSocket = Depends(connection_throttle),  # (1)!\n):\n    await websocket.accept()\n\n    close_code = 1000\n    reason = \"Normal closure\"\n\n    try:\n        while True:\n            data = await websocket.receive_json()\n\n            # Check the per-message limit on each incoming frame\n            await message_throttle(websocket)  # (2)!\n\n            if is_throttled(websocket):  # (3)!\n                # Client already got a \"rate_limit\" JSON notification.\n                # Skip processing this message but keep the connection open.\n                continue\n\n            # Process the message normally\n            response = {\"echo\": data, \"type\": \"message\"}\n            await websocket.send_json(response)\n\n    except Exception:\n        close_code = 1011\n        reason = \"Internal error\"\n\n    await websocket.close(code=close_code, reason=reason)\n</code></pre> <ol> <li>Gate the connection itself. If the connection limit is exceeded before the handshake, the connection is rejected with a <code>HTTP/1.1 429 Too Many Requests</code>.</li> <li>Call the message throttle directly on every frame.</li> <li><code>is_throttled(websocket)</code> returns <code>True</code> if the last throttle call rejected the message. The client already received a JSON notification like <code>{\"type\": \"rate_limit\", \"error\": \"Too many messages\", \"retry_after\": 1}</code>.</li> </ol> <p>What the client sees when throttled</p> <p>When a WebSocket message is rate-limited, Traffik sends the client a structured JSON frame instead of silently dropping the message or closing the connection:</p> <pre><code>{\n  \"type\": \"rate_limit\",\n  \"error\": \"Too many messages\",\n  \"retry_after\": 1\n}\n</code></pre> <p>This lets clients back off gracefully. If you want different behavior (e.g., immediately close the connection), provide a custom <code>handle_throttled</code> handler.</p>"},{"location":"quickstart/#custom-identifiers","title":"Custom Identifiers","text":"<p>By default, Traffik identifies clients by their IP address. For authenticated APIs, you almost always want to identify by user or API key instead:</p> <pre><code>from starlette.requests import Request\nfrom traffik import HTTPThrottle, EXEMPTED\n\n# Identify by user ID from a JWT\nasync def user_identifier(request: Request) -&gt; str:\n    # Extract user_id from your auth system\n    user_id = request.state.user_id  # Set by your auth middleware\n    if user_id is None:\n        return request.client.host  # Fall back to IP for anonymous users\n    return f\"user:{user_id}\"\n\n# Identify by API key, and exempt internal services\nasync def api_key_identifier(request: Request) -&gt; str:\n    api_key = request.headers.get(\"x-api-key\", \"\")\n    if api_key.startswith(\"internal-\"):\n        return EXEMPTED  # Internal API keys bypass rate limiting\n    return f\"key:{api_key}\"\n\n# Use the custom identifier\nthrottle = HTTPThrottle(\n    \"api:user-scoped\",\n    rate=\"500/hour\",\n    identifier=user_identifier,\n)\n</code></pre> <p>The <code>EXEMPTED</code> sentinel</p> <p>Returning <code>EXEMPTED</code> from an identifier function completely bypasses throttling for that connection. This is the cleanest way to whitelist specific clients, internal services, or admin users, no special-casing needed in your throttle configuration.</p>"},{"location":"quickstart/#whats-next","title":"What's Next?","text":"<p>You now know the essential patterns. Here's where to go deeper:</p> <ul> <li>Core Concepts - Understand rates, backends, strategies, and identifiers in depth.</li> <li>Integration Guide - Everything about dependencies, decorators, middleware, and direct calls.</li> <li>Advanced Features - Throttle rules, request costs, exemptions, quota contexts, and more.</li> <li>Error Handling - Circuit breakers, failover, and custom error handlers.</li> <li>Testing - How to write tests for throttled endpoints without fighting your own rate limiter.</li> </ul>"},{"location":"testing/","title":"Testing","text":"<p>Testing rate-limited endpoints should be fast, isolated, and deterministic. Traffik makes this easy with <code>InMemoryBackend</code> \u2014 no Redis, no Memcached, no Docker required for unit tests.</p>"},{"location":"testing/#the-golden-rule","title":"The Golden Rule","text":"<p>Use <code>InMemoryBackend</code> with <code>persistent=False</code> for tests. This gives you a backend that:</p> <ul> <li>Starts instantly</li> <li>Has zero external dependencies</li> <li>Resets cleanly between tests</li> <li>Is fast enough to run in tight loops</li> </ul> <pre><code>from traffik.backends.inmemory import InMemoryBackend\n\nbackend = InMemoryBackend(namespace=\"test\", persistent=False)\n</code></pre> <p>Always reset between tests</p> <p>Use a fresh backend per test (via fixture scope) or call <code>await backend.reset()</code> in a teardown. Without this, rate limit state leaks between tests and you'll get mysterious failures. A <code>pytest</code> fixture with <code>scope=\"function\"</code> handles this automatically.</p>"},{"location":"testing/#basic-test-pattern","title":"Basic Test Pattern","text":"<p>Here's the standard setup with <code>pytest</code>, <code>anyio</code>, and <code>httpx.AsyncClient</code>:</p> <pre><code># tests/test_my_endpoints.py\n\nimport pytest\nimport pytest_asyncio\nfrom httpx import AsyncClient, ASGITransport\nfrom fastapi import FastAPI, Request, Depends\nfrom traffik import HTTPThrottle\nfrom traffik.backends.inmemory import InMemoryBackend\n\n\n@pytest.fixture\ndef backend():\n    \"\"\"Fresh backend for each test.\"\"\"\n    return InMemoryBackend(namespace=\"test\", persistent=False)\n\n\n@pytest.fixture\ndef app(backend):\n    \"\"\"FastAPI app with a rate-limited endpoint.\"\"\"\n    throttle = HTTPThrottle(\"test:api\", rate=\"3/min\", backend=backend)\n    app = FastAPI(lifespan=backend.lifespan)\n\n    @app.get(\"/items\")\n    async def get_items(request: Request = Depends(throttle)):\n        return {\"items\": [1, 2, 3]}\n\n    return app\n\n\n@pytest.mark.anyio\nasync def test_allows_requests_under_limit(app):\n    async with AsyncClient(transport=ASGITransport(app=app), base_url=\"http://test\") as client:\n        for _ in range(3):\n            response = await client.get(\"/items\")\n            assert response.status_code == 200\n\n\n@pytest.mark.anyio\nasync def test_rejects_requests_over_limit(app):\n    async with AsyncClient(transport=ASGITransport(app=app), base_url=\"http://test\") as client:\n        # Use up all 3 allowed requests\n        for _ in range(3):\n            await client.get(\"/items\")\n\n        # The 4th should be rejected\n        response = await client.get(\"/items\")\n        assert response.status_code == 429\n</code></pre> <p>Add <code>asyncio_mode = \"auto\"</code> to your <code>pytest.ini</code> or <code>pyproject.toml</code>:</p> <pre><code># pyproject.toml\n[tool.pytest.ini_options]\nasyncio_mode = \"auto\"\n</code></pre>"},{"location":"testing/#strategy-parametrize-pattern","title":"Strategy Parametrize Pattern","text":"<p>Test that your endpoint behaves correctly regardless of which strategy is used:</p> <pre><code>import pytest\nfrom traffik.strategies import (\n    FixedWindow,\n    TokenBucket,\n    SlidingWindowCounter,\n)\n\nSTRATEGIES = [\n    FixedWindow(),\n    TokenBucket(),\n    SlidingWindowCounter(),\n]\n\n\n@pytest.mark.parametrize(\"strategy\", STRATEGIES, ids=[\"fixed\", \"token\", \"sliding\"])\n@pytest.mark.anyio\nasync def test_rate_limit_with_strategy(strategy):\n    backend = InMemoryBackend(namespace=\"test\", persistent=False)\n    throttle = HTTPThrottle(\n        f\"test:strategy:{type(strategy).__name__}\",\n        rate=\"2/min\",\n        backend=backend,\n        strategy=strategy,\n    )\n\n    app = FastAPI(lifespan=backend.lifespan)\n\n    @app.get(\"/data\")\n    async def get_data(request: Request = Depends(throttle)):\n        return {\"ok\": True}\n\n    async with AsyncClient(transport=ASGITransport(app=app), base_url=\"http://test\") as client:\n        assert (await client.get(\"/data\")).status_code == 200\n        assert (await client.get(\"/data\")).status_code == 200\n        assert (await client.get(\"/data\")).status_code == 429\n</code></pre>"},{"location":"testing/#error-handler-test","title":"Error Handler Test","text":"<p>Test that your <code>on_error</code> fallback behaves correctly when the backend fails:</p> <pre><code>import pytest\nfrom unittest.mock import AsyncMock, patch\nfrom traffik.exceptions import BackendError\n\n\n@pytest.mark.anyio\nasync def test_error_handler_fallback():\n    \"\"\"When the primary backend fails, the fallback should kick in.\"\"\"\n    primary = InMemoryBackend(namespace=\"primary\", persistent=False)\n    fallback = InMemoryBackend(namespace=\"fallback\", persistent=False)\n\n    from traffik.error_handlers import backend_fallback\n\n    throttle = HTTPThrottle(\n        \"test:fallback\",\n        rate=\"10/min\",\n        backend=primary,\n        on_error=backend_fallback(backend=fallback, fallback_on=(BackendError,)),\n    )\n\n    app = FastAPI(lifespan=primary.lifespan)\n\n    @app.get(\"/api\")\n    async def api_endpoint(request: Request = Depends(throttle)):\n        return {\"ok\": True}\n\n    # Patch primary backend to raise BackendError\n    with patch.object(primary, \"increment_with_ttl\", side_effect=BackendError(\"Redis down\")):\n        async with AsyncClient(transport=ASGITransport(app=app), base_url=\"http://test\") as client:\n            # Should succeed via fallback backend\n            response = await client.get(\"/api\")\n            assert response.status_code == 200\n\n\n@pytest.mark.anyio\nasync def test_on_error_allow():\n    \"\"\"on_error='allow' should let requests through when backend fails.\"\"\"\n    backend = InMemoryBackend(namespace=\"test\", persistent=False)\n    throttle = HTTPThrottle(\"test:allow\", rate=\"1/min\", backend=backend, on_error=\"allow\")\n\n    app = FastAPI(lifespan=backend.lifespan)\n\n    @app.get(\"/api\")\n    async def api_endpoint(request: Request = Depends(throttle)):\n        return {\"ok\": True}\n\n    with patch.object(backend, \"increment_with_ttl\", side_effect=Exception(\"DB error\")):\n        async with AsyncClient(transport=ASGITransport(app=app), base_url=\"http://test\") as client:\n            response = await client.get(\"/api\")\n            assert response.status_code == 200  # Allowed through despite error\n</code></pre>"},{"location":"testing/#websocket-test-pattern","title":"WebSocket Test Pattern","text":"<pre><code>import pytest\nfrom starlette.testclient import TestClient\nfrom starlette.websockets import WebSocket\nfrom traffik import WebSocketThrottle\nfrom traffik.backends.inmemory import InMemoryBackend\nfrom fastapi import FastAPI\n\n\n@pytest.mark.anyio\nasync def test_websocket_rate_limiting():\n    backend = InMemoryBackend(namespace=\"test\", persistent=False)\n    ws_throttle = WebSocketThrottle(\"test:ws\", rate=\"2/min\", backend=backend)\n\n    app = FastAPI(lifespan=backend.lifespan)\n\n    @app.websocket(\"/ws\")\n    async def websocket_endpoint(websocket: WebSocket):\n        await websocket.accept()\n        while True:\n            data = await websocket.receive_text()\n            await ws_throttle.hit(websocket)  # Throttle each message\n            await websocket.send_text(f\"echo: {data}\")\n\n    client = TestClient(app)\n    with client.websocket_connect(\"/ws\") as ws:\n        ws.send_text(\"hello\")\n        response = ws.receive_text()\n        assert \"echo\" in response\n\n        ws.send_text(\"world\")\n        response = ws.receive_text()\n        assert \"echo\" in response\n\n        # Third message should be rate-limited\n        ws.send_text(\"third\")\n        # WebSocket throttle sends a rate_limit JSON message, not an exception\n        response = ws.receive_json()\n        assert response[\"type\"] == \"rate_limit\"\n</code></pre>"},{"location":"testing/#running-tests-with-docker","title":"Running Tests with Docker","text":"<p>If you want to test against real Redis or Memcached, use the Docker testing script:</p> <pre><code># Run the full test suite (InMemory + Redis + Memcached)\n./docker-test.sh test\n\n# Run fast tests (InMemory only, skips external backends)\n./docker-test.sh test-fast\n\n# Test across multiple Python versions (3.9, 3.10, 3.11, 3.12)\n./docker-test.sh test-matrix\n\n# Start a development environment with Redis running\n./docker-test.sh dev\n</code></pre> <p>The <code>test-fast</code> command skips Redis and Memcached tests by setting <code>SKIP_REDIS_TESTS=true</code> and <code>SKIP_MEMCACHED_TESTS=true</code>. Great for quick iteration during development.</p> <p>CI strategy</p> <p>For your CI pipeline, run <code>make test-fast</code> on every PR (fast, catches logic bugs) and <code>make test</code> on merge to main (comprehensive, catches distributed coordination issues). The full test suite including Redis and Memcached is where race condition correctness tests run.</p>"},{"location":"advanced/","title":"Advanced Features","text":"<p>You know the basics: create a throttle, attach it to a route, done. That gets you surprisingly far, but Traffik has a deeper toolkit for the situations where \"one size fits all\" doesn't quite fit.</p> <p>This section is for when you're ready to graduate from <code>\"100/min\"</code> to something that actually mirrors how your API works in the real world.</p>"},{"location":"advanced/#whats-in-here","title":"What's in Here","text":""},{"location":"advanced/#request-costs","title":"Request Costs","text":"<p>Not every request deserves the same weight on the rate limit scale. A health check is a feather. A file upload is a boulder. Request costs let you reflect that asymmetry directly in how quota is consumed.</p> <pre><code># A bulk export endpoint that counts as 10 requests worth of quota\nthrottle = HTTPThrottle(\"export\", rate=\"100/min\", cost=10)\n</code></pre> <p>Learn about request costs \u2192</p>"},{"location":"advanced/#multiple-rate-limits","title":"Multiple Rate Limits","text":"<p>Production APIs rarely need just one limit. You need a strict burst cap and a generous hourly envelope. Stack throttles to enforce both simultaneously, with a clear failure model.</p> <pre><code>burst  = HTTPThrottle(\"api:burst\",     rate=\"20/min\")\nhourly = HTTPThrottle(\"api:sustained\", rate=\"500/hour\")\n\n# Both must pass \u2014 first failure wins\n@app.get(\"/api/data\", dependencies=[Depends(burst), Depends(hourly)])\nasync def get_data():\n    ...\n</code></pre> <p>Learn about layered throttling \u2192</p>"},{"location":"advanced/#exemptions","title":"Exemptions","text":"<p>Some clients should never be throttled, such as your internal services, premium users, admin tokens, and whitelisted IPs. The <code>EXEMPTED</code> sentinel lets you carve out those exceptions cleanly, with zero overhead: no counter touched, no backend called.</p> <pre><code>from traffik import EXEMPTED\n\nasync def tiered_identifier(request: Request):\n    if request.headers.get(\"x-admin-token\") == ADMIN_TOKEN:\n        return EXEMPTED       # Admin? Go right through.\n    return request.client.host  # Everyone else: throttled by IP.\n</code></pre> <p>Learn about exemptions \u2192</p>"},{"location":"advanced/#context-aware-backends","title":"Context-Aware Backends","text":"<p>Building a multi-tenant SaaS? Different tenants likely need different backends. Enterprise tenants get their own Redis instance, free-tier users share one pool. The <code>dynamic_backend=True</code> flag makes the throttle resolve which backend to use on every request instead of locking in one backend at startup.</p> <pre><code>throttle = HTTPThrottle(\n    \"api:quota\",\n    rate=\"1000/hour\",\n    dynamic_backend=True,   # Resolved per-request from context\n)\n</code></pre> <p>Learn about context-aware backends \u2192</p>"},{"location":"advanced/#when-do-you-need-these","title":"When Do You Need These?","text":"You want to... Feature to use Charge more quota for expensive operations Request Costs Enforce burst + sustained limits together Multiple Rate Limits Let admins or premium users bypass throttling Exemptions Give each tenant isolated rate limit counters Context-Aware Backends <p>You can combine all of these</p> <p>These features compose cleanly. A dynamic-backend throttle can have per-request costs and an identifier that returns <code>EXEMPTED</code> for admin tokens. Stack them as your use case demands.</p>"},{"location":"advanced/context-backends/","title":"Context-Aware Backends (Multi-Tenant)","text":"<p>By default, a throttle picks a backend once \u2014 at startup \u2014 and uses it for every request it ever sees. That's the right call for most apps. But in a multi-tenant SaaS, \"one backend for everyone\" creates a problem: tenant A's traffic eats into tenant B's quota counters, or worse, their limits bleed into each other.</p> <p><code>dynamic_backend=True</code> solves this. Instead of caching a backend at startup, the throttle asks \"which backend should I use right now?\" on every request, reading the answer from the current async context. Your middleware sets the right backend before the request reaches the throttle; the throttle picks it up automatically.</p>"},{"location":"advanced/context-backends/#when-do-you-need-this","title":"When Do You Need This?","text":"Scenario Use <code>dynamic_backend</code>? Single shared backend for all users No \u2014 pass <code>backend=...</code> directly Different Redis instances per customer tier Yes A/B testing different backends per route Yes Swapping backends in tests (nested context managers) Yes One Redis cluster, namespaced by tenant No \u2014 use a custom <code>identifier</code> instead <p>Don't reach for this by default</p> <p><code>dynamic_backend=True</code> adds 1\u201320 ms of overhead per request (one context variable lookup + one <code>initialize()</code> call if the backend hasn't started yet). For simple shared backends, the <code>backend=...</code> parameter is always faster and simpler.</p>"},{"location":"advanced/context-backends/#how-it-works","title":"How It Works","text":"<p>Traffik uses a Python <code>ContextVar</code> to track the \"active backend\" in the current async task. When <code>dynamic_backend=True</code>, the throttle reads from this contextvar on every <code>hit(...)</code> call instead of using a cached reference.</p> <p>Your middleware is responsible for pushing the right backend into context before the route handler runs. The key tool is the backend's async context manager:</p> <pre><code>async with my_backend(app, close_on_exit=False, persistent=True):\n    # The ContextVar now points to `my_backend` for the duration of this block.\n    # Any dynamic-backend throttle called here will use it.\n    response = await call_next(request)\n</code></pre> <p>The <code>close_on_exit=False</code> flag is important in middleware: you don't want to close and reset the backend connection after every single request. <code>persistent=True</code> means the backend's stored data survives the context exit (the counters aren't wiped).</p>"},{"location":"advanced/context-backends/#setting-up-a-dynamic-throttle","title":"Setting Up a Dynamic Throttle","text":"<p>Creating a dynamic throttle is the same as a normal throttle, except you omit <code>backend</code> and set <code>dynamic_backend=True</code>. Traffik will raise a <code>ValueError</code> if you provide both.</p> <pre><code>from traffik import HTTPThrottle\n\napi_throttle = HTTPThrottle(\n    uid=\"api:quota\",\n    rate=\"1000/hour\",\n    dynamic_backend=True,    # Resolve backend from context at request time\n    identifier=get_user_id,  # Still identify clients the usual way\n)\n\n# api_throttle.backend is None \u2014 that's expected and correct.\n</code></pre>"},{"location":"advanced/context-backends/#full-multi-tenant-example","title":"Full Multi-Tenant Example","text":"<p>This example models a SaaS with three tiers:</p> <ul> <li>Enterprise: dedicated Redis instance, isolated completely.</li> <li>Premium: shared Redis pool, separate from the free tier.</li> <li>Free: in-memory backend, sufficient for lower traffic.</li> </ul> <p>The middleware reads the tenant's tier from the <code>Authorization</code> header and activates the corresponding backend for the duration of that request.</p> <pre><code>from contextlib import asynccontextmanager\nfrom fastapi import Depends, FastAPI, Request\nfrom starlette.middleware.base import BaseHTTPMiddleware\nfrom traffik import HTTPThrottle\nfrom traffik.backends.inmemory import InMemoryBackend\nfrom traffik.backends.redis import RedisBackend\n\n# One throttle, three different backends depending on tenant tier\napi_throttle = HTTPThrottle(\n    uid=\"api:quota\",\n    rate=\"1000/hour\",\n    dynamic_backend=True,\n)\n\n# Backend instances \u2014 created once at module level, NOT per-request\nenterprise_backend = RedisBackend(\n    \"redis://enterprise-redis:6379\",\n    namespace=\"enterprise\",\n    persistent=True,\n)\npremium_backend = RedisBackend(\n    \"redis://shared-redis:6379\",\n    namespace=\"premium\",\n    persistent=True,\n)\nfree_backend = InMemoryBackend(persistent=True)\n\n\n@asynccontextmanager\nasync def lifespan(app: FastAPI):\n    # Start all backends at application startup\n    async with (\n        enterprise_backend(app, close_on_exit=False, persistent=True),\n        premium_backend(app, close_on_exit=False, persistent=True),\n        free_backend(app, close_on_exit=False, persistent=True),\n    ):\n        yield\n    # All backends close cleanly when the app shuts down\n\n\nclass TenantRoutingMiddleware(BaseHTTPMiddleware):\n    async def dispatch(self, request: Request, call_next):\n        auth = request.headers.get(\"authorization\", \"\")\n        tier = decode_tenant_tier(auth)  # Your JWT/token logic here\n\n        if tier == \"enterprise\":\n            async with enterprise_backend(close_on_exit=False, persistent=True):\n                return await call_next(request)\n\n        elif tier == \"premium\":\n            async with premium_backend(close_on_exit=False, persistent=True):\n                return await call_next(request)\n\n        else:\n            # Free tier: in-memory (or a shared Redis with tight limits)\n            async with free_backend(close_on_exit=False, persistent=True):\n                return await call_next(request)\n\n\napp = FastAPI(lifespan=lifespan)\napp.add_middleware(TenantRoutingMiddleware)  # type: ignore[arg-type]\n\n\n@app.get(\"/api/data\", dependencies=[Depends(api_throttle)])\nasync def get_data():\n    return {\"data\": \"...\"}\n</code></pre> <p>When an enterprise request arrives, the middleware wraps <code>call_next</code> in <code>enterprise_backend(...)</code>. The throttle calls <code>get_backend(connection)</code>, finds <code>enterprise_backend</code> in the ContextVar, and uses it \u2014 all without any modification to the route handler.</p>"},{"location":"advanced/context-backends/#the-backendapp-close_on_exitfalse-persistenttrue-pattern","title":"The <code>backend(app, close_on_exit=False, persistent=True)</code> Pattern","text":"<p>You'll see this pattern everywhere in dynamic backend code. Here's what each parameter does:</p> Parameter Value Meaning <code>app</code> Your FastAPI/Starlette app Registers the backend on <code>app.state</code> so it's also available to non-async code <code>close_on_exit</code> <code>False</code> Don't close the Redis connection after each request \u2014 that would be catastrophic <code>persistent</code> <code>True</code> Don't wipe the rate limit counters on context exit \u2014 they need to persist between requests <p>Never use <code>persistent=False</code> with middleware</p> <p>Without <code>persistent=True</code>, every request would clear the rate limit counters for that backend on exit. Your throttle would effectively be disabled \u2014 every client would start fresh on every request.</p>"},{"location":"advanced/context-backends/#testing-with-nested-contexts","title":"Testing with Nested Contexts","text":"<p><code>dynamic_backend=True</code> also makes testing much cleaner. You can nest context managers to switch backends mid-test without creating multiple throttle instances.</p> <pre><code>import pytest\nfrom starlette.requests import Request\nfrom traffik import HTTPThrottle\nfrom traffik.backends.inmemory import InMemoryBackend\n\n@pytest.mark.asyncio\nasync def test_different_backends_isolated():\n    throttle = HTTPThrottle(\n        uid=\"test:dynamic\",\n        rate=\"2/min\",\n        dynamic_backend=True,\n    )\n\n    backend_a = InMemoryBackend(persistent=True)\n    backend_b = InMemoryBackend(persistent=True)\n\n    request = make_dummy_request()  # Helper that builds a Request object\n\n    async with backend_a():\n        await throttle(request)  # Uses backend_a \u2014 hit 1\n        await throttle(request)  # Uses backend_a \u2014 hit 2\n\n        async with backend_b():\n            # Inner context switches to backend_b \u2014 fresh counter\n            await throttle(request)  # Uses backend_b \u2014 hit 1 (not throttled!)\n            await throttle(request)  # Uses backend_b \u2014 hit 2\n\n        # Back to backend_a \u2014 counter is still at 2\n        # Third hit in backend_a will be throttled\n</code></pre> <p>This is the main reason <code>dynamic_backend</code> was designed \u2014 it makes integration tests with per-test isolation trivial.</p>"},{"location":"advanced/context-backends/#performance-impact","title":"Performance Impact","text":"Operation Overhead Static backend (normal <code>backend=...</code>) ~0 ms \u2014 cached attribute lookup Dynamic backend (ContextVar read) 1\u20135 ms typical, up to 20 ms under contention <p>The overhead comes from reading the <code>ContextVar</code> and potentially calling <code>initialize()</code> if the backend hasn't been set up yet for that context. This is negligible for most APIs, but worth knowing if you're optimizing a hot path with sub-10ms latency requirements.</p> <p>If you only have one backend, don't use <code>dynamic_backend</code></p> <p>The overhead of dynamic backend resolution is only justified when you genuinely need different backends for different requests. If you're running a single Redis cluster and just want to namespace keys per tenant, a custom <code>identifier</code> function is the right tool:</p> <pre><code>async def tenant_identifier(request: Request):\n    tenant_id = extract_tenant_id(request)\n    client_ip = request.client.host\n    return f\"{tenant_id}:{client_ip}\"\n\n# One backend, but counters are automatically isolated per tenant\nthrottle = HTTPThrottle(\n    uid=\"api:quota\",\n    rate=\"1000/hour\",\n    backend=shared_redis_backend,\n    identifier=tenant_identifier,\n)\n</code></pre>"},{"location":"advanced/context-backends/#anti-patterns","title":"Anti-Patterns","text":"<p>Don't create backends inside the middleware <code>dispatch</code> method</p> <p>Creating a new backend object on every request is expensive and defeats connection pooling. Create backend instances once at module level (or in lifespan), then activate them in middleware via their context manager.</p> <pre><code># Bad: new backend object every request\nclass BadMiddleware(BaseHTTPMiddleware):\n    async def dispatch(self, request, call_next):\n        backend = RedisBackend(\"redis://...\", namespace=\"app\")  # Don't do this\n        async with backend():\n            return await call_next(request)\n\n# Good: backend created once, reused across requests\n_backend = RedisBackend(\"redis://...\", namespace=\"app\", persistent=True)\n\nclass GoodMiddleware(BaseHTTPMiddleware):\n    async def dispatch(self, request, call_next):\n        async with _backend(close_on_exit=False, persistent=True):\n            return await call_next(request)\n</code></pre> <p>Don't mix <code>dynamic_backend=True</code> with an explicit <code>backend=...</code></p> <p>Traffik raises a <code>ValueError</code> at construction time if you try:</p> <pre><code># This raises ValueError\nthrottle = HTTPThrottle(\n    uid=\"bad\",\n    rate=\"100/min\",\n    dynamic_backend=True,\n    backend=my_backend,   # Can't combine these\n)\n</code></pre>"},{"location":"advanced/exemptions/","title":"Exemptions","text":"<p>Some clients should never hit a rate limit. Your internal health check service. Your admin dashboard. A premium user on a contract with a dedicated SLA. A whitelisted IP for a partner integration.</p> <p>Traffik handles this cleanly through the <code>EXEMPTED</code> sentinel. When your identifier function returns <code>EXEMPTED</code>, Traffik short-circuits the entire throttle pipeline: no counter is incremented, no backend is consulted, no lock is acquired. The request passes through as if the throttle wasn't there.</p>"},{"location":"advanced/exemptions/#the-exempted-sentinel","title":"The <code>EXEMPTED</code> Sentinel","text":"<p><code>EXEMPTED</code> is a special singleton value exported from <code>traffik</code>:</p> <pre><code>from traffik import EXEMPTED\n</code></pre> <p>Return it from your identifier function to exempt a connection from throttling:</p> <pre><code>from traffik import EXEMPTED, HTTPThrottle\nfrom starlette.requests import Request\n\nasync def my_identifier(request: Request):\n    if some_condition(request):\n        return EXEMPTED  # This connection is exempt\n    return request.client.host  # Everyone else: throttled by IP\n</code></pre> <p>Zero overhead</p> <p>When <code>EXEMPTED</code> is returned, Traffik exits immediately. No backend call is made, no lock is acquired. It's an extremely efficient way to bypass throttling. Note that <code>cost=0</code> is actually checked before the identifier, so a zero-cost request never even reaches the identifier stage \u2014 but <code>EXEMPTED</code> skips everything that follows the identifier, including the backend lookup.</p>"},{"location":"advanced/exemptions/#how-it-works","title":"How It Works","text":"<p>Traffik's <code>hit(...)</code> method evaluates exemptions in a specific order:</p> <ol> <li>Cost check \u2014 if <code>cost=0</code> is resolved (e.g., via a dynamic cost function returning <code>0</code>), the hit is a no-op: no backend call, no counter increment. The connection passes through.</li> <li>Identifier check \u2014 the identifier function is called. If the result is <code>EXEMPTED</code> \u2014 stop. Return the connection untouched.</li> <li>Otherwise, proceed with backend lookup and counter increment.</li> </ol> <p>Step 2 is the key short-circuit: the entire downstream machinery simply never runs when <code>EXEMPTED</code> is returned. Note that <code>cost=0</code> is checked first (step 1) \u2014 this means a zero-cost hit never even reaches the identifier stage.</p>"},{"location":"advanced/exemptions/#common-exemption-patterns","title":"Common Exemption Patterns","text":""},{"location":"advanced/exemptions/#exempt-by-token-admin-access","title":"Exempt by Token (Admin Access)","text":"<p>Check a header for an admin token. Admins get through unconditionally; everyone else is throttled by user ID.</p> <pre><code>from traffik import EXEMPTED, HTTPThrottle, default_identifier\nfrom starlette.requests import Request\n\nADMIN_SECRET = \"super-secret-admin-token\"  # Load from env in practice\n\nasync def admin_aware_identifier(request: Request):\n    if request.headers.get(\"x-admin-token\") == ADMIN_SECRET:\n        return EXEMPTED\n    # Fall back to the default (remote IP address)\n    return await default_identifier(request)\n\nthrottle = HTTPThrottle(\n    uid=\"api:public\",\n    rate=\"100/min\",\n    identifier=admin_aware_identifier,\n)\n</code></pre>"},{"location":"advanced/exemptions/#exempt-by-user-tier-premium-vs-free","title":"Exempt by User Tier (Premium vs Free)","text":"<p>Load the user from your database (or a JWT claim) and branch on their tier:</p> <pre><code>from traffik import EXEMPTED, HTTPThrottle\nfrom starlette.requests import Request\n\nasync def tiered_identifier(request: Request):\n    user = getattr(request.state, \"user\", None)\n\n    if user is None:\n        # Unauthenticated \u2014 throttle by IP\n        return request.client.host or \"anonymous\"\n\n    if user.tier == \"enterprise\":\n        return EXEMPTED          # Enterprise: no limit\n\n    if user.tier == \"premium\":\n        return f\"premium:{user.id}\"   # Premium: separate (generous) bucket\n\n    return f\"free:{user.id}\"          # Free: the default (strict) bucket\n\nthrottle = HTTPThrottle(\n    uid=\"api:tiered\",\n    rate=\"60/min\",   # Applies to free and premium users; enterprise skips entirely\n    identifier=tiered_identifier,\n)\n</code></pre>"},{"location":"advanced/exemptions/#exempt-by-ip-allowlist-internal-services","title":"Exempt by IP Allowlist (Internal Services)","text":"<p>Internal services that call your API from known IP ranges shouldn't consume quota:</p> <pre><code>from traffik import EXEMPTED, HTTPThrottle\nfrom starlette.requests import Request\nimport ipaddress\n\nINTERNAL_NETWORKS = [\n    ipaddress.ip_network(\"10.0.0.0/8\"),\n    ipaddress.ip_network(\"172.16.0.0/12\"),\n    ipaddress.ip_network(\"192.168.0.0/16\"),\n]\n\ndef is_internal_ip(ip_str: str) -&gt; bool:\n    try:\n        ip = ipaddress.ip_address(ip_str)\n        return any(ip in net for net in INTERNAL_NETWORKS)\n    except ValueError:\n        return False\n\nasync def allowlist_identifier(request: Request):\n    client_ip = request.client.host or \"\"\n    if is_internal_ip(client_ip):\n        return EXEMPTED\n    return client_ip\n\ninternal_throttle = HTTPThrottle(\n    uid=\"api:external-only\",\n    rate=\"200/min\",\n    identifier=allowlist_identifier,\n)\n</code></pre>"},{"location":"advanced/exemptions/#a-full-tiered-example","title":"A Full Tiered Example","text":"<p>This identifier combines all three approaches: admin tokens, tier-based logic, and a fallback to IP for unauthenticated users.</p> <pre><code>from traffik import EXEMPTED, HTTPThrottle, default_identifier\nfrom starlette.requests import Request\n\nasync def smart_identifier(request: Request):\n    # 1. Internal admin token? Exempt entirely.\n    if request.headers.get(\"x-admin-token\") == ADMIN_TOKEN:\n        return EXEMPTED\n\n    user = getattr(request.state, \"user\", None)\n\n    # 2. No authenticated user? Fall back to IP.\n    if user is None:\n        return await default_identifier(request)\n\n    # 3. Enterprise tier? Exempt.\n    if user.plan == \"enterprise\":\n        return EXEMPTED\n\n    # 4. Everyone else: throttle by user ID.\n    return f\"user:{user.id}\"\n\napi_throttle = HTTPThrottle(\n    uid=\"api:main\",\n    rate=\"100/min\",\n    identifier=smart_identifier,\n)\n</code></pre>"},{"location":"advanced/exemptions/#alternative-exemption-mechanisms","title":"Alternative Exemption Mechanisms","text":"<p><code>EXEMPTED</code> is the cleanest way to skip throttling for a specific client, but it's not the only tool in the box. Two lighter-weight alternatives are worth knowing:</p>"},{"location":"advanced/exemptions/#cost0-silent-pass-through","title":"<code>cost=0</code> \u2014 Silent Pass-Through","text":"<p>Setting <code>cost=0</code> on a throttle call (or returning <code>0</code> from a dynamic cost function) causes Traffik to skip the entire backend operation for that hit \u2014 no counter is incremented, no lock is acquired. The check happens before the identifier is even called, making it the cheapest possible path through the throttle:</p> <pre><code>from traffik import HTTPThrottle\n\nasync def dynamic_cost(connection, context=None) -&gt; int:\n    # Internal health check probes carry no cost\n    if connection.headers.get(\"x-internal-probe\") == \"1\":\n        return 0\n    return 1\n\nthrottle = HTTPThrottle(uid=\"api:main\", rate=\"100/min\", cost=dynamic_cost)\n</code></pre> <p>This is more efficient than <code>EXEMPTED</code> for cases where you don't need to identify the client at all \u2014 the cost check short-circuits before the identifier runs.</p>"},{"location":"advanced/exemptions/#very-low-rate-soft-throttle-soft-allow","title":"Very Low <code>rate</code> \u2014 Soft Throttle / Soft Allow","text":"<p>Setting a very high limit (e.g. <code>rate=\"1000000/min\"</code>) effectively gives a client unlimited capacity without fully exempting them. They still go through the full throttle pipeline and still consume quota \u2014 just at a rate that will never realistically be hit. This is useful when you want throttle telemetry (statistics, headers) for a client but don't want to restrict them:</p> <pre><code>from traffik import HTTPThrottle, Rate\n\nasync def effective_rate(connection, context=None) -&gt; Rate:\n    user = getattr(connection.state, \"user\", None)\n    if user and user.tier == \"enterprise\":\n        return Rate.parse(\"1000000/min\")  # Effectively unlimited, but tracked\n    return Rate.parse(\"100/min\")\n\nthrottle = HTTPThrottle(uid=\"api:tiered\", rate=effective_rate)\n</code></pre> <p>Which to reach for?</p> <ul> <li>Use <code>EXEMPTED</code> when a client should produce zero backend activity \u2014 health checks, internal services, admin tokens.</li> <li>Use <code>cost=0</code> when you want to suppress quota consumption based on request content (headers, payload type) rather than client identity.</li> <li>Use a very high <code>rate</code> when you want the client tracked in telemetry but never blocked.</li> </ul>"},{"location":"advanced/exemptions/#exempted-vs-bypassthrottlerule","title":"<code>EXEMPTED</code> vs <code>BypassThrottleRule</code>","text":"<p>Traffik offers two ways to skip throttling, and they're designed for different situations:</p> <code>EXEMPTED</code> (from identifier) <code>BypassThrottleRule</code> Decision based on Who the client is (user, IP, token) What the request is (path, method) Configured on The <code>identifier</code> function The throttle's <code>rules</code> parameter Overhead Near zero \u2014 stops at identifier stage Near zero \u2014 stops at rule check stage Use for Trusted clients, premium users, admin tokens Paths or methods that should never be throttled <p>Path-based exemptions: use <code>BypassThrottleRule</code></p> <p>If you want to skip throttling for <code>GET /health</code> or <code>GET /metrics</code>, you should use <code>BypassThrottleRule</code> rather than encoding path logic in your identifier:</p> <pre><code>from traffik.registry import BypassThrottleRule\n\nthrottle = HTTPThrottle(\n    uid=\"api:main\",\n    rate=\"100/min\",\n    rules={\n        BypassThrottleRule(path=\"/health\", methods={\"GET\"}),\n        BypassThrottleRule(path=\"/metrics\", methods={\"GET\"}),\n    },\n)\n</code></pre> <p>This keeps your identifier focused on who the client is, and your rules focused on what the request is.</p>"},{"location":"advanced/exemptions/#important-exempted-affects-all-throttles-using-that-identifier","title":"Important: <code>EXEMPTED</code> Affects All Throttles Using That Identifier","text":"<p>When an identifier returns <code>EXEMPTED</code>, that exemption applies to the specific throttle whose identifier is being called \u2014 not globally across all throttles.</p> <p>If you have two throttles that share the same identifier function, and that function returns <code>EXEMPTED</code> for a given client, both throttles exempt that client. This is usually what you want: a true VIP client skips every limit.</p> <p>If you want selective exemptions \u2014 skip throttle A but not throttle B for the same client \u2014 use separate identifier functions, or use <code>BypassThrottleRule</code> on the specific throttle you want to bypass.</p> <pre><code># Throttle A: exempts admins entirely\nthrottle_a = HTTPThrottle(\"api:general\", rate=\"100/min\", identifier=admin_aware_id)\n\n# Throttle B: admins still subject to this limit (audit trail, etc.)\nthrottle_b = HTTPThrottle(\"api:audit\",   rate=\"1000/min\", identifier=default_identifier)\n</code></pre>"},{"location":"advanced/headers/","title":"Response Headers","text":"<p>Rate limiting is half the battle. The other half is telling clients how they're doing before they slam into a wall.</p> <p>Without response headers, your clients are flying blind - they have no idea how many requests they have left, when their window resets, or why they suddenly got a <code>429</code>. With the right headers, a well-behaved client can slow itself down gracefully, show users a meaningful \"try again in X seconds\" message, and stop hammering your API unnecessarily.</p> <p>This page covers the <code>Header</code> and <code>Headers</code> system that Traffik uses to resolve rate limit header values. Traffik does not automatically inject headers into responses, it computes the values and returns them. It's up to you to attach them to your response however you see fit (e.g., via middleware, a custom dependency, or a Starlette response). The <code>headers=</code> parameter on a throttle tells Traffik which headers to resolve, not which ones to auto-inject.</p>"},{"location":"advanced/headers/#why-headers-matter","title":"Why Headers Matter","text":"<p>Standard rate limit headers have become a de facto convention - GitHub, Stripe, Cloudflare, and most large APIs all use some variation of these three:</p> Header Meaning <code>X-RateLimit-Limit</code> Maximum requests allowed in the current window <code>X-RateLimit-Remaining</code> Requests left in the current window <code>Retry-After</code> Seconds to wait before trying again (RFC 6585) <p>A client that reads these headers can back off gracefully, show users a countdown timer, or queue requests intelligently. A client that doesn't get them just crashes and retries until you block it.</p> <p>Traffik lets you attach these headers to every response, or only to throttled responses, whichever fits your API contract.</p>"},{"location":"advanced/headers/#built-in-header-presets","title":"Built-in Header Presets","text":"<p>For most use cases, you don't need to build your own headers from scratch. Traffik ships two ready-made presets, available from both <code>traffik</code> and <code>traffik.headers</code>:</p> <pre><code>from traffik.headers import DEFAULT_HEADERS_ALWAYS, DEFAULT_HEADERS_THROTTLED\n# or equivalently:\n# from traffik import DEFAULT_HEADERS_ALWAYS, DEFAULT_HEADERS_THROTTLED\n</code></pre> <p><code>DEFAULT_HEADERS_ALWAYS</code> - Sends <code>X-RateLimit-Limit</code>, <code>X-RateLimit-Remaining</code>, and <code>Retry-After</code> on every response. Clients can always see their current status.</p> <p><code>DEFAULT_HEADERS_THROTTLED</code> - Sends the same three headers only when a client is throttled (i.e., receives a <code>429</code>). Quieter by default; headers appear only when they matter most.</p> <p>Which should I use?</p> <p><code>DEFAULT_HEADERS_ALWAYS</code> is friendlier for client developers, they can poll their remaining quota without making a special endpoint for it. <code>DEFAULT_HEADERS_THROTTLED</code> keeps response size smaller on normal requests and is common in APIs where clients are expected to track their own usage. GitHub uses \"always\"; many simpler APIs use \"throttled\".</p>"},{"location":"advanced/headers/#attaching-headers-to-a-throttle","title":"Attaching Headers to a Throttle","text":"<p>Pass a <code>headers</code> argument when constructing any throttle:</p> <pre><code>from traffik import HTTPThrottle\nfrom traffik.headers import DEFAULT_HEADERS_ALWAYS\n\nthrottle = HTTPThrottle(\n    \"api:items\",\n    rate=\"100/min\",\n    headers=DEFAULT_HEADERS_ALWAYS,\n)\n</code></pre> <p>The <code>headers=</code> parameter tells Traffik which headers to resolve on a hit. On every hit (throttled or not, depending on the preset), the header values are computed from the current strategy statistics and returned by <code>throttle.get_headers()</code>. You are responsible for attaching them to your response, for example by calling <code>response.headers.update(resolved)</code> in a custom handler or middleware.</p>"},{"location":"advanced/headers/#the-header-class","title":"The <code>Header</code> Class","text":"<p>Internally, each header in a <code>Headers</code> collection is a <code>Header</code> instance, a small, immutable object that knows when to include itself and how to compute its value.</p> <pre><code>from traffik.headers import Header\n</code></pre>"},{"location":"advanced/headers/#built-in-header-builders","title":"Built-in Header Builders","text":"<p><code>Header</code> exposes four class methods for the standard rate limit headers:</p> <pre><code>Header.LIMIT(when=...)          # X-RateLimit-Limit: max hits per window\nHeader.REMAINING(when=...)      # X-RateLimit-Remaining: hits left this window\nHeader.RESET_SECONDS(when=...)  # Retry-After: seconds until window resets\nHeader.RESET_MILLISECONDS(when=...)  # X-RateLimit-Reset-Ms: milliseconds until reset\n</code></pre> <p>Each returns a <code>Header</code> instance that resolves its value at request time from the live strategy statistics. The <code>when</code> parameter is required, it controls when the header appears in the response.</p> <p>Why is <code>when</code> required on class methods?</p> <p>Traffik makes the inclusion condition explicit rather than defaulting silently. Forgetting a <code>when</code> value on a header builder would be a common footgun: should a <code>Retry-After</code> header appear on every <code>200</code> response? Probably not. Making it explicit keeps the contract clear.</p>"},{"location":"advanced/headers/#header-conditions-whenalways-vs-whenthrottled","title":"Header Conditions: <code>when=\"always\"</code> vs <code>when=\"throttled\"</code>","text":"<p>The <code>when</code> parameter on every <code>Header</code> controls its inclusion logic:</p> <pre><code># Include this header on every response, throttled or not\nHeader.REMAINING(when=\"always\")\n\n# Include this header only when the client is throttled (hits_remaining &lt;= 0)\nHeader.REMAINING(when=\"throttled\")\n</code></pre> <p>You can also pass a custom callable for more nuanced conditions:</p> <pre><code>def warn_when_low(connection, stat, context):\n    \"\"\"Include the header when the client is below 10% quota remaining.\"\"\"\n    return stat.hits_remaining &lt; (stat.rate.limit * 0.1)\n\nHeader.REMAINING(when=warn_when_low)\n</code></pre> <p>The callable receives three arguments:</p> <ul> <li><code>connection</code> - the current <code>Request</code> or <code>WebSocket</code></li> <li><code>stat</code> - the <code>StrategyStat</code> object with <code>.hits_remaining</code>, <code>.rate</code>, <code>.wait_ms</code>, etc.</li> <li><code>context</code> - the throttle context dict, or <code>None</code></li> </ul> <p>It should return <code>True</code> to include the header, <code>False</code> to skip it.</p>"},{"location":"advanced/headers/#chaining-with-when","title":"Chaining with <code>.when()</code>","text":"<p>If you have an existing <code>Header</code> instance and want a copy with a different condition, use the <code>.when()</code> method or the convenience properties:</p> <pre><code>base = Header.REMAINING(when=\"always\")\n\n# These are all equivalent\nthrottled_only = base.when(\"throttled\")\nthrottled_only = base.throttled\n\nalways_on = base.when(\"always\")\nalways_on = base.always\n</code></pre>"},{"location":"advanced/headers/#custom-header-values","title":"Custom Header Values","text":""},{"location":"advanced/headers/#static-values","title":"Static values","text":"<p>For a header whose value never changes, just pass a string:</p> <pre><code>from traffik.headers import Header\n\n# A static policy string (useful for RFC 9440 RateLimit-Policy header)\npolicy_header = Header(\"100;w=60\")\n</code></pre>"},{"location":"advanced/headers/#dynamic-values-via-resolver-function","title":"Dynamic values via resolver function","text":"<p>For a value computed at request time, pass a callable with signature <code>(connection, stat, context) -&gt; str</code>:</p> <pre><code>from traffik.headers import Header\n\ndef my_resolver(connection, stat, context):\n    if stat:\n        return str(int(stat.hits_remaining))\n    return \"unknown\"\n\ncustom_dynamic = Header(my_resolver, when=\"always\")\n</code></pre> <p>The resolver is called on every request where the <code>when</code> condition passes. Keep it fast, it's on the hot path.</p>"},{"location":"advanced/headers/#building-a-headers-collection","title":"Building a <code>Headers</code> Collection","text":"<p><code>Headers</code> is a mapping of header name strings to <code>Header</code> instances (or raw strings for static values). It's the recommended way to package headers for a throttle:</p> <pre><code>from traffik import HTTPThrottle\nfrom traffik.headers import Headers, Header\n\nmy_headers = Headers({\n    \"X-RateLimit-Limit\": Header.LIMIT(when=\"always\"),\n    \"X-RateLimit-Remaining\": Header.REMAINING(when=\"always\"),\n    \"Retry-After\": Header.RESET_SECONDS(when=\"throttled\"),\n    \"RateLimit-Policy\": Header(\"100;w=60\"),  # (1)!\n})\n\nthrottle = HTTPThrottle(\"api:items\", rate=\"100/min\", headers=my_headers)\n</code></pre> <ol> <li>A static string is perfectly valid here - <code>Headers</code> will recognize it as always-included and optimise resolution accordingly.</li> </ol> <p>Performance note</p> <p><code>Headers</code> optimises resolution. If all headers in a collection are static strings with <code>when=\"always\"</code>, Traffik pre-encodes them once and skips the resolution step entirely on every request. The more static headers you have, the cheaper header resolution gets.</p>"},{"location":"advanced/headers/#merging-headers","title":"Merging Headers","text":"<p><code>Headers</code> supports the <code>|</code> operator for creating merged copies, and <code>|=</code> for in-place merging. This is useful when you want to extend a preset with extra headers:</p> <pre><code>from traffik.headers import Headers, Header, DEFAULT_HEADERS_ALWAYS\n\nextra = Headers({\n    \"X-Service-Name\": Header(\"my-api\"),\n    \"X-RateLimit-Reset-Ms\": Header.RESET_MILLISECONDS(when=\"always\"),\n})\n\n# Create a merged copy - DEFAULT_HEADERS_ALWAYS is not mutated\ncombined = DEFAULT_HEADERS_ALWAYS | extra\n</code></pre> <p>You can also <code>.copy()</code> a <code>Headers</code> instance for a shallow duplicate:</p> <pre><code>from traffik.headers import DEFAULT_HEADERS_ALWAYS, Header\n\nmy_copy = DEFAULT_HEADERS_ALWAYS.copy()\nmy_copy[\"X-Service-Name\"] = Header(\"my-api\")\n</code></pre>"},{"location":"advanced/headers/#disabling-a-header-per-request","title":"Disabling a Header Per-Request","text":"<p>Sometimes you want to suppress a specific header for a particular call, for example you have a global <code>DEFAULT_HEADERS_ALWAYS</code> on a throttle but want to hide <code>X-RateLimit-Remaining</code> on one specific endpoint.</p> <p>Use <code>Header.DISABLE</code> as a sentinel value in an override mapping passed to <code>throttle.get_headers()</code>:</p> <pre><code>from traffik.headers import Header\n\n# Resolve headers, but skip X-RateLimit-Remaining for this call\nresolved = await throttle.get_headers(\n    connection,\n    headers={\"X-RateLimit-Remaining\": Header.DISABLE},\n)\n</code></pre> <p><code>Header.DISABLE</code> is a string sentinel (<code>\":___disabled___:\"</code>). The resolver checks identity (<code>is</code>), not equality, so a plain string with the same content would not trigger the disable logic.</p> <p>Identity matters</p> <p>Always use <code>Header.DISABLE</code> directly. A variable holding the same string value won't work because Python string interning doesn't guarantee identity for all strings.</p>"},{"location":"advanced/headers/#real-world-example-githubstripe-style-headers","title":"Real-World Example: GitHub/Stripe-Style Headers","text":"<p>Here's a complete setup that matches the header pattern used by most large REST APIs, with headers resolved on every hit plus a millisecond-precision reset time for clients that need it:</p> <pre><code>from fastapi import FastAPI, Depends, Request, Response\nfrom traffik import HTTPThrottle\nfrom traffik.backends.inmemory import InMemoryBackend\nfrom traffik.headers import Headers, Header\n\n# --- Build a custom header collection ---\napi_headers = Headers({\n    # Core rate limit headers - always present\n    \"X-RateLimit-Limit\": Header.LIMIT(when=\"always\"),\n    \"X-RateLimit-Remaining\": Header.REMAINING(when=\"always\"),\n\n    # Retry-After only on 429 (RFC 6585 compliance)\n    \"Retry-After\": Header.RESET_SECONDS(when=\"throttled\"),\n\n    # Millisecond-precision reset for clients that want precision  (1)!\n    \"X-RateLimit-Reset-Ms\": Header.RESET_MILLISECONDS(when=\"always\"),\n\n    # A static policy descriptor (RFC 9440)\n    \"RateLimit-Policy\": Header(\"1000;w=60\"),\n})\n\n# --- Wire everything up ---\nbackend = InMemoryBackend(namespace=\"myapi\")\napp = FastAPI(lifespan=backend.lifespan)\n\napi_throttle = HTTPThrottle(\n    \"api:v1\",\n    rate=\"1000/min\",\n    headers=api_headers,\n)\n\n@app.get(\"/items\")\nasync def list_items(\n    request: Request,\n    response: Response,  # (2)!\n    _=Depends(api_throttle),\n):\n    # Resolve headers for the current request and attach them to the response.\n    # get_headers() fetches live stats from the backend automatically.\n    headers = await api_throttle.get_headers(request)  # (3)!\n    response.headers.update(headers)\n    return {\"items\": [\"widget\", \"gizmo\"]}\n</code></pre> <ol> <li>Some clients (JavaScript frontends, mobile apps) find millisecond precision more useful for scheduling retries than whole seconds.</li> <li>FastAPI injects a mutable <code>Response</code> object. Headers set on it are merged into the final response automatically.</li> <li><code>get_headers()</code> calls <code>throttle.stat()</code> internally to fetch the current rate limit statistics from the backend, then resolves each header's value. You don't need to pass <code>stat</code> manually.</li> </ol> <p>Using <code>JSONResponse</code> directly</p> <p>If you prefer to build the response yourself, pass <code>headers=</code> to <code>JSONResponse</code>: <pre><code>from fastapi.responses import JSONResponse\n@app.get(\"/items\")\nasync def list_items(request: Request, _=Depends(api_throttle)):\n    headers = await api_throttle.get_headers(request)\n    return JSONResponse({\"items\": [\"widget\", \"gizmo\"]}, headers=headers)\n</code></pre></p> <p>Headers on throttled responses</p> <p>When a client is throttled (HTTP 429), the throttle raises an exception before your handler runs. To attach headers to 429 responses, add them in a custom exception handler. See Custom Throttled Handlers for the pattern.</p> <p>A typical successful response from this setup would look like:</p> <pre><code>HTTP/1.1 200 OK\nX-RateLimit-Limit: 1000\nX-RateLimit-Remaining: 997\nX-RateLimit-Reset-Ms: 43200\nRateLimit-Policy: 1000;w=60\nContent-Type: application/json\n\n{\"items\": [\"widget\", \"gizmo\"]}\n</code></pre> <p>And a throttled response:</p> <pre><code>HTTP/1.1 429 Too Many Requests\nX-RateLimit-Limit: 1000\nX-RateLimit-Remaining: 0\nX-RateLimit-Reset-Ms: 8400\nRateLimit-Policy: 1000;w=60\nRetry-After: 9\nContent-Type: application/json\n\n{\"detail\": \"Too many requests. Retry in 9 seconds.\"}\n</code></pre> <p>RFC 9440 - The emerging standard</p> <p>The IETF's RFC 9440 standardises rate limit headers. If your API needs to be strictly spec-compliant, the <code>RateLimit</code> (combined) and <code>RateLimit-Policy</code> headers are the future direction. The <code>X-RateLimit-*</code> prefix is widely understood but not standardised, it's the safe pragmatic choice for compatibility today.</p>"},{"location":"advanced/headers/#summary","title":"Summary","text":"Concept What it does <code>DEFAULT_HEADERS_ALWAYS</code> (from <code>traffik.headers</code>) Preset: <code>Limit</code> + <code>Remaining</code> + <code>Retry-After</code> resolved on every hit <code>DEFAULT_HEADERS_THROTTLED</code> (from <code>traffik.headers</code>) Preset: same three headers, but only resolved on throttled (<code>429</code>) hits <code>Header.LIMIT(when=...)</code> Resolves to the window's max hit count <code>Header.REMAINING(when=...)</code> Resolves to hits left this window <code>Header.RESET_SECONDS(when=...)</code> Resolves to seconds until window resets <code>Header.RESET_MILLISECONDS(when=...)</code> Resolves to milliseconds until window resets <code>Header(\"static\")</code> A fixed string, always included <code>Header(fn, when=...)</code> A dynamic value via resolver callable <code>Headers({...})</code> A collection of headers attached to a throttle <code>headers_a \\| headers_b</code> Merge two header collections (non-mutating) <code>Header.DISABLE</code> Sentinel to suppress a specific header in a call"},{"location":"advanced/multiple-limits/","title":"Multiple Rate Limits (Layered Throttling)","text":"<p>A single rate limit is rarely enough for a production API.</p> <p>A user who sends 20 requests in the first second and then waits 59 seconds hasn't violated a \"100/minute\" rule \u2014 but they've hammered your service with a burst that your infrastructure may not appreciate. Conversely, a steady drumbeat of 1 request every second looks fine per-minute but might add up to a problematic volume per hour.</p> <p>Layered throttling lets you enforce both at once: a tight short-term cap and a generous long-term envelope. The user must stay within all limits simultaneously.</p>"},{"location":"advanced/multiple-limits/#how-it-works","title":"How It Works","text":"<p>Think of it like a two-tier system:</p> <ul> <li>Burst limit: Protects your service from sudden traffic spikes. Short window, tight cap.</li> <li>Sustained limit: Ensures fair long-term use. Long window, generous cap.</li> </ul> <p>A request only succeeds if it passes every throttle in the chain. The first throttle to reject it wins \u2014 subsequent throttles in the chain are not evaluated.</p>"},{"location":"advanced/multiple-limits/#stacking-with-depends","title":"Stacking with <code>Depends</code>","text":"<p>The most straightforward approach for FastAPI: list your throttles as dependencies. FastAPI resolves them in order.</p> <pre><code>from fastapi import Depends, FastAPI\nfrom traffik import HTTPThrottle\nfrom traffik.backends.redis import RedisBackend\n\nbackend = RedisBackend(\"redis://localhost:6379\", namespace=\"myapp\")\n\n# Tier 1: No more than 10 requests per minute (burst protection)\nburst_throttle = HTTPThrottle(\n    uid=\"api:burst\",\n    rate=\"10/min\",\n    backend=backend,\n)\n\n# Tier 2: No more than 100 requests per hour (sustained fairness)\nsustained_throttle = HTTPThrottle(\n    uid=\"api:sustained\",\n    rate=\"100/hour\",\n    backend=backend,\n)\n\napp = FastAPI()\n\n@app.get(\n    \"/api/data\",\n    dependencies=[Depends(burst_throttle), Depends(sustained_throttle)],\n)\nasync def get_data():\n    return {\"data\": \"...\"}\n</code></pre> <p>A client who hits this endpoint is subject to both rules simultaneously. They can make at most 10 requests per minute, and at most 100 over the course of an hour.</p>"},{"location":"advanced/multiple-limits/#stacking-with-throttled","title":"Stacking with <code>@throttled</code>","text":"<p>The <code>@throttled</code> decorator from <code>traffik.decorators</code> is the FastAPI-native way to apply multiple throttles as a route decorator. Pass all throttles in a single call.</p> <pre><code>from fastapi import FastAPI\nfrom traffik import HTTPThrottle\nfrom traffik.decorators import throttled  # FastAPI-specific decorator\n\nburst_throttle     = HTTPThrottle(uid=\"api:burst\",     rate=\"10/min\")\nsustained_throttle = HTTPThrottle(uid=\"api:sustained\", rate=\"100/hour\")\n\napp = FastAPI()\n\n@app.get(\"/api/data\")\n@throttled(burst_throttle, sustained_throttle)\nasync def get_data():\n    return {\"data\": \"...\"}\n</code></pre> <p>Import the right <code>throttled</code></p> <p>There are two <code>throttled</code> functions in Traffik:</p> <ul> <li><code>traffik.decorators.throttled</code> \u2014 designed for FastAPI, works with its   dependency injection system. Routes do not need an explicit connection parameter.</li> <li><code>traffik.throttles.throttled</code> \u2014 the Starlette version. The decorated route   must have a <code>Request</code> or <code>WebSocket</code> parameter.</li> </ul> <p>For FastAPI, always use <code>from traffik.decorators import throttled</code>.</p>"},{"location":"advanced/multiple-limits/#stacking-with-depends-on-a-router","title":"Stacking with <code>Depends</code> on a Router","text":"<p>For router-wide limits, add the throttles to the router's <code>dependencies</code> list. Every route on the router inherits them.</p> <pre><code>from fastapi import APIRouter, Depends\nfrom traffik import HTTPThrottle\n\nburst_throttle     = HTTPThrottle(uid=\"v1:burst\",     rate=\"10/min\")\nsustained_throttle = HTTPThrottle(uid=\"v1:sustained\", rate=\"100/hour\")\n\nrouter = APIRouter(\n    prefix=\"/api/v1\",\n    dependencies=[Depends(burst_throttle), Depends(sustained_throttle)],\n)\n\n@router.get(\"/users\")\nasync def get_users():\n    ...\n\n@router.get(\"/orgs\")\nasync def get_orgs():\n    ...\n</code></pre>"},{"location":"advanced/multiple-limits/#order-matters","title":"Order Matters","text":"<p>Throttles are checked sequentially in the order you pass them. The first failure stops the chain. No subsequent throttles run.</p> <p>This has practical implications for efficiency and user experience:</p> <pre><code># Good: burst check first \u2014 cheaper (shorter window, fewer stored keys)\n#       and more informative (the user knows they burst, not that they\n#       exhausted their hourly budget)\ndependencies=[Depends(burst_throttle), Depends(sustained_throttle)]\n\n# Also valid: sustained check first \u2014 useful if you want to enforce\n#             the hourly cap as the primary signal\ndependencies=[Depends(sustained_throttle), Depends(burst_throttle)]\n</code></pre> <p>Recommend: strictest limit first</p> <p>Put your tightest limit first. It fails fast and saves you a backend roundtrip for the looser limit. It also gives the client a more actionable <code>Retry-After</code> header \u2014 \"wait 30 seconds\" is more useful than \"wait 54 minutes\".</p>"},{"location":"advanced/multiple-limits/#real-world-example-api-with-three-tiers","title":"Real-World Example: API with Three Tiers","text":"<p>A realistic public API often needs three tiers: per-second, per-minute, and per-day.</p> <pre><code>from fastapi import Depends, FastAPI\nfrom traffik import HTTPThrottle\nfrom traffik.backends.redis import RedisBackend\n\nbackend = RedisBackend(\"redis://localhost:6379\", namespace=\"myapp\")\n\n# 1. Per-second burst cap: no firehosing\nper_second = HTTPThrottle(\n    uid=\"api:per-second\",\n    rate=\"5/s\",\n    backend=backend,\n)\n\n# 2. Per-minute envelope: sustained usage\nper_minute = HTTPThrottle(\n    uid=\"api:per-minute\",\n    rate=\"60/min\",\n    backend=backend,\n)\n\n# 3. Daily quota: long-term fair use\nper_day = HTTPThrottle(\n    uid=\"api:per-day\",\n    rate=\"5000/day\",\n    backend=backend,\n)\n\napp = FastAPI()\n\n@app.get(\n    \"/api/data\",\n    dependencies=[\n        Depends(per_second),   # Strictest first\n        Depends(per_minute),\n        Depends(per_day),      # Loosest last\n    ],\n)\nasync def get_data():\n    return {\"data\": \"...\"}\n</code></pre> <p>A well-behaved client using this endpoint can sustain 5 requests per second, as long as they don't exceed 60 per minute, and don't blow their 5000-per-day quota.</p>"},{"location":"advanced/multiple-limits/#starlette-example","title":"Starlette Example","text":"<p>If you're using Starlette directly (not FastAPI), use <code>@throttled</code> from <code>traffik.throttles</code>. Your route must accept a <code>Request</code> parameter \u2014 the decorator finds it automatically.</p> <pre><code>from starlette.applications import Starlette\nfrom starlette.requests import Request\nfrom starlette.responses import JSONResponse\nfrom starlette.routing import Route\nfrom traffik import HTTPThrottle\nfrom traffik.throttles import throttled  # Starlette version\n\nburst_throttle     = HTTPThrottle(uid=\"star:burst\",     rate=\"10/min\")\nsustained_throttle = HTTPThrottle(uid=\"star:sustained\", rate=\"100/hour\")\n\n@throttled(burst_throttle, sustained_throttle)\nasync def get_data(request: Request):\n    return JSONResponse({\"data\": \"...\"})\n\napp = Starlette(routes=[Route(\"/api/data\", get_data)])\n</code></pre>"},{"location":"advanced/multiple-limits/#quick-reference","title":"Quick Reference","text":"Pattern Use case <code>Depends(a), Depends(b)</code> Route-level, explicit, most readable <code>@throttled(a, b)</code> FastAPI decorator style, same result Router <code>dependencies=[...]</code> Apply to every route in a router Multiple <code>@throttled</code> decorators Not supported \u2014 pass all throttles in one call <p>Each throttle has its own counter</p> <p>Every <code>HTTPThrottle</code> instance tracks usage independently by its <code>uid</code>. Make sure each throttle you create has a unique <code>uid</code> \u2014 Traffik raises a <code>ConfigurationError</code> if you try to register two throttles with the same UID.</p>"},{"location":"advanced/quota-context/","title":"Quota Context (Deferred Throttling)","text":"<p>This is one of Traffik's useful features, and also the one most people don't need on day one. But when you do need it, you'll be very glad it exists.</p>"},{"location":"advanced/quota-context/#the-problem","title":"The Problem","text":"<p>Standard throttling is optimistic: consume quota first, then do the work. Most of the time that's fine. But imagine this scenario:</p> <ol> <li>Client sends a request to generate a report (costs 10 quota)</li> <li>Traffik immediately deducts 10 from the quota counter</li> <li>The report generation fails halfway through</li> <li>The quota is gone, but the client got nothing</li> </ol> <p>Or the reverse:</p> <ol> <li>You want to check three different throttles before doing expensive work</li> <li>If any of them would reject the request, you don't want the others consumed either</li> <li>With standard throttling you'd have to hit each throttle, track which ones fired, and manually undo... which you can't</li> </ol> <p>Quota Context solves this. It queues throttle hits and only actually consumes them when you say so \u2014 or not at all if something goes wrong.</p> <p>This is an advanced feature</p> <p>Quota Context adds real complexity. For most use cases, standard <code>Depends(throttle)</code> is simpler and should be preferred. Use Quota Context when you have specific conditional consumption requirements.</p> <p>QuotaContext is NOT atomic</p> <p>QuotaContext consumption does not guarantee \"all or nothing\" like an ACID transaction. It prevents optimistic over-consumption (quota is not deducted until <code>apply()</code> is called), but if a context partially succeeds before failing during <code>apply()</code> \u2014 for example, if entry 3 of 5 fails \u2014 the quota already consumed by entries 1 and 2 is not rolled back. The cost of wasted quota falls on the API, not the client. Design your quota contexts accordingly: keep them small, and use <code>apply_on_error=False</code> (the default) to avoid charging clients for server-side failures.</p>"},{"location":"advanced/quota-context/#bound-mode","title":"Bound Mode","text":"<p>The most common usage: create a context tied to one throttle.</p> <pre><code>from fastapi import FastAPI, Request, Depends\nfrom traffik import HTTPThrottle\nfrom traffik.backends.redis import RedisBackend\n\nbackend = RedisBackend(\"redis://localhost:6379\", namespace=\"myapp\")\napp = FastAPI(lifespan=backend.lifespan)\n\nthrottle = HTTPThrottle(\"api:reports\", rate=\"50/hour\", backend=backend)\n\n\n@app.post(\"/reports/generate\")\nasync def generate_report(request: Request):\n    # Queue throttle hits \u2014 they are NOT consumed yet\n    async with throttle.quota(request) as quota:\n        await quota(cost=10)        # Queued: cost=10\n        await quota(cost=5)         # Aggregated! Still one entry: cost=15\n\n        # Do the expensive work\n        report = await generate_expensive_report()\n\n    # Context exits successfully -&gt; quota consumed (15 units deducted)\n    return {\"report\": report}\n</code></pre> <p>If <code>generate_expensive_report()</code> raises an exception, the context exits with an error \u2014 and no quota is consumed. The client doesn't pay for a failed operation.</p>"},{"location":"advanced/quota-context/#unbound-mode","title":"Unbound Mode","text":"<p>When you need to control multiple throttles together, use <code>QuotaContext</code> directly:</p> <pre><code>from traffik.quotas import QuotaContext\n\nburst_throttle = HTTPThrottle(\"api:burst\", rate=\"20/min\", backend=backend)\ndaily_throttle = HTTPThrottle(\"api:daily\", rate=\"500/day\", backend=backend)\n\n\n@app.post(\"/exports\")\nasync def create_export(request: Request):\n    async with QuotaContext(request) as quota:\n        # Must specify throttle explicitly in unbound mode\n        await quota(burst_throttle, cost=5)\n        await quota(daily_throttle, cost=5)\n\n        result = await run_export()\n\n    return {\"export\": result}\n</code></pre> <p>Both throttles are consumed atomically on successful exit, or neither is consumed on failure.</p>"},{"location":"advanced/quota-context/#cost-aggregation","title":"Cost Aggregation","text":"<p>Consecutive calls with the same throttle and identical configuration are automatically merged into a single backend operation. This is a performance optimization \u2014 fewer round-trips to the backend.</p> <pre><code>async with throttle.quota(request) as quota:\n    await quota(cost=2)          # Entry 1: cost=2\n    await quota(cost=3)          # Aggregated into Entry 1: cost=5\n    await quota()                # Aggregated into Entry 1: cost=6\n    await quota(other_throttle)  # Entry 2: different throttle, new entry\n    await quota(cost=1)          # Entry 1 again? No \u2014 other_throttle broke the streak\n                                 # This becomes Entry 3\n</code></pre> <p>Aggregation breaks when: different throttle, different context, different retry config, or a cost function is used (can't know the cost until apply time).</p>"},{"location":"advanced/quota-context/#conditional-consumption","title":"Conditional Consumption","text":""},{"location":"advanced/quota-context/#apply_on_error","title":"apply_on_error","text":"<p>Control whether quota is consumed when an exception occurs:</p> <pre><code># Default: don't consume on any exception\nasync with throttle.quota(request, apply_on_error=False) as quota:\n    await quota(cost=5)\n    result = await risky_operation()  # Exception -&gt; no quota consumed\n\n# Always consume, even on exceptions (use sparingly)\nasync with throttle.quota(request, apply_on_error=True) as quota:\n    await quota(cost=5)\n    await risky_operation()  # Exception -&gt; quota still consumed\n\n# Consume only for specific exception types\nfrom fastapi import HTTPException\nasync with throttle.quota(request, apply_on_error=(ValueError,)) as quota:\n    await quota(cost=5)\n    await risky_operation()\n    # ValueError -&gt; consume quota\n    # Any other exception -&gt; don't consume quota\n</code></pre>"},{"location":"advanced/quota-context/#apply_on_exitfalse","title":"apply_on_exit=False","text":"<p>Disable automatic consumption on exit and manage it yourself:</p> <pre><code>async with throttle.quota(request, apply_on_exit=False) as quota:\n    await quota(cost=5)\n\n    is_valid = await validate_business_rules()\n    if not is_valid:\n        await quota.cancel()  # Discard \u2014 no quota consumed\n        return {\"error\": \"validation_failed\"}\n\n    result = await process()\n    await quota.apply()  # Manually consume quota after success\n\nreturn {\"result\": result}\n</code></pre>"},{"location":"advanced/quota-context/#manual-apply-and-cancel","title":"Manual apply() and cancel()","text":"<pre><code>async with throttle.quota(request, apply_on_exit=False) as quota:\n    await quota(cost=10)\n\n    try:\n        result = await complex_operation()\n        await quota.apply()   # Success: consume\n    except ExpectedError:\n        await quota.cancel()  # Known failure: don't consume\n        raise\n</code></pre> <p><code>apply()</code> is idempotent \u2014 calling it twice is safe. <code>cancel()</code> is final \u2014 you cannot un-cancel a context.</p>"},{"location":"advanced/quota-context/#pre-checking-with-check","title":"Pre-checking with check()","text":"<p>Sometimes you want to check if quota would be available before committing to an operation:</p> <pre><code># Check at the throttle level\nif not await throttle.check(request, cost=10):\n    raise HTTPException(429, \"Insufficient quota for this operation\")\n\n# Check inside a quota context\nasync with throttle.quota(request) as quota:\n    await quota(cost=10)\n\n    if not await quota.check():  # Check using the owner throttle\n        await quota.cancel()\n        raise HTTPException(429, \"Rate limit would be exceeded\")\n\n    result = await expensive_operation()\n</code></pre> <p>TOCTOU caveat</p> <p><code>check()</code> is a best-effort snapshot. Between the check and the actual consumption, another request from the same client could use up the remaining quota. For strong guarantees, use <code>lock=True</code> (see below) or design your system to handle <code>ConnectionThrottled</code> exceptions gracefully.</p>"},{"location":"advanced/quota-context/#nested-contexts","title":"Nested Contexts","text":"<p>Child contexts merge their queued entries into the parent when they exit successfully:</p> <pre><code>async with throttle.quota(request) as parent:\n    await parent(cost=2)\n\n    async with parent.nested() as child:\n        await child(cost=1)\n        # child exits successfully -&gt; merges cost=1 into parent's queue\n\n    # Parent's queue now has cost=3 total\n    await parent(cost=1)  # Aggregated: still same entry if config matches\n\n# All consumed here: 4 total units\n</code></pre> <p>The parent acquires the lock (if configured). Child contexts under a parent don't acquire their own lock by default \u2014 they operate under the parent's lock context.</p>"},{"location":"advanced/quota-context/#locking","title":"Locking","text":"<p>Enable locking to make the entire quota context atomic with respect to other contexts using the same key:</p> <pre><code># Use throttle UID as lock key (simplest)\nasync with throttle.quota(request, lock=True) as quota:\n    await quota(cost=5)\n    result = await process()  # Lock held for entire duration \u2014 keep this fast!\n\n# Custom lock key\nasync with throttle.quota(request, lock=\"user:123:api_calls\") as quota:\n    await quota(cost=5)\n\n# Custom lock config\nasync with throttle.quota(\n    request,\n    lock=True,\n    lock_config={\"ttl\": 30, \"blocking_timeout\": 5}\n) as quota:\n    await quota(cost=5)\n</code></pre> <p>Keep locked contexts fast</p> <p>The lock is held for the entire duration of the <code>async with</code> block. Long-running operations (database queries, external API calls) inside a locked context will block other requests waiting for the same lock, increasing latency significantly.</p>"},{"location":"advanced/quota-context/#retry-with-backoff","title":"Retry with Backoff","text":"<p>Individual quota entries can be configured to retry on failure:</p> <pre><code>from traffik.backoff import ExponentialBackoff\n\nasync with throttle.quota(request) as quota:\n    await quota(\n        cost=5,\n        retry=3,                           # Up to 3 retries\n        backoff=ExponentialBackoff(multiplier=2.0, max_delay=10.0),\n        base_delay=0.5,                    # Start with 0.5s delay\n    )\n    await quota(\n        cost=1,\n        retry=2,\n        retry_on=(TimeoutError,),          # Only retry on timeouts\n    )\n</code></pre>"},{"location":"advanced/quota-context/#backoff-strategies","title":"Backoff Strategies","text":"Strategy Behavior Best For <code>ConstantBackoff</code> Same delay every retry Simple retry logic, predictable timing <code>LinearBackoff(increment=1.0)</code> Delay grows by <code>increment</code> each attempt Gradual back-pressure <code>ExponentialBackoff(multiplier=2.0)</code> Delay doubles each attempt, with optional jitter Production retries, thundering-herd prevention <code>LogarithmicBackoff(base=2.0)</code> Delay grows logarithmically Many retries with diminishing returns <p>The default backoff when <code>retry &gt; 0</code> is <code>ExponentialBackoff(multiplier=2.0, max_delay=60.0, jitter=True)</code>.</p>"},{"location":"advanced/quota-context/#inspecting-the-context","title":"Inspecting the Context","text":"<p>You can inspect a quota context's state at any time:</p> <pre><code>async with throttle.quota(request) as quota:\n    await quota(cost=5)\n    await quota(cost=3)\n\n    print(quota.queued_cost)    # 8 (estimated, excludes cost functions)\n    print(quota.applied_cost)   # 0 (nothing consumed yet)\n    print(quota.active)         # True (not consumed or cancelled)\n    print(quota.consumed)       # False\n    print(quota.cancelled)      # False\n    print(quota.is_bound)       # True (created via throttle.quota())\n    print(quota.is_nested)      # False\n    print(quota.depth)          # 0\n</code></pre> <p>After the context exits:</p> <pre><code>print(quota.applied_cost)   # 8\nprint(quota.consumed)       # True\nprint(quota.active)         # False\n</code></pre>"},{"location":"advanced/quota-context/#limitations","title":"Limitations","text":"<p>Be aware of these before reaching for <code>QuotaContext</code>:</p> Limitation Detail No rollback on partial failure If entry 3 of 5 fails during <code>apply()</code>, entries 1 and 2 are already consumed TOCTOU with <code>check()</code> Quota can change between <code>check()</code> and <code>apply()</code> \u2014 use locks for strong consistency <code>cancelled</code> is final Once cancelled, a context cannot be un-cancelled or re-used <code>apply()</code> is idempotent Calling it multiple times only consumes once \u2014 safe but not a retry mechanism Nested lock ordering Acquiring locks in different orders in nested contexts can deadlock Cost functions resolved at apply If your throttle uses a cost function, <code>queued_cost</code> is an estimate"},{"location":"advanced/registry/","title":"Throttle Registry","text":"<p>Every throttle in Traffik belongs to a <code>ThrottleRegistry</code>. The registry is the coordination layer that holds throttle memberships, attaches rules, and lets you disable or re-enable throttles at runtime without touching application code.</p>"},{"location":"advanced/registry/#what-is-the-throttleregistry","title":"What is the ThrottleRegistry?","text":"<p><code>ThrottleRegistry</code> is a lightweight, thread-safe container that:</p> <ul> <li>Tracks which throttle UIDs are active (\"registered\").</li> <li>Stores the <code>ThrottleRule</code> / <code>BypassThrottleRule</code> sets that gate each throttle's <code>hit()</code> call.</li> <li>Keeps a weak reference to each throttle instance so it can forward <code>disable()</code> / <code>enable()</code> calls without preventing garbage collection.</li> </ul> <pre><code>from traffik.registry import ThrottleRegistry\n\nregistry = ThrottleRegistry()\n</code></pre> <p>A module-level singleton <code>GLOBAL_REGISTRY</code> is the default registry used when you don't pass one explicitly:</p> <pre><code>from traffik.registry import GLOBAL_REGISTRY\n</code></pre>"},{"location":"advanced/registry/#how-throttles-register","title":"How throttles register","text":"<p>Throttles register themselves automatically when they are created. You never need to call <code>registry.register()</code> manually. When the throttle is garbage-collected, it is automatically unregistered via a <code>weakref.finalize</code> callback.</p> <pre><code>from traffik import HTTPThrottle\nfrom traffik.registry import ThrottleRegistry\n\nregistry = ThrottleRegistry()\n\n# Automatically registered on construction, unregistered on GC\nthrottle = HTTPThrottle(\"api:v1\", rate=\"100/min\", registry=registry)\n\nregistry.exist(\"api:v1\")   # True\n</code></pre>"},{"location":"advanced/registry/#sharing-a-registry","title":"Sharing a registry","text":"<p>Multiple throttles sharing a single registry lets you manage them as a group: attach rules to several throttles at once, or disable them all in one call.</p> <pre><code>from traffik import HTTPThrottle\nfrom traffik.registry import ThrottleRegistry\n\nregistry = ThrottleRegistry()\n\nread_throttle  = HTTPThrottle(\"api:read\",  rate=\"200/min\", registry=registry)\nwrite_throttle = HTTPThrottle(\"api:write\", rate=\"50/min\",  registry=registry)\nadmin_throttle = HTTPThrottle(\"api:admin\", rate=\"500/min\", registry=registry)\n</code></pre>"},{"location":"advanced/registry/#adding-rules","title":"Adding rules","text":"<p><code>add_rules()</code> attaches <code>ThrottleRule</code> / <code>BypassThrottleRule</code> instances to a throttle by UID. Rules are checked conjunctively on every <code>hit()</code> call. If any rule returns <code>False</code>, the throttle is skipped for that request.</p> <pre><code>from traffik.registry import ThrottleRule, BypassThrottleRule\n\n# Only apply the write throttle on POST/PUT/DELETE\nregistry.add_rules(\n    \"api:write\",\n    ThrottleRule(methods={\"POST\", \"PUT\", \"DELETE\"}),\n)\n\n# Exempt health checks from all throttles in the registry\nfor uid in (\"api:read\", \"api:write\", \"api:admin\"):\n    registry.add_rules(uid, BypassThrottleRule(path=\"/health\"))\n</code></pre> <p>See Throttle Rules &amp; Wildcards for the full path-matching and predicate API.</p>"},{"location":"advanced/registry/#disable-and-enable","title":"Disable and enable","text":""},{"location":"advanced/registry/#per-throttle","title":"Per-throttle","text":"<p>Call <code>disable()</code> / <code>enable()</code> directly on the throttle instance. Both are async and acquire the throttle's internal update lock, so they are safe to call concurrently with <code>hit()</code>.</p> <pre><code># Disable a throttle \u2014 subsequent hit() calls return immediately without consuming quota\nawait throttle.disable()\n\n# Check status\nthrottle.is_disabled   # True\n\n# Re-enable\nawait throttle.enable()\nthrottle.is_disabled   # False\n</code></pre> <p>A common use-case is disabling a throttle from an error handler when the backend becomes unavailable:</p> <pre><code>from traffik import HTTPThrottle\n\nasync def my_on_error(connection, exc_info):\n    throttle = exc_info[\"throttle\"]\n    await throttle.disable()   # Let all traffic through while backend recovers\n    return 0                   # Allow this request\n\nrate_throttle = HTTPThrottle(\n    \"api:v1\",\n    rate=\"100/min\",\n    on_error=my_on_error,\n)\n</code></pre>"},{"location":"advanced/registry/#via-registry","title":"Via registry","text":"<p>Use <code>registry.disable(uid)</code> when you only have access to the registry (e.g., from a management endpoint or startup hook):</p> <pre><code>found = await registry.disable(\"api:write\")   # True if throttle is alive\nfound = await registry.enable(\"api:write\")    # True if throttle is alive\n</code></pre> <p>Both methods return <code>True</code> if the throttle was found and acted upon, or <code>False</code> if the UID is unknown or the instance has been garbage-collected.</p>"},{"location":"advanced/registry/#all-at-once","title":"All at once","text":"<p><code>disable_all()</code> and <code>enable_all()</code> iterate every live throttle in the registry:</p> <pre><code># Emergency kill switch \u2014 let all traffic through\nawait registry.disable_all()\n\n# Resume normal throttling\nawait registry.enable_all()\n</code></pre> <p>Dead (GC'd) throttles are silently skipped.</p>"},{"location":"advanced/registry/#runtime-updates","title":"Runtime updates","text":"<p>Each mutable throttle property has its own <code>update_*()</code> method. All are async and acquire the same update lock as <code>disable()</code> / <code>enable()</code>, so concurrent callers always see a consistent state.</p> <pre><code>from traffik import HTTPThrottle\nfrom traffik.headers import DEFAULT_HEADERS_ALWAYS\n\n# Change rate limit (accepts a string, Rate object, or async callable)\nawait throttle.update_rate(\"200/min\")\n\n# Swap backend (e.g., after hot-reload)\nawait throttle.update_backend(new_backend)\n\n# Swap strategy\nfrom traffik.strategies.token_bucket import TokenBucketStrategy\nawait throttle.update_strategy(TokenBucketStrategy())\n\n# Adjust cost\nawait throttle.update_cost(2)\n\n# Change minimum wait floor\nawait throttle.update_min_wait_period(500)  # 500 ms\n\n# Replace throttled-response handler\nawait throttle.update_handle_throttled(my_handler)\n\n# Replace response headers\nawait throttle.update_headers(DEFAULT_HEADERS_ALWAYS)\n\n# Replace identifier function (changes how connections are keyed)\nawait throttle.update_identifier(new_identifier_fn)\n</code></pre> <p>Properties that are intentionally immutable, <code>uid</code> and <code>registry</code>, have no update method. Changing either would break registry membership and rule lookups.</p>"},{"location":"advanced/registry/#inspecting-the-registry","title":"Inspecting the registry","text":"<pre><code>from traffik.registry import ThrottleRegistry\n\nregistry = ThrottleRegistry()\n\n# Check if a UID is registered\nregistry.exist(\"api:v1\")          # True / False\n\n# Retrieve attached rules\nrules = registry.get_rules(\"api:v1\")   # List[ThrottleRule]\n\n# Retrieve the live throttle instance (or None if GC'd)\nthrottle = registry.get_throttle(\"api:v1\")\n\n# Wipe everything (unregisters all UIDs, rules, and refs)\nregistry.clear()\n</code></pre>"},{"location":"advanced/registry/#summary","title":"Summary","text":"Method / attribute What it does <code>registry.exist(uid)</code> Check if a UID is registered <code>registry.add_rules(uid, *rules)</code> Attach rules to a throttle <code>registry.get_rules(uid)</code> Retrieve all rules for a throttle <code>registry.get_throttle(uid)</code> Return the live throttle instance, or <code>None</code> <code>registry.disable(uid)</code> Disable a specific throttle (async) <code>registry.enable(uid)</code> Re-enable a specific throttle (async) <code>registry.disable_all()</code> Disable every live throttle in the registry (async) <code>registry.enable_all()</code> Re-enable every live throttle in the registry (async) <code>registry.clear()</code> Wipe all registrations, rules, and refs <code>throttle.is_disabled</code> <code>True</code> if the throttle is currently disabled <code>throttle.disable()</code> Disable this throttle (async, acquires update lock) <code>throttle.enable()</code> Re-enable this throttle (async, acquires update lock) <code>throttle.update_rate(rate)</code> Update rate (string / <code>Rate</code> / callable) atomically <code>throttle.update_backend(b)</code> Swap backend atomically <code>throttle.update_strategy(s)</code> Swap strategy atomically <code>throttle.update_cost(c)</code> Update cost (int / callable) atomically <code>throttle.update_min_wait_period(ms)</code> Set wait floor (ms) atomically <code>throttle.update_handle_throttled(h)</code> Swap throttled-response handler atomically <code>throttle.update_headers(h)</code> Replace header collection atomically <code>throttle.update_identifier(fn)</code> Replace identifier function atomically"},{"location":"advanced/request-costs/","title":"Request Costs","text":"<p>Here's a scenario: your <code>/upload</code> endpoint lets users push files up to 100 MB. Your <code>/health</code> endpoint returns <code>{\"status\": \"ok\"}</code> in microseconds. Should they both tick the rate limit counter by exactly one?</p> <p>Probably not.</p> <p>Request costs are Traffik's answer to this. Every <code>hit(...)</code> consumes a configurable amount of quota instead of a flat <code>1</code>. You get to decide what \"expensive\" means for your API.</p>"},{"location":"advanced/request-costs/#the-default-cost-of-1","title":"The Default: Cost of 1","text":"<p>By default, every request costs 1 unit of quota. A throttle set to <code>\"100/min\"</code> allows 100 requests per minute. This is fine for most endpoints.</p> <pre><code>from traffik import HTTPThrottle\n\n# 100 requests per minute, each costs 1 (the default)\nthrottle = HTTPThrottle(uid=\"api:default\", rate=\"100/min\")\n</code></pre>"},{"location":"advanced/request-costs/#fixed-cost-at-initialization","title":"Fixed Cost at Initialization","text":"<p>Pass <code>cost=N</code> when you create the throttle to make every request through it consume <code>N</code> units of quota. This is the simplest way to make an endpoint feel \"heavier\" to the rate limiter.</p> <pre><code>from traffik import HTTPThrottle\nfrom fastapi import Depends\n\n# Each request to this throttle burns 10 units \u2014 same as 10 normal requests\nexport_throttle = HTTPThrottle(\n    uid=\"api:export\",\n    rate=\"100/min\",\n    cost=10,\n)\n\n@app.get(\"/export\", dependencies=[Depends(export_throttle)])\nasync def export_data():\n    ...\n</code></pre> <p>Thinking in units</p> <p>A throttle with <code>rate=\"100/min\"</code> and <code>cost=10</code> effectively allows 10 export requests per minute. The math is: <code>rate.limit / cost = effective_limit</code>.</p>"},{"location":"advanced/request-costs/#dynamic-cost-via-function","title":"Dynamic Cost via Function","text":"<p>When the cost depends on the request itself \u2014 file size, number of records, operation type \u2014 pass an async function instead of an integer. Traffik calls it on every hit, passing the connection and the current context.</p> <pre><code>from traffik import HTTPThrottle\nfrom starlette.requests import Request\nimport typing\n\nasync def upload_cost(\n    request: Request,\n    context: typing.Optional[typing.Dict[str, typing.Any]],\n) -&gt; int:\n    # Cost = file size in megabytes, minimum 1\n    content_length = int(request.headers.get(\"content-length\", 0))\n    size_mb = content_length // (1024 * 1024)\n    return max(size_mb, 1)\n\nupload_throttle = HTTPThrottle(\n    uid=\"api:upload\",\n    rate=\"500/hour\",  # 500 MB of uploads per user per hour\n    cost=upload_cost,\n)\n</code></pre> <p>The cost function signature is:</p> <pre><code>async def my_cost(\n    connection: Request,\n    context: typing.Optional[typing.Dict[str, typing.Any]],\n) -&gt; int:\n    ...\n</code></pre> <p>Both parameters are provided automatically by Traffik. <code>context</code> will be <code>None</code> unless you passed a <code>context</code> dict when initializing the throttle or calling <code>hit(...)</code>.</p>"},{"location":"advanced/request-costs/#per-call-override","title":"Per-Call Override","text":"<p>Sometimes you know the cost only at the moment of the call \u2014 for example, after parsing a request body. Pass <code>cost=N</code> directly to <code>hit(...)</code> or <code>__call__(...)</code> to override the throttle's default for that one request.</p> <pre><code>@app.post(\"/process\")\nasync def process(request: Request):\n    body = await request.json()\n    operation = body.get(\"operation\", \"read\")\n\n    # Charge more quota for mutations\n    cost_map = {\"read\": 1, \"write\": 5, \"delete\": 10}\n    cost = cost_map.get(operation, 1)\n\n    await throttle(request, cost=cost)\n    return {\"status\": \"processed\"}\n</code></pre> <p>Per-call cost takes priority</p> <p>When you pass <code>cost=N</code> to <code>hit(...)</code>, it overrides both the fixed <code>cost</code> set at initialization and any dynamic cost function. Think of it as the last word.</p>"},{"location":"advanced/request-costs/#cost-of-zero-the-exemption-shortcut","title":"Cost of Zero: The Exemption Shortcut","text":"<p>A cost of <code>0</code> is special: Traffik sees it and short-circuits immediately \u2014 no counter is incremented, no backend is called. The request passes through as if it was never throttled.</p> <pre><code>async def selective_cost(\n    request: Request,\n    context: typing.Optional[typing.Dict[str, typing.Any]],\n) -&gt; int:\n    # Health checks and metrics scrapes don't count against the limit\n    if request.url.path in {\"/health\", \"/metrics\", \"/readyz\"}:\n        return 0\n    return 1\n</code></pre> <p>Cost 0 vs <code>EXEMPTED</code></p> <p>Returning <code>0</code> from a cost function and returning <code>EXEMPTED</code> from an identifier function both skip throttling \u2014 but they do it at different stages:</p> <ul> <li><code>cost=0</code>: Skips after the identifier is resolved. The identifier is still called.</li> <li><code>EXEMPTED</code>: Skips at the identifier stage. Nothing downstream runs at all.</li> </ul> <p>For exempting entire client categories (admins, internal services), <code>EXEMPTED</code> from the identifier is cleaner and slightly cheaper. For skipping specific paths or methods within a shared throttle, <code>cost=0</code> is a pragmatic shortcut.</p>"},{"location":"advanced/request-costs/#real-world-examples","title":"Real-World Examples","text":"File Upload (Size-Based)AI Token CountingOperation Type (Read / Write / Delete)Bulk Operations <p>Charge quota proportional to the uploaded file's size. A 50 MB upload consumes 50x the quota of a 1 MB upload.</p> <pre><code>from traffik import HTTPThrottle\nfrom starlette.requests import Request\nimport typing\n\nasync def file_size_cost(\n    request: Request,\n    context: typing.Optional[typing.Dict[str, typing.Any]],\n) -&gt; int:\n    content_length = int(request.headers.get(\"content-length\", 0))\n    # Cost = ceil(bytes / 1 MB), minimum 1\n    size_mb = max(1, -(-content_length // (1024 * 1024)))  # ceiling division\n    return size_mb\n\nupload_throttle = HTTPThrottle(\n    uid=\"uploads:per-user\",\n    rate=\"500/hour\",   # 500 MB of uploads per user per hour\n    cost=file_size_cost,\n    identifier=get_user_id,\n)\n</code></pre> <p>Charge quota based on how many tokens an AI request consumed, reported back by the model. Use <code>hit(...)</code> manually after the response so you know the actual count.</p> <pre><code>from traffik import HTTPThrottle\nfrom fastapi import Request\n\ntoken_throttle = HTTPThrottle(\n    uid=\"ai:token-budget\",\n    rate=\"100000/day\",   # 100k tokens per user per day\n    identifier=get_user_id,\n)\n\n@app.post(\"/ai/complete\")\nasync def complete(request: Request):\n    body = await request.json()\n\n    # Call the model first \u2014 we need the actual token count\n    result = await call_llm(body[\"prompt\"])\n    tokens_used = result[\"usage\"][\"total_tokens\"]\n\n    # Now consume that many units of quota\n    await token_throttle(request, cost=tokens_used)\n\n    return result\n</code></pre> <p>Different operations have different blast radii. A DELETE that wipes a table should cost more than a GET that reads one row.</p> <pre><code>from traffik import HTTPThrottle\nfrom starlette.requests import Request\nimport typing\n\nOPERATION_COSTS = {\n    \"GET\":    1,\n    \"POST\":   3,\n    \"PUT\":    3,\n    \"PATCH\":  3,\n    \"DELETE\": 10,\n}\n\nasync def method_cost(\n    request: Request,\n    context: typing.Optional[typing.Dict[str, typing.Any]],\n) -&gt; int:\n    method = request.method.upper()\n    return OPERATION_COSTS.get(method, 1)\n\napi_throttle = HTTPThrottle(\n    uid=\"api:weighted\",\n    rate=\"100/min\",\n    cost=method_cost,\n)\n\n# With this setup:\n# - 100 GET requests per minute (100 \u00d7 1 = 100 units)\n# - 33 POST/PUT/PATCH requests per minute (33 \u00d7 3 \u2248 100 units)\n# - 10 DELETE requests per minute (10 \u00d7 10 = 100 units)\n</code></pre> <p>Endpoints that operate on multiple records at once should count each record, not each HTTP request.</p> <pre><code>from traffik import HTTPThrottle\nfrom fastapi import Request\n\nasync def bulk_cost(\n    request: Request,\n    context: typing.Optional[typing.Dict[str, typing.Any]],\n) -&gt; int:\n    body = await request.json()\n    items = body.get(\"items\", [])\n    # Each item in the batch counts as one unit, minimum 1\n    return max(len(items), 1)\n\nbulk_throttle = HTTPThrottle(\n    uid=\"api:bulk-write\",\n    rate=\"1000/hour\",\n    cost=bulk_cost,\n)\n</code></pre>"},{"location":"advanced/request-costs/#summary","title":"Summary","text":"Approach When to use <code>cost=N</code> (integer) All requests to this throttle are equally expensive <code>cost=async_fn</code> Cost depends on the request content or headers <code>await throttle(request, cost=N)</code> You know the cost only at call time <code>cost=0</code> Skip throttling for certain requests within a shared throttle"},{"location":"advanced/rules/","title":"Throttle Rules &amp; Wildcards","text":"<p>Throttles apply globally by default. Attach one to a router and it runs on every request that hits that router, no questions asked. That's perfect for simple cases, but real APIs rarely have simple cases.</p> <p>What if your global API throttle should apply to most routes but not the health check? What if <code>GET /users</code> has its own stricter limit and shouldn't also drain the shared pool? What if you want one throttle to govern anonymous users and a totally different one for premium accounts?</p> <p>That's what throttle rules are for. Rules let a throttle ask \"does this connection even apply to me?\" before doing anything, before touching the backend, before computing the client identifier, before any of it. If the rule says no, the throttle steps aside cleanly.</p>"},{"location":"advanced/rules/#the-problem-selective-throttling","title":"The Problem: Selective Throttling","text":"<p>Here's the scenario that rules are designed for. Imagine a versioned API:</p> <pre><code>/api/v1                  (GET: 1000/min, POST: 300/min)\n  /api/v1/users          (GET: 500/min,  POST: unlimited)\n  /api/v1/organizations  (GET: unlimited, POST: 600/min)\n    /api/v1/organizations/{id}  (GET: 100/min additional limit)\n</code></pre> <p>The global throttle covers everything under <code>/api/v1</code>. But:</p> <ul> <li><code>GET /api/v1/users</code> has its own 500/min limit, so it shouldn't also eat from the 1000/min global pool for GETs.</li> <li><code>POST /api/v1/organizations</code> has its own 600/min limit, so it shouldn't also count against the global 300/min POST limit.</li> </ul> <p>Without rules, you'd have to manually split your routing, duplicate logic, or accept over-counting. With rules, you express exactly this in a few lines.</p>"},{"location":"advanced/rules/#throttlerule-only-apply-when","title":"<code>ThrottleRule</code>: \"Only apply when...\"","text":"<p>A <code>ThrottleRule</code> is a gate: the throttle applies only when the connection matches the rule. Non-matching connections pass straight through without consuming any quota.</p> <pre><code>from traffik.registry import ThrottleRule\n\n# Only throttle GET requests to /api/users\nrule = ThrottleRule(path=\"/api/users\", methods={\"GET\"})\nthrottle = HTTPThrottle(\"api:users\", rate=\"500/min\", rules={rule})\n</code></pre> <p>A <code>ThrottleRule</code> accepts three optional parameters, all are conjunctive:</p> <ul> <li><code>path</code> - a path pattern (string, glob, or compiled regex). The connection's path must match.</li> <li><code>methods</code> - a set of HTTP method strings (e.g. <code>{\"GET\", \"POST\"}</code>). The connection's method must be in the set. Case-insensitive.</li> <li><code>predicate</code> - an async callable for custom logic. Must return <code>True</code> for the throttle to apply.</li> </ul> <p>If any of the specified criteria don't match, the throttle is skipped for that connection. All criteria that are specified must pass.</p>"},{"location":"advanced/rules/#bypassthrottlerule-skip-when","title":"<code>BypassThrottleRule</code>: \"Skip when...\"","text":"<p><code>BypassThrottleRule</code> is the inverse: the throttle is skipped when the connection matches.</p> <pre><code>from traffik.registry import BypassThrottleRule\n\n# Global throttle applies to everything EXCEPT GET /api/users\nbypass = BypassThrottleRule(path=\"/api/users\", methods={\"GET\"})\nglobal_throttle = HTTPThrottle(\"api:global\", rate=\"1000/min\", rules={bypass})\n</code></pre> <p>This is often more ergonomic for global throttles with carve-outs. Rather than listing every path that should be throttled (which changes as you add routes), you list the exceptions.</p> <p><code>ThrottleRule</code> vs <code>BypassThrottleRule</code>: which to reach for?</p> <p>Use <code>ThrottleRule</code> when you're building a targeted throttle that should only apply to specific routes, e.g., a strict limit on the login endpoint.</p> <p>Use <code>BypassThrottleRule</code> when you have a broad throttle (e.g., a global router-level limit) and want to carve out exceptions, e.g., \"everything except health checks and the CDN callback.\"</p>"},{"location":"advanced/rules/#wildcard-path-patterns","title":"Wildcard Path Patterns","text":"<p>The <code>path</code> parameter on both rule types accepts glob-style wildcards:</p> Pattern Matches Does not match <code>/api/*</code> <code>/api/users</code>, <code>/api/v1</code> <code>/api/v1/users</code> (contains nested <code>/</code>) <code>/api/**</code> <code>/api/users</code>, <code>/api/v1/users</code>, <code>/api/a/b/c</code> <code>/other/path</code> <code>/api/v*/users</code> <code>/api/v1/users</code>, <code>/api/v2/users</code> <code>/api/v1/admins</code> <code>/api/users</code> <code>/api/users</code>, <code>/api/users/dashboard</code> <code>/other/users</code> <p>The rules are:</p> <ul> <li><code>*</code> matches a single path segment, everything except <code>/</code>. Useful for matching one level of path hierarchy.</li> <li><code>**</code> matches multiple segments including <code>/</code>. Useful for \"everything under this prefix\".</li> <li>Plain strings (no <code>*</code>) are treated as prefix regex matches, so <code>/api/users</code> matches <code>/api/users</code>, <code>/api/users/123</code>, <code>/api/users/dashboard</code>, etc.</li> <li>Compiled <code>re.Pattern</code> objects are matched as-is against the full connection path.</li> </ul> <pre><code>import re\nfrom traffik.registry import BypassThrottleRule, ThrottleRule\n\n# Glob wildcards\nBypassThrottleRule(path=\"/api/v*/users\")    # /api/v1/users, /api/v2/users, etc.\nBypassThrottleRule(path=\"/api/**\")           # Anything under /api/\n\n# Compiled regex (exact control)\nThrottleRule(path=re.compile(r\"^/api/users/\\d+$\"))  # Only numeric user IDs\n\n# Prefix match (no wildcards)\nBypassThrottleRule(path=\"/api/internal\")  # /api/internal, /api/internal/health, etc.\n</code></pre> <p>Regex vs prefix matching</p> <p>When you pass a plain string with no <code>*</code>, Traffik compiles it as a regex and uses <code>re.Pattern.match()</code>, which matches from the start of the string. Since <code>match()</code> doesn't require matching to the end, <code>/api/users</code> becomes a prefix match. If you need a full-string match, use a compiled regex with <code>$</code> at the end.</p> <p><code>ThrottleMiddleware</code> path uses <code>ThrottleRule</code> internally</p> <p>The <code>path</code> parameter on <code>MiddlewareThrottle</code> (used with <code>ThrottleMiddleware</code>) uses the same <code>ThrottleRule</code> path-matching logic internally. That means it supports the same wildcard patterns: <code>*</code>, <code>**</code>, plain string prefix match, and compiled <code>re.Pattern</code>. See Middleware for usage examples.</p>"},{"location":"advanced/rules/#attaching-rules-at-initialization","title":"Attaching Rules at Initialization","text":"<p>Pass a <code>rules</code> set when constructing the throttle:</p> <pre><code>from traffik import HTTPThrottle\nfrom traffik.registry import BypassThrottleRule\n\nbypass1 = BypassThrottleRule(path=\"/api/users\", methods={\"GET\"})\nbypass2 = BypassThrottleRule(path=\"/api/organizations\", methods={\"POST\"})\n\nglobal_throttle = HTTPThrottle(\n    \"api:v1\",\n    rate=\"1000/min\",\n    rules={bypass1, bypass2},  # Both bypass rules are attached at creation\n)\n</code></pre> <p>Rules passed here are attached directly to this throttle. They're deduplicated and sorted for optimal evaluation order automatically.</p>"},{"location":"advanced/rules/#adding-rules-after-initialization","title":"Adding Rules After Initialization","text":"<p>You can also attach rules to any registered throttle by its UID after the fact, using <code>add_rules()</code> on any throttle instance:</p> <pre><code>from traffik import HTTPThrottle\nfrom traffik.registry import BypassThrottleRule\n\nglobal_throttle = HTTPThrottle(\"api:v1\", rate=\"1000/min\")\n\n# Later, when setting up the users router:\nusers_throttle = HTTPThrottle(\"api:users\", rate=\"500/min\")\n\nbypass_GET_users = BypassThrottleRule(path=\"/api/v1/users\", methods={\"GET\"})\nusers_throttle.add_rules(\"api:v1\", bypass_GET_users)\n# ^ Attaches the rule to global_throttle (uid=\"api:v1\"), not to users_throttle\n</code></pre> <p>Notice the <code>add_rules</code> call: the first argument is the target throttle's UID, the throttle you want to modify. The rule is added to the global registry, and the target throttle will pick it up on its next <code>hit()</code> call.</p> <p>UIDs must be registered first</p> <p><code>add_rules(\"api:v1\", ...)</code> will raise a <code>ConfigurationError</code> if no throttle with UID <code>\"api:v1\"</code> has been created yet. Create throttles before attaching rules to them.</p> <p>By default, rules added via <code>add_rules()</code> are picked up on the first request after they're added and cached for efficiency. If you need rules to be re-fetched on every single request (rare), set <code>dynamic_rules=True</code> on the throttle.</p>"},{"location":"advanced/rules/#custom-predicate-rules","title":"Custom Predicate Rules","text":"<p>For conditions that path and methods can't express, such as \"only throttle premium users\" or \"skip throttling for internal service calls\", use a <code>predicate</code>:</p> <pre><code>from traffik.registry import ThrottleRule, BypassThrottleRule\n\n# Only apply this throttle to users on the \"premium\" tier\nasync def only_premium(connection) -&gt; bool:\n    user = getattr(connection.state, \"user\", None)\n    if user is None:\n        return False\n    return user.tier == \"premium\"\n\npremium_rule = ThrottleRule(predicate=only_premium)\npremium_throttle = HTTPThrottle(\"api:premium\", rate=\"5000/min\", rules={premium_rule})\n</code></pre> <p>Predicates can optionally accept a <code>context</code> argument:</p> <pre><code>async def only_premium_with_context(connection, context=None) -&gt; bool:\n    # context contains the throttle's merged context dict\n    tier = (context or {}).get(\"user_tier\")\n    return tier == \"premium\"\n</code></pre> <p>Traffik inspects the predicate's signature and passes <code>context</code> only if the function accepts it.</p>"},{"location":"advanced/rules/#rule-evaluation-order","title":"Rule Evaluation Order","text":"<p>Rules are evaluated in an order optimised for short-circuit performance. Traffik automatically sorts them:</p> <ol> <li><code>BypassThrottleRule</code> without predicate - fastest path; a frozenset lookup + regex match. Returns <code>False</code> on match (throttle skipped immediately).</li> <li><code>ThrottleRule</code> without predicate - same cost as above. Returns <code>False</code> on non-match (throttle skipped immediately).</li> <li><code>BypassThrottleRule</code> with predicate - async; predicate runs after path/method check passes.</li> <li><code>ThrottleRule</code> with predicate - async; slowest. Runs only if all cheaper rules passed.</li> </ol> <p>You don't need to think about this ordering, Traffik handles it. But it's good to know why cheap method/path rules should always be preferred over predicates when they can express the same condition.</p> <p><code>BypassThrottleRule</code> always wins first</p> <p><code>BypassThrottleRule</code> instances are always checked before <code>ThrottleRule</code> instances within the same priority tier. This short-circuit means: if any bypass rule matches, the throttle is skipped immediately without evaluating any <code>ThrottleRule</code> predicates. Keep bypass rules cheap and tight, they protect everything downstream.</p>"},{"location":"advanced/rules/#the-throttleregistry-and-global_registry","title":"The <code>ThrottleRegistry</code> and <code>GLOBAL_REGISTRY</code>","text":"<p>Every throttle registers itself in the <code>GLOBAL_REGISTRY</code>, an instance of <code>ThrottleRegistry</code>, on construction (unless you pass a custom <code>registry</code>). This is what makes <code>add_rules()</code> work across module boundaries without passing references around.</p> <pre><code>from traffik import HTTPThrottle\nfrom traffik.registry import GLOBAL_REGISTRY, ThrottleRegistry\n\n# Check if a throttle has been registered\nGLOBAL_REGISTRY.exist(\"api:v1\")   # True if HTTPThrottle(\"api:v1\", ...) was called\nGLOBAL_REGISTRY.exist(\"api:v99\")  # False\n\n# Add rules directly through the registry\nGLOBAL_REGISTRY.add_rules(\"api:v1\", bypass_rule)\n\n# You can also create isolated registries for testing or multi-tenant setups\ncustom_registry = ThrottleRegistry()\nthrottle = HTTPThrottle(\"api:v1\", rate=\"100/min\", registry=custom_registry)\n</code></pre> <p>The registry uses a re-entrant lock internally, so it's safe to register throttles and add rules from different threads during application startup.</p> <p>UID uniqueness</p> <p>Throttle UIDs must be globally unique within a registry. If you try to create two <code>HTTPThrottle</code> instances with the same UID, the second one raises a <code>ConfigurationError</code>. This is intentional, shared UIDs would cause state collisions in the backend.</p>"},{"location":"advanced/rules/#full-real-world-example","title":"Full Real-World Example","text":"<p>Here's the complete pattern for a versioned API with a global throttle and per-router carve-outs:</p> <pre><code>import typing\nfrom fastapi import APIRouter, Depends, FastAPI, Request\n\nfrom traffik import HTTPThrottle, Rate\nfrom traffik.backends.inmemory import InMemoryBackend\nfrom traffik.decorators import throttled\nfrom traffik.registry import BypassThrottleRule\n\napp = FastAPI(lifespan=InMemoryBackend().lifespan)\n\n# GLOBAL THROTTLE\n# GET: 1000/min, POST: 300/min for everything under /api/v1\n# Exceptions are carved out below with BypassThrottleRules.\n\nasync def global_rate(\n    connection: Request,\n    context: typing.Optional[typing.Dict[str, typing.Any]],\n) -&gt; Rate:\n    if connection.scope[\"method\"] == \"GET\":\n        return Rate.parse(\"1000/min\")\n    return Rate.parse(\"300/min\")\n\n\n# Carve-outs: global throttle should NOT apply to these\nbypass_GET_users = BypassThrottleRule(path=\"/api/v1/users\", methods={\"GET\"})\nbypass_POST_orgs = BypassThrottleRule(path=\"/api/v1/organizations\", methods={\"POST\"})\n\nglobal_throttle = HTTPThrottle(\n    \"api:v1\",\n    rate=global_rate,\n    rules={bypass_GET_users, bypass_POST_orgs},  # (1)!\n)\nmain_router = APIRouter(prefix=\"/api/v1\", dependencies=[Depends(global_throttle)])\n\n\nasync def get_user_id(connection: Request) -&gt; str:\n    return getattr(connection.state, \"user_id\", \"__anon__\")\n\n\n# USERS ROUTER \n# GET: 500/min (per-user), POST: unlimited\n\nasync def user_rate(\n    connection: Request,\n    context: typing.Optional[typing.Dict[str, typing.Any]],\n) -&gt; Rate:\n    if connection.scope[\"method\"] == \"GET\":\n        return Rate.parse(\"500/min\")\n    return Rate()  # Rate() with no args = unlimited = no throttling\n\n\nusers_throttle = HTTPThrottle(\n    \"api:users\",\n    rate=user_rate,\n    identifier=get_user_id,  # Per-user tracking (2)!\n)\nusers_router = APIRouter(prefix=\"/users\", dependencies=[Depends(users_throttle)])\n\n\n# ORGANIZATIONS ROUTER\n# GET: unlimited, POST: 600/min (per-user)\n\nasync def orgs_rate(\n    connection: Request,\n    context: typing.Optional[typing.Dict[str, typing.Any]],\n) -&gt; Rate:\n    if connection.scope[\"method\"] == \"POST\":\n        return Rate.parse(\"600/min\")\n    return Rate()  # Unlimited for GET\n\n\norgs_throttle = HTTPThrottle(\n    \"api:orgs\",\n    rate=orgs_rate,\n    identifier=get_user_id,\n)\norgs_router = APIRouter(\n    prefix=\"/organizations\",\n    dependencies=[Depends(orgs_throttle)],\n)\n\n\n@orgs_router.post(\"/\")\nasync def create_organization(data: typing.Any):\n    # Only orgs_throttle applies (POST; global is bypassed)\n    pass\n\n\n@orgs_router.get(\"/{org_id}\")\n@throttled(\n    HTTPThrottle(\"api:orgs:get\", rate=\"100/min\")  # (3)!\n)\nasync def get_organization(org_id: str):\n    # orgs_throttle + api:orgs:get throttle both apply here\n    pass\n\n\n# ASSEMBLE THE APP\nmain_router.include_router(users_router)\nmain_router.include_router(orgs_router)\napp.include_router(main_router)\n</code></pre> <ol> <li>Rules are passed at construction here. Alternatively, you can call <code>users_throttle.add_rules(\"api:v1\", bypass_GET_users)</code> after the fact, both approaches produce identical results.</li> <li>Without <code>identifier=get_user_id</code>, the throttle would use the remote IP address. Per-user throttling ensures authenticated users each get their own 500/min quota rather than sharing a per-IP pool.</li> <li>Using <code>@throttled(...)</code> stacks an additional throttle on top of the router-level <code>orgs_throttle</code>. Both run on <code>GET /{org_id}</code>. Use this for routes that need stricter limits than their parent router.</li> </ol>"},{"location":"advanced/rules/#what-happens-on-each-route","title":"What happens on each route","text":"Route Throttles that apply <code>GET /api/v1/users</code> <code>api:users</code> only (global bypassed) <code>POST /api/v1/users</code> <code>api:v1</code> global (300/min) + <code>api:users</code> unlimited <code>GET /api/v1/organizations</code> <code>api:v1</code> global (1000/min) + <code>api:orgs</code> unlimited <code>POST /api/v1/organizations</code> <code>api:orgs</code> (600/min) only (global bypassed) <code>GET /api/v1/organizations/{id}</code> <code>api:v1</code> global (1000/min) + <code>api:orgs</code> unlimited + <code>api:orgs:get</code> (100/min)"},{"location":"advanced/rules/#choosing-the-right-tool","title":"Choosing the Right Tool","text":"<p>There are three ways to make a throttle apply differently to different connections. Here's when to reach for each:</p> RulesDynamic IdentifiersPredicate Rules <p>Best when: The condition is based on the path or HTTP method. Structural routing concerns.</p> <pre><code># Apply only to admin routes\nThrottleRule(path=\"/api/admin/**\")\n\n# Skip for health checks\nBypassThrottleRule(path=\"/health\")\n</code></pre> <p>Rules are evaluated before any backend I/O. Zero quota is consumed when a rule skips a throttle. They're the cheapest way to make structural decisions.</p> <p>Best when: You want the same throttle to track different groups of clients separately by returning different identifier strings.</p> <pre><code>async def tier_identifier(connection: Request) -&gt; str:\n    user = connection.state.user\n    return f\"{user.tier}:{user.id}\"\n# premium:42 and free:42 get separate counters\n</code></pre> <p>Use this when the throttle logic is the same but you want per-group accounting, e.g., premium vs free tier users each get their own 1000/min pool.</p> <p>Best when: The condition involves async logic or data not available from the path/method alone, like user tier, feature flags, or tenant config.</p> <pre><code>async def only_external(connection) -&gt; bool:\n    return not getattr(connection.state, \"is_internal\", False)\n\nThrottleRule(predicate=only_external)\n</code></pre> <p>Predicates run after path/method checks and involve an <code>await</code>. Use them for cross-cutting concerns that can't be expressed structurally.</p> <p>Combine freely</p> <p>Rules, identifiers, and predicates compose. A single throttle can have multiple bypass rules (e.g., one for <code>/health</code>, one for internal IPs) and a custom identifier and a predicate, all working together. Traffik evaluates them in the correct order automatically.</p>"},{"location":"advanced/rules/#summary","title":"Summary","text":"Concept What it does <code>ThrottleRule(path, methods, predicate)</code> Apply throttle only when connection matches <code>BypassThrottleRule(path, methods, predicate)</code> Skip throttle when connection matches <code>BypassThrottleRule</code> checked first Short-circuits before any <code>ThrottleRule</code> in the same tier <code>rules={...}</code> in constructor Attach rules at throttle creation <code>throttle.add_rules(\"uid\", rule)</code> Attach rules to another throttle by UID after creation <code>*</code> in path pattern Matches one path segment (no <code>/</code>) <code>**</code> in path pattern Matches multiple segments (including <code>/</code>) Plain string path Prefix regex match <code>re.compile(...)</code> path Exact regex match <code>ThrottleRegistry</code> The class backing <code>GLOBAL_REGISTRY</code>; pass a custom instance via <code>registry=</code> <code>GLOBAL_REGISTRY.exist(\"uid\")</code> Check if a throttle UID is registered <code>dynamic_rules=True</code> Re-fetch registry rules on every request"},{"location":"advanced/statistics/","title":"Strategy Statistics","text":"<p>Sometimes you want to look at the rate limit counter without actually touching it. Maybe you want to add <code>X-RateLimit-Remaining</code> headers to every response. Maybe you want a <code>/usage</code> endpoint that tells clients where they stand. Maybe you want to feed data into Prometheus.</p> <p>That's what <code>throttle.stat()</code> is for.</p> <p>stat() is read-only</p> <p>Calling <code>stat()</code> never consumes quota. It reads the current state from the backend and returns it. Your clients can call a stats endpoint as often as they like \u2014 it won't move the rate limit needle.</p>"},{"location":"advanced/statistics/#basic-usage","title":"Basic Usage","text":"<pre><code>stat = await throttle.stat(request, context={...})\n</code></pre> <p><code>stat()</code> returns a <code>StrategyStat</code> object, or <code>None</code> if the strategy doesn't support stats (custom strategies) or if the client is exempt from throttling.</p> <pre><code>from fastapi import FastAPI, Request, Depends\nfrom traffik import HTTPThrottle\nfrom traffik.backends.redis import RedisBackend\n\nbackend = RedisBackend(\"redis://localhost:6379\", namespace=\"myapp\")\napp = FastAPI(lifespan=backend.lifespan)\n\nthrottle = HTTPThrottle(\"api:read\", rate=\"100/min\", backend=backend)\n\n@app.get(\"/items\")\nasync def get_items(request: Request = Depends(throttle)):\n    stat = await throttle.stat(request)\n    if stat:\n        print(f\"Hits remaining: {stat.hits_remaining}\")\n        print(f\"Wait time: {stat.wait_ms}ms\")\n    return {\"items\": [...]}\n</code></pre>"},{"location":"advanced/statistics/#strategystat-fields","title":"StrategyStat Fields","text":"Field Type Description <code>key</code> <code>Stringable</code> The full namespaced throttle key for this client <code>rate</code> <code>Rate</code> The rate limit definition (e.g., 100 requests/minute) <code>hits_remaining</code> <code>float</code> Quota left in the current window. Can be <code>inf</code> for unlimited. <code>wait_ms</code> <code>float</code> Milliseconds until the client can send again (0 if not throttled) <code>metadata</code> <code>TypedDict \\| None</code> Strategy-specific data (window timestamps, token counts, etc.)"},{"location":"advanced/statistics/#typed-metadata-per-strategy","title":"Typed Metadata Per Strategy","text":"<p>Each built-in strategy exposes rich metadata you can use for observability.</p> FixedWindowSlidingWindowLogSlidingWindowCounterTokenBucketLeakyBucket <pre><code>from traffik.strategies.fixed_window import FixedWindowStatMetadata\n\nstat = await throttle.stat(request)\nif stat and stat.metadata:\n    meta: FixedWindowStatMetadata = stat.metadata\n    # meta[\"strategy\"]          -&gt; \"fixed_window\"\n    # meta[\"window_start_ms\"]   -&gt; window start (Unix ms)\n    # meta[\"window_end_ms\"]     -&gt; window end (Unix ms)\n    # meta[\"current_count\"]     -&gt; requests so far in this window\n</code></pre> <pre><code>from traffik.strategies.sliding_window import SlidingWindowLogStatMetadata\n\nstat = await throttle.stat(request)\nif stat and stat.metadata:\n    meta: SlidingWindowLogStatMetadata = stat.metadata\n    # meta[\"strategy\"]          -&gt; \"sliding_window_log\"\n    # meta[\"window_start_ms\"]   -&gt; rolling window start\n    # meta[\"entry_count\"]       -&gt; number of log entries in window\n    # meta[\"current_cost_sum\"]  -&gt; total cost in window\n    # meta[\"oldest_entry_ms\"]   -&gt; timestamp of oldest entry\n</code></pre> <pre><code>from traffik.strategies.sliding_window import SlidingWindowCounterStatMetadata\n\nstat = await throttle.stat(request)\nif stat and stat.metadata:\n    meta: SlidingWindowCounterStatMetadata = stat.metadata\n    # meta[\"strategy\"]          -&gt; \"sliding_window_counter\"\n    # meta[\"current_window_id\"] -&gt; current window identifier\n</code></pre> <pre><code>from traffik.strategies.token_bucket import TokenBucketStatMetadata\n\nstat = await throttle.stat(request)\nif stat and stat.metadata:\n    meta: TokenBucketStatMetadata = stat.metadata\n    # meta[\"strategy\"]             -&gt; \"token_bucket\"\n    # meta[\"tokens\"]               -&gt; current token count\n    # meta[\"capacity\"]             -&gt; bucket capacity (burst size)\n    # meta[\"refill_rate_per_ms\"]   -&gt; tokens added per millisecond\n</code></pre> <pre><code>from traffik.strategies.leaky_bucket import LeakyBucketStatMetadata\n\nstat = await throttle.stat(request)\nif stat and stat.metadata:\n    meta: LeakyBucketStatMetadata = stat.metadata\n    # meta[\"strategy\"] -&gt; \"leaky_bucket\"\n</code></pre>"},{"location":"advanced/statistics/#adding-rate-limit-headers-to-responses","title":"Adding Rate Limit Headers to Responses","text":"<p>The most common use of stats is adding standard <code>X-RateLimit-*</code> headers so clients know where they stand.</p> <pre><code>from fastapi import FastAPI, Request, Response, Depends\nfrom traffik import HTTPThrottle\nfrom traffik.backends.redis import RedisBackend\nimport math\n\nbackend = RedisBackend(\"redis://localhost:6379\", namespace=\"myapp\")\napp = FastAPI(lifespan=backend.lifespan)\n\nthrottle = HTTPThrottle(\"api\", rate=\"100/min\", backend=backend)\n\n\n@app.get(\"/items\")\nasync def get_items(\n    request: Request = Depends(throttle),\n    response: Response = None,\n):\n    stat = await throttle.stat(request)\n    if stat:\n        response.headers[\"X-RateLimit-Limit\"] = str(int(stat.rate.limit))\n        response.headers[\"X-RateLimit-Remaining\"] = str(max(int(stat.hits_remaining), 0))\n        if stat.wait_ms &gt; 0:\n            retry_after = math.ceil(stat.wait_ms / 1000)\n            response.headers[\"Retry-After\"] = str(retry_after)\n\n    return {\"items\": [...]}\n</code></pre> <p>Built-in header support</p> <p>You can also configure headers directly on the throttle using the <code>headers</code> parameter and the <code>Header</code> API \u2014 Traffik will resolve them automatically on each throttled response. See the Headers reference for details.</p>"},{"location":"advanced/statistics/#building-a-usage-endpoint","title":"Building a /usage Endpoint","text":"<p>Give your clients a dedicated endpoint to check their quota without making a real request:</p> <pre><code>from fastapi import FastAPI, Request\nfrom traffik import HTTPThrottle\nfrom traffik.backends.redis import RedisBackend\n\nbackend = RedisBackend(\"redis://localhost:6379\", namespace=\"myapp\")\napp = FastAPI(lifespan=backend.lifespan)\n\napi_throttle = HTTPThrottle(\"api:standard\", rate=\"1000/hour\", backend=backend)\n\n\n@app.get(\"/usage\")\nasync def get_usage(request: Request):\n    \"\"\"Returns quota info without consuming quota.\"\"\"\n    stat = await api_throttle.stat(request)\n\n    if stat is None:\n        return {\"message\": \"No rate limit info available (you may be exempt)\"}\n\n    return {\n        \"limit\": int(stat.rate.limit),\n        \"remaining\": max(int(stat.hits_remaining), 0),\n        \"reset_in_ms\": stat.wait_ms,\n        \"window\": str(stat.rate),\n    }\n</code></pre> <p>Clients can hit <code>/usage</code> as often as they like \u2014 it reads the counter but never writes to it.</p>"},{"location":"advanced/statistics/#prometheus-metrics-example","title":"Prometheus Metrics Example","text":"<p>If you're exporting metrics to Prometheus, stats give you everything you need:</p> <pre><code>from prometheus_client import Gauge, Counter\nfrom fastapi import FastAPI, Request, Depends\nfrom traffik import HTTPThrottle\nfrom traffik.backends.redis import RedisBackend\n\nbackend = RedisBackend(\"redis://localhost:6379\", namespace=\"myapp\")\napp = FastAPI(lifespan=backend.lifespan)\n\nthrottle = HTTPThrottle(\"api\", rate=\"1000/hour\", backend=backend)\n\nquota_remaining = Gauge(\n    \"traffik_quota_remaining\",\n    \"Quota remaining for the current window\",\n    [\"throttle_uid\"],\n)\nthrottle_hits = Counter(\n    \"traffik_requests_total\",\n    \"Total requests processed by throttle\",\n    [\"throttle_uid\"],\n)\n\n\n@app.middleware(\"http\")\nasync def collect_throttle_metrics(request: Request, call_next):\n    response = await call_next(request)\n\n    stat = await throttle.stat(request)\n    if stat:\n        quota_remaining.labels(throttle_uid=\"api\").set(stat.hits_remaining)\n        throttle_hits.labels(throttle_uid=\"api\").inc()\n\n    return response\n</code></pre> <p>Avoid calling stat() in tight loops</p> <p>Each <code>stat()</code> call hits the backend. In a middleware that runs on every request, that's fine \u2014 it's one extra read per request. But calling it in a loop for many keys at once is a different story. Batch them if you need to.</p>"},{"location":"advanced/statistics/#summary","title":"Summary","text":"<ul> <li><code>stat()</code> is your window into the rate limiter's current state \u2014 without modifying it</li> <li>Returns <code>None</code> for exempt clients and strategies without stat support</li> <li>Use it for response headers, usage endpoints, dashboards, and metrics</li> <li>All built-in strategies support <code>stat()</code> with typed metadata</li> </ul>"},{"location":"advanced/throttled-handlers/","title":"Custom Throttled Handlers","text":"<p>So a client hits the limit. What happens next?</p> <p>By default, Traffik handles it for you: HTTP throttles raise a <code>429 Too Many Requests</code> exception, and WebSocket throttles send a JSON message. For most apps that's perfectly fine.</p> <p>But \"most apps\" isn't \"every app\". Maybe you want to include a <code>Retry-After</code> header with an exact timestamp. Maybe your WebSocket protocol has a specific message format for rate limit events. Maybe your API team has strong opinions about error response shapes.</p> <p>That's what custom throttled handlers are for.</p>"},{"location":"advanced/throttled-handlers/#default-behavior","title":"Default Behavior","text":"<p>Before you customize, it helps to know what you're replacing:</p> <ul> <li>HTTPThrottle: Raises <code>ConnectionThrottled</code> which becomes an HTTP <code>429</code> response with a plain text body.</li> <li>WebSocketThrottle: If the connection is open, sends a JSON message and keeps the connection alive. If the connection isn't open yet, it raises <code>ConnectionThrottled</code>.</li> </ul> <p>The WebSocket default message looks like this:</p> <pre><code>{\n    \"type\": \"rate_limit\",\n    \"error\": \"Too many messages\",\n    \"retry_after\": 5\n}\n</code></pre>"},{"location":"advanced/throttled-handlers/#handler-signature","title":"Handler Signature","text":"<p>All throttled handlers \u2014 HTTP and WebSocket \u2014 share the same signature:</p> <pre><code>async def handler(\n    connection,   # Request or WebSocket\n    wait_ms,      # float: milliseconds until client can retry\n    throttle,     # the Throttle instance that triggered this\n    context,      # dict: throttle context\n) -&gt; Any:\n    ...\n</code></pre> <p>The <code>wait_ms</code> value is already in milliseconds. Convert to seconds with <code>math.ceil(wait_ms / 1000)</code> for human-friendly messages.</p>"},{"location":"advanced/throttled-handlers/#custom-http-handler","title":"Custom HTTP Handler","text":""},{"location":"advanced/throttled-handlers/#custom-headers-and-status-code","title":"Custom Headers and Status Code","text":"<pre><code>import math\nfrom fastapi import FastAPI, Request, Depends\nfrom starlette.responses import JSONResponse\nfrom traffik import HTTPThrottle\nfrom traffik.backends.redis import RedisBackend\n\nbackend = RedisBackend(\"redis://localhost:6379\", namespace=\"myapp\")\napp = FastAPI(lifespan=backend.lifespan)\n\n\nasync def my_http_handler(request, wait_ms, throttle, context):\n    retry_after = math.ceil(wait_ms / 1000)\n    raise Exception  # We raise, not return, from HTTP handlers\n\n# Actually: HTTP handlers raise an HTTP response, not return one.\n# The cleanest approach is to raise a Starlette HTTPException with custom headers:\n\nfrom starlette.exceptions import HTTPException as StarletteHTTPException\n\nasync def rate_limit_handler(request, wait_ms, throttle, context):\n    retry_after = math.ceil(wait_ms / 1000)\n    raise StarletteHTTPException(\n        status_code=429,\n        detail={\n            \"error\": \"rate_limit_exceeded\",\n            \"message\": f\"You've hit the {throttle.uid!r} limit. Slow down!\",\n            \"retry_after_seconds\": retry_after,\n            \"limit\": throttle.rate.limit if not callable(throttle.rate) else \"dynamic\",\n        },\n        headers={\n            \"Retry-After\": str(retry_after),\n            \"X-RateLimit-Reset\": str(retry_after),\n        },\n    )\n\n\nthrottle = HTTPThrottle(\n    \"api:read\",\n    rate=\"100/min\",\n    backend=backend,\n    handle_throttled=rate_limit_handler,\n)\n</code></pre>"},{"location":"advanced/throttled-handlers/#include-stats-in-the-response","title":"Include Stats in the Response","text":"<p>If you want to include <code>X-RateLimit-Remaining</code> in the error response, call <code>throttle.stat()</code> inside the handler:</p> <pre><code>import math\nfrom starlette.exceptions import HTTPException as StarletteHTTPException\n\nasync def rich_rate_limit_handler(request, wait_ms, throttle, context):\n    retry_after = math.ceil(wait_ms / 1000)\n\n    # Read stats without consuming quota\n    stat = await throttle.stat(request, context)\n    headers = {\"Retry-After\": str(retry_after)}\n    if stat:\n        headers[\"X-RateLimit-Limit\"] = str(int(stat.rate.limit))\n        headers[\"X-RateLimit-Remaining\"] = \"0\"\n\n    raise StarletteHTTPException(\n        status_code=429,\n        detail={\"error\": \"too_many_requests\", \"retry_after\": retry_after},\n        headers=headers,\n    )\n</code></pre>"},{"location":"advanced/throttled-handlers/#custom-websocket-handler","title":"Custom WebSocket Handler","text":"<p>WebSocket handlers have two styles: send a message (recommended) or raise an exception.</p>"},{"location":"advanced/throttled-handlers/#option-1-send-a-message-recommended","title":"Option 1: Send a Message (Recommended)","text":"<p>The connection stays open. The client gets notified and can back off gracefully:</p> <pre><code>import math\nfrom starlette.websockets import WebSocketState\n\nasync def ws_rate_limit_handler(websocket, wait_ms, throttle, context):\n    retry_after = math.ceil(wait_ms / 1000)\n\n    if websocket.application_state == WebSocketState.CONNECTED:\n        await websocket.send_json({\n            \"type\": \"rate_limit\",\n            \"error\": \"message_rate_exceeded\",\n            \"retry_after\": retry_after,\n            \"throttle\": throttle.uid,\n        })\n    # Don't raise \u2014 just return. The connection stays alive.\n</code></pre> <p>Performance note</p> <p>Sending a message is significantly faster than raising an exception in WebSocket handlers. Exception propagation has overhead. For high-frequency message throttling (think chat apps or telemetry streams), the send-and-return pattern is measurably better.</p>"},{"location":"advanced/throttled-handlers/#option-2-raise-an-exception-not-recommended","title":"Option 2: Raise an Exception (Not Recommended)","text":"<p>This closes the connection from the server side. Only use it when you genuinely want to disconnect the client:</p> <pre><code>from traffik.exceptions import ConnectionThrottled\n\nasync def ws_strict_handler(websocket, wait_ms, throttle, context):\n    retry_after = math.ceil(wait_ms / 1000)\n    # This raises ConnectionThrottled which terminates the WebSocket\n    raise ConnectionThrottled(\n        wait_period=retry_after,\n        detail=\"Too many messages. Reconnect after the wait period.\",\n    )\n</code></pre> <p>Raising in WebSocket handlers</p> <p>Raising inside a WebSocket throttled handler closes the connection. That's usually not what you want for message-level throttling. Use it only for connection-level throttling (e.g., max connections per IP at connect time).</p>"},{"location":"advanced/throttled-handlers/#close-the-connection-on-throttle-code-1008","title":"Close the Connection on Throttle (Code 1008)","text":"<p>If you want to explicitly close the WebSocket with a policy violation close code:</p> <pre><code>from starlette.websockets import WebSocketState, WebSocketDisconnect\nimport math\n\nasync def ws_close_handler(websocket, wait_ms, throttle, context):\n    retry_after = math.ceil(wait_ms / 1000)\n    if websocket.application_state == WebSocketState.CONNECTED:\n        await websocket.close(code=1008, reason=f\"Rate limit exceeded. Retry after {retry_after}s.\")\n</code></pre> <p>WebSocket close code <code>1008</code> is the standard \"Policy Violation\" code \u2014 the right semantic choice for rate limiting.</p>"},{"location":"advanced/throttled-handlers/#backend-level-default-handler","title":"Backend-Level Default Handler","text":"<p>Instead of setting a handler on each throttle, you can set a default at the backend level. All throttles that use this backend will inherit it (unless they override it with their own handler):</p> <pre><code>from traffik.backends.redis import RedisBackend\nimport math\nfrom starlette.exceptions import HTTPException as StarletteHTTPException\n\nasync def default_handler(connection, wait_ms, throttle, context):\n    retry_after = math.ceil(wait_ms / 1000)\n    raise StarletteHTTPException(\n        status_code=429,\n        detail=f\"Rate limit exceeded. Retry in {retry_after}s.\",\n        headers={\"Retry-After\": str(retry_after)},\n    )\n\nbackend = RedisBackend(\n    \"redis://localhost:6379\",\n    namespace=\"myapp\",\n    handle_throttled=default_handler,   # Applied to all throttles on this backend\n)\n</code></pre> <p>Throttle-level handlers take precedence over backend-level handlers. If a throttle defines its own <code>handle_throttled</code>, the backend default is ignored for that throttle.</p>"},{"location":"advanced/throttled-handlers/#quick-reference","title":"Quick Reference","text":"Scenario Recommendation Custom HTTP error shape Raise <code>HTTPException</code> with custom <code>detail</code> and <code>headers</code> WebSocket: notify but keep alive Send JSON message, then return WebSocket: disconnect client <code>await websocket.close(code=1008)</code> Apply to all throttles on a backend Set <code>handle_throttled</code> on the backend Per-throttle override Pass <code>handle_throttled</code> to <code>HTTPThrottle(...)</code>"},{"location":"core-concepts/","title":"Core Concepts","text":"<p>Welcome to the heart of Traffik. Rate limiting can sound intimidating, but Traffik is built around four simple, composable ideas. Master these and you master everything.</p>"},{"location":"core-concepts/#the-four-building-blocks","title":"The Four Building Blocks","text":""},{"location":"core-concepts/#rate","title":"Rate","text":"<p>A Rate is the answer to \"how much traffic is too much?\" It bundles a request limit together with a time window, \"100 requests per minute\", \"5 per second\", \"1000 per hour\". You can express it as a human-friendly string like <code>\"100/min\"</code> or construct it as a <code>Rate</code> object when you need a more complex period.</p> <p>Rates in depth \u2192</p>"},{"location":"core-concepts/#backend","title":"Backend","text":"<p>A Backend is where Traffik keeps score. Every time a request comes in, the backend increments a counter, checks whether the limit has been crossed, and hands back a verdict. Traffik ships with three backends: an <code>InMemoryBackend</code> for development and single-process apps, a <code>RedisBackend</code> for production and distributed systems, and a <code>MemcachedBackend</code> for when you already have Memcached in your stack.</p> <p>Backends in depth \u2192</p>"},{"location":"core-concepts/#strategy","title":"Strategy","text":"<p>A Strategy is the algorithm that decides how to count. Should Traffik use a simple fixed window that resets every minute? A sliding window that's immune to boundary bursts? A token bucket that allows occasional burst traffic? There are seven strategies built in, from the dead-simple <code>FixedWindow</code> to the telecom-grade <code>GCRA</code>, plus a whole collection of advanced strategies in <code>traffik.strategies.custom</code>.</p> <p>Strategies in depth \u2192</p>"},{"location":"core-concepts/#identifier","title":"Identifier","text":"<p>An Identifier is how Traffik knows who is who. By default it reads the client's IP address, but you can swap in anything: a user ID extracted from a JWT, an API key from a header, a tenant slug from the URL. You can even return the special <code>EXEMPTED</code> sentinel to let a specific client sail straight through without touching the backend at all.</p> <p>Identifiers in depth \u2192</p>"},{"location":"core-concepts/#putting-it-together","title":"Putting It Together","text":"<pre><code>from traffik import HTTPThrottle\nfrom traffik.backends.redis import RedisBackend\n\nbackend = RedisBackend(\"redis://localhost:6379\", namespace=\"myapp\")\n\nthrottle = HTTPThrottle(\n    uid=\"my-endpoint\",\n    rate=\"100/min\",       # Rate\n    backend=backend,      # Backend\n    # strategy defaults to FixedWindow\n    # identifier defaults to remote IP\n)\n</code></pre> <p>Once you understand these four things, Rate, Backend, Strategy, Identifier, you understand Traffik.</p>"},{"location":"core-concepts/backends/","title":"Backends","text":"<p>A backend is where Traffik keeps score. Every increment, every counter check, every lock acquisition goes through the backend. Choosing the right one is usually the first architectural decision you'll make when adding Traffik to a project.</p>"},{"location":"core-concepts/backends/#choosing-a-backend","title":"Choosing a backend","text":"Feature InMemory Redis Memcached Best for Dev, tests, single process Production, distributed Existing Memcached stacks Persistence No Optional (<code>persistent=True</code>) Optional (<code>persistent=True</code> &amp; <code>track_keys=True</code>(enables resets)) Distributed No Yes Yes Lock type asyncio RLock Redis Lua / Redlock Memcached <code>add</code> CAS Overhead Lowest Low (Lua scripts, pipelining) Low Requires extra dep No <code>redis</code> + <code>pottery</code> <code>aiomcache</code> <code>reset()</code> / <code>clear()</code> Full Full (Lua SCAN) Only when <code>track_keys=True</code>"},{"location":"core-concepts/backends/#inmemory-backend","title":"InMemory backend","text":"<p>The <code>InMemoryBackend</code> stores everything in process memory using sharded <code>OrderedDict</code> stores. It is not suitable for multi-process or distributed deployments, but it is perfect for development and testing.</p> <pre><code>from traffik.backends.inmemory import InMemoryBackend\n\nbackend = InMemoryBackend(\n    namespace=\"myapp\",          # Key prefix for all throttle keys\n    persistent=False,           # Clear data on context exit (default)\n    on_error=\"throttle\",        # \"allow\" | \"throttle\" | \"raise\" | callable\n    number_of_shards=3,         # Shard count for concurrent access (default 3)\n    cleanup_frequency=10.0,     # Seconds between expired-key sweeps (default 10.0)\n    lock_kind=\"unfair\",         # \"fair\" | \"unfair\" (default \"unfair\")\n    lock_blocking=True,         # Block when acquiring locks\n    lock_ttl=None,              # Lock TTL in seconds (None = no expiry)\n    lock_blocking_timeout=None, # Max wait for locks in seconds\n)\n</code></pre> <p>Characteristics:</p> <ul> <li>Lock striping via shards reduces contention significantly, especially when hitting multiple keys simultaneously.</li> <li>A background cleanup task (configurable via <code>cleanup_frequency</code>) sweeps expired   keys on a schedule. Set it to <code>None</code> to disable background cleanup; Traffik will   still lazily evict expired entries on reads.</li> <li><code>lock_kind=\"fair\"</code> gives strict FIFO ordering across tasks at the cost of slightly   higher overhead.</li> </ul> <p>When to use: Local development, unit tests, or genuinely single-process deployments where you never need to share state between workers.</p> <p>Always use InMemory in tests</p> <p>Swap out your production backend for <code>InMemoryBackend</code> in your test suite. It requires no external services, resets itself cleanly between test runs, and adds no I/O latency. Your tests will be fast enough to make you smile.</p>"},{"location":"core-concepts/backends/#redis-backend","title":"Redis backend","text":"<p><code>RedisBackend</code> is the go-to choice for production. It uses <code>redis.asyncio</code> for all operations, pre-loads Lua scripts for atomic increment-with-TTL, and supports two distinct distributed lock implementations.</p>"},{"location":"core-concepts/backends/#from-a-url","title":"From a URL","text":"<pre><code>from traffik.backends.redis import RedisBackend\n\nbackend = RedisBackend(\n    \"redis://localhost:6379\",\n    namespace=\"myapp\",\n    persistent=False,\n    on_error=\"throttle\",\n    lock_type=\"redis\",      # \"redis\" (default) or \"redlock\"\n    lock_blocking=True,\n    lock_ttl=None,\n    lock_blocking_timeout=None,\n)\n</code></pre>"},{"location":"core-concepts/backends/#from-a-factory-function","title":"From a factory function","text":"<p>When you need full control over the connection (connection pools, TLS, password, etc.), pass an async factory instead of a URL:</p> <pre><code>import redis.asyncio as aioredis\nfrom traffik.backends.redis import RedisBackend\n\nasync def get_redis():\n    return await aioredis.from_url(\n        \"redis://:secretpassword@redis-host:6379/0\",\n        decode_responses=True,\n        max_connections=20,\n    )\n\nbackend = RedisBackend(\n    get_redis,       # async callable, not a string\n    namespace=\"myapp\",\n)\n</code></pre>"},{"location":"core-concepts/backends/#lock-types","title":"Lock types","text":"<code>lock_type</code> Algorithm Best for <code>\"redis\"</code> SET NX EX + Lua fencing Single Redis instance, lowest latency <code>\"redlock\"</code> Redlock (via <code>pottery.AIORedlock</code>) Redis clusters, multiple instances <p>Redlock is slower by design</p> <p>Redlock involves multiple round-trips across several Redis nodes. Unless you are running a genuine Redis cluster with multiple masters, stick with <code>lock_type=\"redis\"</code>. The added latency of Redlock is rarely worth it for a single node.</p> <p>Characteristics:</p> <ul> <li>Atomic <code>increment_with_ttl</code> is implemented as a single Lua script, no race   conditions between increment and expire.</li> <li><code>multi_get</code> uses Redis <code>MGET</code> (one round-trip for multiple keys).</li> <li><code>multi_set</code> uses a Redis pipeline with <code>MULTI/EXEC</code> for atomicity.</li> <li><code>clear()</code> / <code>reset()</code> scan and delete namespace keys via a Lua script to avoid   blocking the server.</li> </ul>"},{"location":"core-concepts/backends/#memcached-backend","title":"Memcached backend","text":"<p><code>MemcachedBackend</code> uses <code>aiomcache</code> for async Memcached operations. It is a solid choice when your stack already runs Memcached and adding Redis would increase operational complexity.</p> <pre><code>from traffik.backends.memcached import MemcachedBackend\n\n# From explicit host/port\nbackend = MemcachedBackend(\n    host=\"localhost\",\n    port=11211,\n    pool_size=2,\n    pool_minsize=1,\n    namespace=\":memcached:\",\n    persistent=False,\n    on_error=\"throttle\",\n    lock_blocking=True,\n    lock_ttl=None,\n    lock_blocking_timeout=None,\n    track_keys=False,  # see below\n)\n\n# Or from a URL\nbackend = MemcachedBackend(\n    url=\"memcached://localhost:11211\",\n    namespace=\":memcached:\",\n)\n</code></pre> <p>Characteristics:</p> <ul> <li>Locks are implemented using Memcached's atomic <code>add</code> operation (add only succeeds   if the key does not exist), with a fencing token for ownership verification.</li> <li>Locks are instance-bound, not task-reentrant the way Redis locks are.</li> <li>Memcached keys are limited to 250 bytes, keep your <code>namespace</code> short.</li> </ul>"},{"location":"core-concepts/backends/#the-track_keys-limitation","title":"The <code>track_keys</code> limitation","text":"<p>Memcached has no equivalent of Redis <code>SCAN</code>, so Traffik cannot natively list all keys in a namespace. The <code>clear()</code> method, called during non-persistent context teardown, is therefore a no-op by default.</p> <p>Enable <code>track_keys=True</code> to have Traffik maintain a side-car key that records every key it sets:</p> <pre><code>backend = MemcachedBackend(\n    host=\"localhost\",\n    port=11211,\n    namespace=\":memcached:\",\n    track_keys=True,  # enables clear() at the cost of extra writes\n)\n</code></pre> <p><code>track_keys=True</code> adds overhead and is not 100% reliable</p> <p>Every <code>set</code> call writes an extra key to the tracking list. In high-throughput scenarios, this tracking list can become a bottleneck and may miss keys under concurrent writes. Only enable it if you genuinely need <code>clear()</code> to work on the Memcached backend. An alternative: if Memcached is dedicated to Traffik, you can override <code>clear()</code> in a subclass to call <code>flush_all()</code> instead.</p>"},{"location":"core-concepts/backends/#backend-lifecycle","title":"Backend lifecycle","text":"<p>Every backend needs to be started before use and closed when you are done. Traffik provides three patterns; pick the one that fits your framework.</p> Lifespan context manager (recommended)Manual async withManual initialize / close <pre><code>from contextlib import asynccontextmanager\nfrom fastapi import FastAPI\nfrom traffik.backends.redis import RedisBackend\n\nbackend = RedisBackend(\"redis://localhost:6379\", namespace=\"myapp\")\n\n@asynccontextmanager\nasync def lifespan(app: FastAPI):\n    async with backend.lifespan(app):\n        yield\n\napp = FastAPI(lifespan=lifespan)\n</code></pre> <pre><code>async with backend(app) as b:\n    # backend is initialized and attached to app.state\n    # on exit: reset (if not persistent) then close\n    ...\n</code></pre> <pre><code>await backend.initialize()\n# ... use backend ...\nawait backend.close()\n</code></pre>"},{"location":"core-concepts/backends/#without-asgi-lifespan-scripts-tests-cli-tools","title":"Without ASGI lifespan (scripts, tests, CLI tools)","text":"<p>When you are not running an ASGI application, for example in a standalone script, a CLI tool, or a test that doesn't need a full app, you can use the backend as an async context manager directly without passing an <code>app</code>:</p> <pre><code>from traffik.backends.inmemory import InMemoryBackend\n\nbackend = InMemoryBackend(namespace=\"myapp\")\n\nasync def main():\n    async with backend():\n        # backend is ready; use throttles here\n        pass\n</code></pre> <p>This initialises the backend on entry and closes it (calling <code>reset()</code> if <code>persistent=False</code>) on exit. No ASGI app, no lifespan fixture required. This pattern is particularly useful for one-off scripts, data migration tools, or test helpers that need to exercise throttle logic without spinning up a full server.</p>"},{"location":"core-concepts/backends/#persistence","title":"Persistence","text":"<p>By default, <code>persistent=False</code>, Traffik calls <code>reset()</code> on the backend when the context exits. This wipes all throttle counters, which is what you usually want between test runs and application restarts.</p> <p>Set <code>persistent=True</code> to keep counter state alive across restarts:</p> <pre><code>backend = RedisBackend(\n    \"redis://localhost:6379\",\n    namespace=\"myapp\",\n    persistent=True,  # counters survive restarts\n)\n</code></pre>"},{"location":"core-concepts/backends/#the-on_error-parameter","title":"The <code>on_error</code> parameter","text":"<p>All three backends share the same error-handling knob:</p> Value Behaviour on backend error <code>\"throttle\"</code> Treat the request as if it exceeded the limit (safe default) <code>\"allow\"</code> Let the request through (optimistic) <code>\"raise\"</code> Propagate the exception to your exception handler <code>callable</code> Call your function <code>(connection, exc_info) -&gt; wait_ms</code>"},{"location":"core-concepts/identifiers/","title":"Identifiers","text":"<p>An identifier is how Traffik knows who is who. Before any strategy runs, Traffik calls the identifier function to get a string key that represents the caller. All counter increments, lock names, and backend keys are namespaced under that key.</p> <p>Get the identifier right and you get per-user, per-tenant, or per-API-key rate limiting. Return the wrong thing and everyone shares a single counter.</p>"},{"location":"core-concepts/identifiers/#default-identifier-ip-address","title":"Default identifier: IP address","text":"<p>If you do not provide an identifier, Traffik uses <code>get_remote_address()</code>, which reads (in order):</p> <ol> <li>The first value in the <code>X-Forwarded-For</code> header</li> <li>The <code>remote-addr</code> header</li> <li><code>connection.client.host</code></li> </ol> <p>The fallback when none of those are available is <code>ANONYMOUS_IDENTIFIER</code> (<code>\"__anonymous__\"</code>).</p> <pre><code>from traffik import HTTPThrottle\n\n# No identifier \u2192 defaults to IP address\nthrottle = HTTPThrottle(uid=\"my-api\", rate=\"100/min\")\n</code></pre>"},{"location":"core-concepts/identifiers/#custom-identifiers","title":"Custom identifiers","text":"<p>An identifier is any <code>async</code> function that takes an <code>HTTPConnection</code> and returns something that can be converted to a string:</p> <pre><code>async def my_identifier(connection: HTTPConnection) -&gt; str:\n    ...\n</code></pre> <p>Pass it to the throttle:</p> <pre><code>from traffik import HTTPThrottle\n\nthrottle = HTTPThrottle(\n    uid=\"my-api\",\n    rate=\"100/min\",\n    identifier=my_identifier,\n)\n</code></pre> <p>You can also set a default identifier at the backend level, which applies to all throttles that use that backend and do not specify their own:</p> <pre><code>from traffik.backends.redis import RedisBackend\n\nbackend = RedisBackend(\n    \"redis://localhost:6379\",\n    namespace=\"myapp\",\n    identifier=my_identifier,  # backend-level default\n)\n</code></pre>"},{"location":"core-concepts/identifiers/#identifier-examples","title":"Identifier examples","text":"IP-based (explicit)User ID from JWTAPI key from headerTenant ID from subdomain <pre><code>from starlette.requests import HTTPConnection\nfrom traffik.utils import get_remote_address\nfrom traffik.config import ANONYMOUS_IDENTIFIER\n\nasync def ip_identifier(connection: HTTPConnection) -&gt; str:\n    return get_remote_address(connection) or ANONYMOUS_IDENTIFIER\n</code></pre> <pre><code>from starlette.requests import Request\n\nasync def user_identifier(connection: Request) -&gt; str:\n    user = connection.state.user  # populated by your auth middleware\n    if user is None:\n        return \"anonymous\"\n    return f\"user:{user.id}\"\n</code></pre> <pre><code>from starlette.requests import HTTPConnection\n\nasync def api_key_identifier(connection: HTTPConnection) -&gt; str:\n    api_key = connection.headers.get(\"X-API-Key\")\n    if not api_key:\n        return \"no-key\"\n    return f\"apikey:{api_key}\"\n</code></pre> <pre><code>from starlette.requests import HTTPConnection\n\nasync def tenant_identifier(connection: HTTPConnection) -&gt; str:\n    host = connection.headers.get(\"host\", \"\")\n    tenant = host.split(\".\")[0]  # e.g. \"acme\" from \"acme.example.com\"\n    return f\"tenant:{tenant}\"\n</code></pre>"},{"location":"core-concepts/identifiers/#exempted-bypassing-all-throttle-logic","title":"<code>EXEMPTED</code>: bypassing all throttle logic","text":"<p>Return the <code>EXEMPTED</code> sentinel to completely bypass throttling for a connection. Traffik will not call the strategy, will not touch the backend, and will not increment any counter. The overhead is essentially zero.</p> <pre><code>from starlette.requests import HTTPConnection\nfrom traffik.types import EXEMPTED\n\nasync def identifier_with_allowlist(connection: HTTPConnection):\n    ip = connection.headers.get(\"x-forwarded-for\", \"\").split(\",\")[0].strip()\n\n    # Internal health checkers, CI runners, etc.\n    if ip in {\"10.0.0.1\", \"10.0.0.2\"}:\n        return EXEMPTED  # this connection is completely untouched\n\n    return ip\n</code></pre> <p>EXEMPTED is the right tool for internal traffic</p> <p>Allowlisting via <code>EXEMPTED</code> costs nothing. Returning an identifier that you then map to an unlimited <code>Rate</code> still touches the backend and runs the strategy. When you want zero overhead, return <code>EXEMPTED</code>.</p>"},{"location":"core-concepts/identifiers/#anonymous_identifier-the-fallback-constant","title":"<code>ANONYMOUS_IDENTIFIER</code>: the fallback constant","text":"<p>When <code>get_remote_address()</code> cannot determine an IP address (e.g., a Unix socket connection, a test client without a client address), it returns <code>None</code>. The default identifier maps that <code>None</code> to the <code>ANONYMOUS_IDENTIFIER</code> constant, which is the string <code>\"__anonymous__\"</code>.</p> <p>All anonymous connections share a single counter, so be aware that a flooded anonymous connection will consume the rate limit for all other anonymous connections. In most real deployments this is not an issue because the IP header is always present.</p> <pre><code>from traffik.config import ANONYMOUS_IDENTIFIER\n\n# ANONYMOUS_IDENTIFIER == \"__anonymous__\"\n</code></pre>"},{"location":"core-concepts/identifiers/#identifier-caching-with-cache_ids","title":"Identifier caching with <code>cache_ids</code>","text":"<p>By default (<code>cache_ids=True</code>), Traffik calls your identifier function once per request and caches the result in the request's context for the lifetime of that request. If multiple throttles apply to the same endpoint, they all reuse the cached value and your identifier function is never called more than once.</p> <pre><code>from traffik import HTTPThrottle\n\nthrottle = HTTPThrottle(\n    uid=\"my-api\",\n    rate=\"100/min\",\n    identifier=my_identifier,\n    cache_ids=True,   # default \u2014 identifier called once, result reused\n)\n</code></pre> <p>Set <code>cache_ids=False</code> only if your identifier function's result can legitimately change between throttle invocations within the same request (which is almost never the case).</p> <p>Avoid slow I/O in identifier functions on hot paths</p> <p>Your identifier runs on every request. An identifier that makes a database query or an external HTTP call to resolve a user ID will add that latency to every single request that hits any throttle. Consider caching resolved identities in <code>request.state</code> yourself, or keep the identifier to headers and path parameters only.</p> <p>If you must do I/O, at least make sure <code>cache_ids=True</code> (the default) so the I/O only happens once per request, not once per throttle per request.</p>"},{"location":"core-concepts/rates/","title":"Rates","text":"<p>A <code>Rate</code> answers one question: how many requests are allowed in how much time?</p> <p>Traffik gives you two ways to express that: a human-friendly string, or the <code>Rate</code> object itself. Both end up in exactly the same place.</p>"},{"location":"core-concepts/rates/#string-format","title":"String format","text":"<p>The string format is the fastest path to a working rate limit. Traffik parses it once, caches the result (LRU, capacity 512), and never touches it again.</p> <pre><code>rate = \"100/min\"     # 100 requests per minute\nrate = \"5/s\"         # 5 per second\nrate = \"10/10s\"      # 10 per 10 seconds\nrate = \"2 per second\"\nrate = \"500/hour\"\nrate = \"1000/500ms\"  # sub-second windows work too\n</code></pre> <p>The general grammar is:</p> <pre><code>&lt;limit&gt;/&lt;period&gt;&lt;unit&gt;\n&lt;limit&gt; per &lt;period&gt;&lt;unit&gt;\n&lt;limit&gt;/&lt;period&gt; &lt;unit&gt;\n</code></pre> <p>Where <code>&lt;period&gt;</code> is an optional integer multiplier (defaults to <code>1</code>) and <code>&lt;unit&gt;</code> is one of the values in the table below.</p>"},{"location":"core-concepts/rates/#supported-units","title":"Supported units","text":"Unit string(s) Meaning <code>ms</code>, <code>millisecond</code>, <code>milliseconds</code> Milliseconds <code>s</code>, <code>sec</code>, <code>second</code>, <code>seconds</code> Seconds <code>m</code>, <code>min</code>, <code>minute</code>, <code>minutes</code> Minutes <code>h</code>, <code>hr</code>, <code>hour</code>, <code>hours</code> Hours <code>d</code>, <code>day</code>, <code>days</code> Days"},{"location":"core-concepts/rates/#rate-string-reference","title":"Rate string reference","text":"String Meaning <code>\"5/m\"</code> 5 requests per minute <code>\"5/min\"</code> 5 requests per minute <code>\"5 per minute\"</code> 5 requests per minute <code>\"100/h\"</code> 100 requests per hour <code>\"100/hour\"</code> 100 requests per hour <code>\"10 per second\"</code> 10 requests per second <code>\"2/5s\"</code> 2 requests per 5 seconds <code>\"10/30 seconds\"</code> 10 requests per 30 seconds <code>\"1000/500ms\"</code> 1000 requests per 500 ms <code>\"50/d\"</code> 50 requests per day <code>\"0/0\"</code> Unlimited (no throttling) <p>Stick to strings for simple limits</p> <p>For everyday limits like \"100 per minute\" or \"5 per second\", the string form is cleaner and immediately readable at a glance. Reserve the <code>Rate</code> object for periods that don't map to a single unit.</p>"},{"location":"core-concepts/rates/#the-rate-object","title":"The <code>Rate</code> object","text":"<p>When you need a period that spans multiple units, for example 5 minutes and 30 seconds, or when you want to construct a rate programmatically, use <code>Rate</code> directly.</p> <pre><code>from traffik.rates import Rate\n\n# Simple: 100 requests per minute\nrate = Rate(limit=100, minutes=1)\n\n# Combined units: 100 requests per 5 minutes and 30 seconds\nrate = Rate(limit=100, minutes=5, seconds=30)\n\n# Sub-second: 50 requests per 500 milliseconds\nrate = Rate(limit=50, milliseconds=500)\n</code></pre> <p><code>Rate</code> accepts the following keyword arguments (all default to <code>0</code>):</p> Parameter Unit <code>limit</code> Request count <code>milliseconds</code> ms <code>seconds</code> s <code>minutes</code> min <code>hours</code> h <p>All time parameters are additive, the expire period is their sum in milliseconds.</p> <p><code>Rate</code> objects are immutable and final (subclassing is forbidden). Once created, every attribute, <code>limit</code>, <code>expire</code>, <code>rps</code>, <code>rpm</code>, <code>rph</code>, <code>rpd</code>, <code>is_subsecond</code>, <code>unlimited</code>, is frozen.</p> <p>Both limit and period must be set together</p> <p>Passing <code>limit=100</code> without any time unit raises <code>ValueError</code>. Passing <code>milliseconds=500</code> without <code>limit</code> also raises <code>ValueError</code>. Both must be non-zero, or both must be zero (unlimited).</p>"},{"location":"core-concepts/rates/#parsing-with-rateparse","title":"Parsing with <code>Rate.parse()</code>","text":"<p>Internally, string rates go through <code>Rate.parse()</code>, which wraps an LRU-cached internal function with a capacity of 512 entries. This means the thousandth time you hit <code>\"100/min\"</code>, the parse cost is effectively zero.</p> <pre><code>rate = Rate.parse(\"100/min\")\nrate = Rate.parse(\"2/5s\")\nrate = Rate.parse(\"10 per second\")\n</code></pre> <p>Parsing is case-insensitive. <code>\"100/MIN\"</code> and <code>\"100/min\"</code> hit the same cache entry.</p>"},{"location":"core-concepts/rates/#dynamic-rate-functions","title":"Dynamic rate functions","text":"<p>Sometimes the right rate limit depends on the request itself. Maybe free users get <code>\"10/min\"</code> and pro users get <code>\"1000/min\"</code>. Traffik supports async rate functions that receive the current connection and context, and return a <code>Rate</code>.</p> <pre><code>from traffik.rates import Rate\nfrom starlette.requests import Request\n\nasync def my_rate(connection: Request, context: dict | None) -&gt; Rate:\n    user = connection.state.user\n    if user and user.plan == \"pro\":\n        return Rate.parse(\"1000/min\")\n    return Rate.parse(\"10/min\")\n</code></pre> <p>Pass the function anywhere a rate is accepted:</p> <pre><code>from traffik import HTTPThrottle\n\nthrottle = HTTPThrottle(\n    uid=\"dynamic-rate\",\n    rate=my_rate,  # async function, not a string or Rate\n)\n</code></pre> <p>Dynamic rates are called on every request</p> <p>Unlike static strings or <code>Rate</code> objects, dynamic functions are called on every request. Keep them fast, a database lookup per request is a latency hit on every single call.</p>"},{"location":"core-concepts/rates/#unlimited-rates","title":"Unlimited rates","text":"<p>To explicitly mark something as unlimited (no throttling at all), use either:</p> <pre><code>Rate()                # no arguments \u2192 unlimited\nRate.parse(\"0/0\")    # string form\n</code></pre> <p>Both produce <code>Rate(unlimited=True)</code> with <code>rate.unlimited == True</code>. Traffik's strategies detect this flag and short-circuit immediately, zero backend I/O, zero overhead.</p> <p>Use unlimited for internal health check endpoints</p> <p>A health check that hits the same throttle as your main API can skew your counters. Give it an unlimited rate (or return <code>EXEMPTED</code> from the identifier) and keep your metrics clean.</p>"},{"location":"core-concepts/strategies/","title":"Strategies","text":"<p>A strategy is the algorithm that decides how Traffik counts. Every strategy receives the same inputs, a key, a rate, and a backend, and returns a wait time in milliseconds. Zero means \"go ahead\". Anything positive means \"slow down\".</p> <p>Choosing a strategy is a trade-off between accuracy, memory, and burst tolerance. The table below gives you the overview; the sections that follow go deeper.</p>"},{"location":"core-concepts/strategies/#strategy-comparison","title":"Strategy comparison","text":"Strategy Accuracy Memory Bursts Best For <code>FixedWindow</code> Low O(1) Yes (boundary) Simple limits, high throughput APIs <code>SlidingWindowCounter</code> Medium O(1) Minimal General purpose, good default <code>SlidingWindowLog</code> Highest O(limit) No Financial, security-critical, strict SLA <code>TokenBucket</code> High O(1) Yes (configurable) Variable traffic, mobile clients <code>TokenBucketWithDebt</code> High O(1) Yes + overdraft Gradual degradation, user-facing APIs <code>LeakyBucket</code> High O(1) No Protecting downstream services <code>LeakyBucketWithQueue</code> High O(limit) No (strict FIFO) Ordered processing, fairness guarantees <code>GCRA</code> Highest O(1) Configurable Telecom, smooth pipelines, strict SLA"},{"location":"core-concepts/strategies/#fixedwindow-default","title":"FixedWindow (default)","text":"<p>The simplest possible algorithm. Time is divided into fixed windows aligned to clock boundaries (e.g., 00:00\u201301:00, 01:00\u201302:00). Each request increments a counter for the current window. When the window ends, the counter resets automatically via TTL.</p> <p>The boundary problem: A user can make up to <code>limit</code> requests at 00:59 and another <code>limit</code> at 01:00, up to 2x the limit within any two-second span. If that is acceptable, <code>FixedWindow</code> is your best friend: it is fast, cheap, and requires only one atomic counter per key.</p> <pre><code>from traffik import HTTPThrottle\nfrom traffik.strategies import FixedWindow\n\nthrottle = HTTPThrottle(\n    uid=\"my-api\",\n    rate=\"100/min\",\n    strategy=FixedWindow(),  # this is also the default if you omit strategy\n)\n</code></pre> <p>Storage keys:</p> <ul> <li><code>{namespace}:{key}:fixedwindow:counter</code> \u2014 request counter (integer, TTL = window duration)</li> <li><code>{namespace}:{key}:fixedwindow:start</code> \u2014 window start timestamp (sub-second windows only)</li> </ul> <p>FixedWindow is the default</p> <p>You do not need to pass <code>strategy=FixedWindow()</code> explicitly, it is what you get when you omit the <code>strategy</code> argument entirely.</p>"},{"location":"core-concepts/strategies/#slidingwindowcounter","title":"SlidingWindowCounter","text":"<p>A smarter cousin of <code>FixedWindow</code>. Instead of snapping to fixed clock boundaries, it tracks two consecutive windows and computes a weighted count:</p> <pre><code>weighted_count = (previous_count * overlap_percentage) + current_count\n</code></pre> <p>As you move further through the current window, the previous window's contribution shrinks toward zero. The result is much smoother traffic flow with only two counters per key.</p> <pre><code>from traffik.strategies import SlidingWindowCounter\n\nthrottle = HTTPThrottle(\n    uid=\"my-api\",\n    rate=\"100/min\",\n    strategy=SlidingWindowCounter(),\n)\n</code></pre> <p>Storage keys:</p> <ul> <li><code>{namespace}:{key}:slidingcounter:{window_id}</code> \u2014 counter per window (TTL = 2x window duration)</li> <li>Two consecutive windows are read on every request.</li> </ul> <p>When to use: General-purpose rate limiting where you want better accuracy than <code>FixedWindow</code> without paying the memory cost of <code>SlidingWindowLog</code>.</p>"},{"location":"core-concepts/strategies/#slidingwindowlog","title":"SlidingWindowLog","text":"<p>The most accurate strategy. Traffik maintains a log of <code>[timestamp, cost]</code> pairs for every request in the window, serialised with MessagePack. On each request it evicts expired entries and sums the remaining costs.</p> <pre><code>from traffik.strategies import SlidingWindowLog\n\nthrottle = HTTPThrottle(\n    uid=\"payment-endpoint\",\n    rate=\"100/min\",\n    strategy=SlidingWindowLog(),\n)\n</code></pre> <p>Storage keys:</p> <ul> <li><code>{namespace}:{key}:slidinglog</code> \u2014 msgpack-encoded list of <code>[timestamp_ms, cost]</code> tuples</li> </ul> <p>Memory: O(limit) \u2014 at peak load the log holds one entry per allowed request. With a limit of 10,000 this can add up. Choose <code>SlidingWindowCounter</code> if memory is a concern.</p> <p>Use SlidingWindowLog only when accuracy is mandatory</p> <p>For most APIs, <code>SlidingWindowCounter</code> gives 95% of the accuracy at a fraction of the memory. Reserve <code>SlidingWindowLog</code> for payment processing, security-critical endpoints, and anywhere where the boundary burst of <code>FixedWindow</code> is genuinely unacceptable.</p>"},{"location":"core-concepts/strategies/#tokenbucket","title":"TokenBucket","text":"<p>Models rate limiting as a bucket that holds tokens. Tokens refill at a constant rate (e.g., <code>100 tokens/minute</code>). Each request consumes one token (or <code>cost</code> tokens). If the bucket is empty, the request must wait until enough tokens have accumulated.</p> <p>The <code>burst_size</code> parameter controls the maximum bucket capacity. Set it higher than <code>rate.limit</code> to allow occasional bursts; set it equal to <code>rate.limit</code> for no extra burst allowance.</p> <pre><code>from traffik.strategies import TokenBucket\n\nthrottle = HTTPThrottle(\n    uid=\"my-api\",\n    rate=\"100/min\",\n    strategy=TokenBucket(burst_size=150),  # allow bursts up to 150\n)\n</code></pre> <p>Storage keys:</p> <ul> <li><code>{namespace}:{key}:tokenbucket:{capacity}</code> \u2014 msgpack <code>{\"tokens\": float, \"last_refill\": ms}</code></li> </ul> <p>Refill formula: <code>new_tokens = min(current + elapsed_ms * (limit / expire_ms), capacity)</code></p> <p>Tokens are refilled lazily on every request, there is no background process.</p>"},{"location":"core-concepts/strategies/#tokenbucketwithdebt","title":"TokenBucketWithDebt","text":"<p>An extended token bucket that lets the bucket go negative. Requests are still allowed when the bucket is at zero, they go into \"debt\" up to <code>max_debt</code>. The debt is paid back through normal token refilling. This produces a softer experience: users never hit a sudden wall; traffic degrades gradually.</p> <pre><code>from traffik.strategies import TokenBucketWithDebt\n\nthrottle = HTTPThrottle(\n    uid=\"user-facing-api\",\n    rate=\"100/min\",\n    strategy=TokenBucketWithDebt(\n        burst_size=150,  # max positive tokens\n        max_debt=50,     # allow up to 50 tokens of debt\n    ),\n)\n</code></pre> <p>Storage keys:</p> <ul> <li><code>{namespace}:{key}:tokenbucket:{capacity}:debt:{max_debt}</code> \u2014 same state format as <code>TokenBucket</code>, but tokens can be negative</li> </ul> <p>When NOT to use: Strict SLA enforcement, third-party API proxying, billing and payment systems. Debt means you can temporarily exceed the nominal rate.</p>"},{"location":"core-concepts/strategies/#leakybucket","title":"LeakyBucket","text":"<p>The inverse of <code>TokenBucket</code>. Instead of tokens filling a bucket, requests fill it, and it drains (leaks) at a constant rate. If the bucket is full when a new request arrives, that request is rejected.</p> <p>The effect is perfectly smooth output, no bursts are ever allowed. This is ideal for protecting downstream services that cannot handle spikes.</p> <pre><code>from traffik.strategies import LeakyBucket\n\nthrottle = HTTPThrottle(\n    uid=\"downstream-proxy\",\n    rate=\"100/min\",\n    strategy=LeakyBucket(),\n)\n</code></pre> <p>Storage keys:</p> <ul> <li><code>{namespace}:{key}:leakybucket:state</code> \u2014 msgpack <code>{\"level\": float, \"last_leak\": ms}</code></li> </ul>"},{"location":"core-concepts/strategies/#leakybucketwithqueue","title":"LeakyBucketWithQueue","text":"<p><code>LeakyBucket</code> with a strict FIFO guarantee. Instead of a simple fill level, Traffik stores the full queue of <code>[timestamp, cost]</code> entries. Requests drain in the exact order they arrived. No request can \"cut in line\".</p> <pre><code>from traffik.strategies import LeakyBucketWithQueue\n\nthrottle = HTTPThrottle(\n    uid=\"ordered-queue\",\n    rate=\"100/min\",\n    strategy=LeakyBucketWithQueue(),\n)\n</code></pre> <p>Storage keys:</p> <ul> <li><code>{namespace}:{key}:leakybucketqueue:state</code> \u2014 msgpack <code>{\"queue\": [[ts, cost], ...], \"last_leak\": ms}</code></li> </ul> <p>Memory: O(limit) for the same reason as <code>SlidingWindowLog</code>.</p>"},{"location":"core-concepts/strategies/#gcra","title":"GCRA","text":"<p>The Generic Cell Rate Algorithm, also called \"leaky bucket as a meter\" or \"virtual scheduling\". Originally designed for ATM networks, it provides the smoothest possible rate enforcement with the smallest memory footprint of any stateful algorithm: a single float (the Theoretical Arrival Time, TAT).</p> <p>How it works: Every request is only allowed if <code>now &gt;= TAT - burst_tolerance_ms</code>. After an allowed request, the TAT advances by one emission interval (<code>window_duration / limit</code>). The result: requests are uniformly spaced in time.</p> <pre><code>from traffik.strategies.custom import GCRA\n\n# Perfectly smooth: 100 req/min = one request every 600ms\nthrottle = HTTPThrottle(\n    uid=\"realtime-pipeline\",\n    rate=\"100/min\",\n    strategy=GCRA(burst_tolerance_ms=0),  # 0 = no bursts at all\n)\n\n# Allow a small burst tolerance (e.g. 1 full emission interval)\nthrottle = HTTPThrottle(\n    uid=\"realtime-pipeline\",\n    rate=\"100/min\",\n    strategy=GCRA(burst_tolerance_ms=600),  # one request's worth of tolerance\n)\n</code></pre> <p>Storage keys:</p> <ul> <li><code>{namespace}:{key}:gcra:tat</code> \u2014 a single float: the TAT in milliseconds</li> </ul> <p>When to use: Telecommunications systems, real-time data pipelines, strict SLA contracts, anywhere you need requests to be evenly spread over time rather than clustered at the start of a window.</p> <p>GCRA is the most memory-efficient accurate strategy</p> <p>Just one float per key. If you need accuracy and have a large number of unique keys (millions of users), GCRA beats <code>SlidingWindowLog</code> decisively on memory.</p>"},{"location":"core-concepts/strategies/#advanced-strategies-in-traffikstrategiescustom","title":"Advanced strategies in <code>traffik.strategies.custom</code>","text":"<p>Traffik ships a second, richer set of strategies for specialised scenarios. They live in <code>traffik.strategies.custom</code> and are imported explicitly:</p> <pre><code>from traffik.strategies.custom import (\n    AdaptiveThrottle,   # dynamically reduces limits under high load\n    TieredRate,         # different multipliers per user tier (free/pro/enterprise)\n    PriorityQueue,      # CRITICAL / HIGH / NORMAL / LOW priority classes\n    QuotaWithRollover,  # monthly quotas where unused quota carries over\n    TimeOfDay,          # different limits by hour of day\n    CostBasedTokenBucket, # refill rate adjusts based on average request cost\n    DistributedFairness,  # deficit round-robin across multiple app instances\n    GeographicDistribution, # per-region capacity with optional spillover\n)\n</code></pre> <p>Each of these strategies is a drop-in replacement, same interface, same <code>__call__</code> signature. Check the API reference and the <code>traffik.strategies.custom</code> module for full configuration details and examples.</p>"},{"location":"extending/backends/","title":"Building Custom Backends","text":"<p>Traffik ships with InMemory, Redis, and Memcached backends. Those cover the vast majority of use cases. But if you have a custom storage layer \u2014 DynamoDB, Cassandra, a SQL database, an in-house cache \u2014 you can plug it in by subclassing <code>ThrottleBackend</code>.</p> <p>The contract is well-defined, the base class handles a lot of the plumbing for you, and you only need to implement the storage operations themselves.</p>"},{"location":"extending/backends/#inherit-from-throttlebackend","title":"Inherit from <code>ThrottleBackend</code>","text":"<pre><code>from traffik.backends.base import ThrottleBackend\n\nclass CustomBackend(ThrottleBackend[YourConnectionType, HTTPConnection]):\n    ...\n</code></pre> <p>The two type parameters are:</p> <ul> <li><code>YourConnectionType</code> \u2014 the type of your underlying storage connection (e.g., <code>aiohttp.ClientSession</code>, <code>asyncpg.Pool</code>, your own client class)</li> <li><code>HTTPConnection</code> \u2014 the Starlette HTTP connection type (<code>Request</code>, <code>WebSocket</code>, or <code>HTTPConnection</code> for both)</li> </ul>"},{"location":"extending/backends/#required-methods","title":"Required Methods","text":"<p>You must implement all of these. The base class will automatically wrap them to re-raise exceptions as <code>BackendError</code> (using the <code>base_exception_type</code> class variable).</p> <pre><code>class CustomBackend(ThrottleBackend):\n    base_exception_type = YourLibraryBaseException  # Exceptions to wrap as BackendError\n\n    async def initialize(self) -&gt; None:\n        \"\"\"Setup connections, create tables, etc. Called before first use.\"\"\"\n        self.connection = await create_connection(...)\n\n    async def get(self, key: str) -&gt; Optional[str]:\n        \"\"\"Get value for key. Returns None if not found.\"\"\"\n        return await self.connection.get(key)\n\n    async def set(self, key: str, value: str, expire: Optional[int] = None) -&gt; None:\n        \"\"\"Set value for key with optional TTL in seconds.\"\"\"\n        await self.connection.set(key, value, ex=expire)\n\n    async def delete(self, key: str) -&gt; None:\n        \"\"\"Remove key. Should not raise if key doesn't exist.\"\"\"\n        await self.connection.delete(key)\n\n    async def increment(self, key: str, amount: int = 1) -&gt; int:\n        \"\"\"Atomically increment counter. Returns new value.\"\"\"\n        return await self.connection.incrby(key, amount)\n\n    async def decrement(self, key: str, amount: int = 1) -&gt; int:\n        \"\"\"Atomically decrement counter. Returns new value.\"\"\"\n        return await self.connection.decrby(key, amount)\n\n    async def expire(self, key: str, seconds: int) -&gt; None:\n        \"\"\"Set TTL on an existing key.\"\"\"\n        await self.connection.expire(key, seconds)\n\n    async def get_lock(self, key: str, timeout: Optional[float] = None) -&gt; AsyncLock:\n        \"\"\"\n        Return a distributed lock object for key.\n        The returned object must implement the AsyncLock protocol:\n          - locked() -&gt; bool\n          - acquire(blocking, blocking_timeout) -&gt; bool\n          - release() -&gt; None\n        \"\"\"\n        return MyDistributedLock(key, timeout=timeout)\n\n    async def reset(self) -&gt; None:\n        \"\"\"Clear all throttling data in this namespace. Used for testing.\"\"\"\n        await self.connection.flushdb()\n\n    async def close(self) -&gt; None:\n        \"\"\"Close connections and release resources.\"\"\"\n        await self.connection.close()\n</code></pre>"},{"location":"extending/backends/#performance-critical-overrides","title":"Performance-Critical Overrides","text":"<p>These methods have default implementations in the base class, but overriding them with native operations is strongly recommended for production backends:</p>"},{"location":"extending/backends/#increment_with_ttl","title":"<code>increment_with_ttl()</code>","text":"<p>The most important override. Called on every request for <code>FixedWindow</code> and <code>SlidingWindowCounter</code> with windows &gt;= 1 second.</p> <pre><code>async def increment_with_ttl(\n    self, key: str, amount: int = 1, ttl: int = 1\n) -&gt; int:\n    \"\"\"\n    Atomically increment counter AND set TTL if key is new.\n\n    This must be atomic. In Redis: MULTI/EXEC or a Lua script.\n    The TTL should only be set when the key is newly created.\n    \"\"\"\n    # Redis example using a pipeline:\n    async with self.connection.pipeline(transaction=True) as pipe:\n        await pipe.incrby(key, amount)\n        await pipe.expire(key, ttl)  # or use SET ... EX ... for atomic set+expire\n        results = await pipe.execute()\n    return results[0]\n</code></pre>"},{"location":"extending/backends/#multi_get","title":"<code>multi_get()</code>","text":"<p>Batch read \u2014 called by <code>SlidingWindowCounter</code> and the stats system:</p> <pre><code>async def multi_get(self, *keys: str) -&gt; List[Optional[str]]:\n    \"\"\"Get multiple keys in a single round-trip.\"\"\"\n    return await self.connection.mget(*keys)\n</code></pre>"},{"location":"extending/backends/#multi_set","title":"<code>multi_set()</code>","text":"<p>Batch write \u2014 called during window resets in sub-second strategies:</p> <pre><code>async def multi_set(\n    self, items: Dict[str, str], expire: Optional[int] = None\n) -&gt; None:\n    \"\"\"Set multiple key-value pairs, optionally all with the same TTL.\"\"\"\n    async with self.connection.pipeline() as pipe:\n        for key, value in items.items():\n            if expire:\n                await pipe.setex(key, expire, value)\n            else:\n                await pipe.set(key, value)\n        await pipe.execute()\n</code></pre>"},{"location":"extending/backends/#full-example-simple-in-process-dictionary-backend","title":"Full Example: Simple In-Process Dictionary Backend","text":"<p>Here's a complete, minimal custom backend using a plain Python dict (for illustration purposes \u2014 don't use this in production):</p> <pre><code>import asyncio\nimport time\nimport typing\nfrom contextlib import asynccontextmanager\nfrom starlette.requests import HTTPConnection\nfrom traffik.backends.base import ThrottleBackend\nfrom traffik.types import AsyncLock\n\n\nclass _SimpleLock:\n    \"\"\"A minimal AsyncLock implementation using asyncio.Lock.\"\"\"\n\n    def __init__(self) -&gt; None:\n        self._lock = asyncio.Lock()\n\n    def locked(self) -&gt; bool:\n        return self._lock.locked()\n\n    async def acquire(\n        self,\n        blocking: bool = True,\n        blocking_timeout: typing.Optional[float] = None,\n    ) -&gt; bool:\n        if not blocking:\n            return self._lock.acquire.__func__  # non-blocking attempt\n        if blocking_timeout is not None:\n            try:\n                await asyncio.wait_for(self._lock.acquire(), timeout=blocking_timeout)\n                return True\n            except asyncio.TimeoutError:\n                return False\n        await self._lock.acquire()\n        return True\n\n    async def release(self) -&gt; None:\n        self._lock.release()\n\n\nclass DictBackend(ThrottleBackend[None, HTTPConnection]):\n    \"\"\"Simple dictionary-based backend for demonstration.\"\"\"\n\n    base_exception_type = Exception\n\n    def __init__(self, namespace: str = \"dict\") -&gt; None:\n        super().__init__(connection=None, namespace=namespace)\n        self._store: typing.Dict[str, typing.Tuple[str, typing.Optional[float]]] = {}\n        # value, expires_at\n        self._locks: typing.Dict[str, _SimpleLock] = {}\n\n    def _is_expired(self, key: str) -&gt; bool:\n        if key not in self._store:\n            return True\n        _, expires_at = self._store[key]\n        if expires_at is not None and time.time() &gt; expires_at:\n            del self._store[key]\n            return True\n        return False\n\n    async def initialize(self) -&gt; None:\n        pass  # Nothing to initialize for a dict backend\n\n    async def get(self, key: str) -&gt; typing.Optional[str]:\n        if self._is_expired(key):\n            return None\n        return self._store[key][0]\n\n    async def set(\n        self, key: str, value: str, expire: typing.Optional[int] = None\n    ) -&gt; None:\n        expires_at = time.time() + expire if expire else None\n        self._store[key] = (str(value), expires_at)\n\n    async def delete(self, key: str) -&gt; None:\n        self._store.pop(key, None)\n\n    async def increment(self, key: str, amount: int = 1) -&gt; int:\n        current = await self.get(key)\n        new_val = (int(current) if current else 0) + amount\n        expires_at = self._store.get(key, (None, None))[1]\n        self._store[key] = (str(new_val), expires_at)\n        return new_val\n\n    async def decrement(self, key: str, amount: int = 1) -&gt; int:\n        return await self.increment(key, -amount)\n\n    async def expire(self, key: str, seconds: int) -&gt; None:\n        if key in self._store:\n            value, _ = self._store[key]\n            self._store[key] = (value, time.time() + seconds)\n\n    async def increment_with_ttl(\n        self, key: str, amount: int = 1, ttl: int = 1\n    ) -&gt; int:\n        current = await self.get(key)\n        new_val = (int(current) if current else 0) + amount\n        expires_at = time.time() + ttl\n        # Only update expiry if key is new (current is None)\n        if current is None:\n            self._store[key] = (str(new_val), expires_at)\n        else:\n            existing_expiry = self._store[key][1]\n            self._store[key] = (str(new_val), existing_expiry)\n        return new_val\n\n    async def multi_get(self, *keys: str) -&gt; typing.List[typing.Optional[str]]:\n        return [await self.get(k) for k in keys]\n\n    async def multi_set(\n        self,\n        items: typing.Dict[str, str],\n        expire: typing.Optional[int] = None,\n    ) -&gt; None:\n        for key, value in items.items():\n            await self.set(key, value, expire=expire)\n\n    async def get_lock(\n        self, key: str, timeout: typing.Optional[float] = None\n    ) -&gt; _SimpleLock:\n        if key not in self._locks:\n            self._locks[key] = _SimpleLock()\n        return self._locks[key]\n\n    async def reset(self) -&gt; None:\n        self._store.clear()\n\n    async def close(self) -&gt; None:\n        self._store.clear()\n        self._locks.clear()\n</code></pre>"},{"location":"extending/backends/#tips-on-connection-pooling","title":"Tips on Connection Pooling","text":"<p>For backends backed by network connections (Redis, Cassandra, HTTP APIs), always use connection pooling:</p> <pre><code>class PooledBackend(ThrottleBackend):\n    def __init__(self, dsn: str, pool_size: int = 10, **kwargs):\n        super().__init__(connection=None, **kwargs)\n        self._dsn = dsn\n        self._pool_size = pool_size\n\n    async def initialize(self) -&gt; None:\n        self.connection = await create_pool(self._dsn, max_size=self._pool_size)\n\n    async def close(self) -&gt; None:\n        if self.connection:\n            await self.connection.close()\n</code></pre> <p>Use the lifespan pattern</p> <p>Implement a <code>lifespan</code> context manager (or use <code>backend.lifespan</code>) to ensure <code>initialize()</code> and <code>close()</code> are called at the right time in your ASGI app lifecycle.</p>"},{"location":"extending/backends/#tips-on-lock-implementation","title":"Tips on Lock Implementation","text":"<p>Distributed locks are the hardest part of a custom backend. A few things to keep in mind:</p> <ul> <li>TTL is critical: Locks must expire automatically. If a worker dies while holding a lock, a TTL prevents permanent deadlock.</li> <li>Non-blocking mode: Implement <code>acquire(blocking=False)</code> properly \u2014 callers use this to avoid waiting for a lock.</li> <li>Blocking timeout: <code>acquire(blocking_timeout=5.0)</code> should give up after 5 seconds rather than waiting forever.</li> <li>Context manager support: The base class wraps <code>get_lock()</code> in an <code>asynccontextmanager</code> \u2014 your lock just needs the <code>acquire/release</code> protocol.</li> </ul>"},{"location":"extending/backends/#all-operations-must-be-non-blocking","title":"All Operations Must Be Non-Blocking","text":"<p>Every method in your backend must be a coroutine that does not block the event loop. This means:</p> <ul> <li>Use <code>await</code> for all I/O</li> <li>Never call synchronous blocking APIs (<code>requests</code>, <code>time.sleep</code>, synchronous DB drivers)</li> <li>Use thread executor only as a last resort: <code>asyncio.get_running_loop().run_in_executor(...)</code></li> </ul> <p>Blocking the event loop under load will stall all requests, not just the throttled ones.</p>"},{"location":"extending/strategies/","title":"Building Custom Strategies","text":"<p>The built-in strategies cover the vast majority of rate limiting needs. But Traffik is built for extension, and writing your own strategy is surprisingly straightforward.</p> <p>Maybe you need a strategy that accounts for user tier. Maybe you want a \"quota with rollover\" that carries unused tokens to the next window. Maybe you've discovered a new algorithm in a paper and want to try it. Whatever the reason, here's how.</p>"},{"location":"extending/strategies/#the-strategy-protocol","title":"The Strategy Protocol","text":"<p>A strategy is any async callable that matches this signature:</p> <pre><code>async def __call__(\n    self,\n    key: Stringable,        # The namespaced throttle key for this client\n    rate: Rate,             # The rate limit definition\n    backend: ThrottleBackend,  # The backend to read/write from\n    cost: int = 1,          # How many units this request costs\n) -&gt; WaitPeriod:            # Milliseconds to wait (0 = allow, &gt;0 = throttle)\n</code></pre> <p>That's the entire contract. Return <code>0.0</code> to allow the request. Return a positive number of milliseconds to throttle the client.</p> <p>Add get_stat() for observability</p> <p>If you also implement <code>get_stat(key, rate, backend) -&gt; StrategyStat</code>, your strategy will support <code>throttle.stat()</code> calls and all the observability features that come with it. It's optional but highly recommended.</p>"},{"location":"extending/strategies/#full-example-sliding-quota-with-priority","title":"Full Example: Sliding Quota with Priority","text":"<p>Here's a complete custom strategy that gives \"priority\" clients a higher limit than regular clients by reading a flag from the context.</p> <p>Actually, a simpler and more useful example: a ResetOnFirstHit strategy that tracks when a client first used the API in a window, and gives them the full window from that point (rather than aligning to clock boundaries):</p> <pre><code>from dataclasses import dataclass, field\nfrom typing import Optional\n\nfrom traffik.backends.base import ThrottleBackend\nfrom traffik.rates import Rate\nfrom traffik.types import LockConfig, StrategyStat, Stringable, WaitPeriod\nfrom traffik.utils import time\n\n\n@dataclass(frozen=True)\nclass RollingWindowStrategy:\n    \"\"\"\n    A rolling window strategy that starts the window on the client's first request.\n\n    Unlike FixedWindow (which aligns to clock boundaries), this gives each client\n    a full 'rate.expire' milliseconds from their first request.\n    \"\"\"\n\n    lock_config: LockConfig = field(default_factory=dict)\n\n    async def __call__(\n        self,\n        key: Stringable,\n        rate: Rate,\n        backend: ThrottleBackend,\n        cost: int = 1,\n    ) -&gt; WaitPeriod:\n        # Always check for unlimited rate first \u2014 zero overhead fast path\n        if rate.unlimited:\n            return 0.0\n\n        now_ms = time() * 1000\n        full_key = backend.get_key(str(key))\n        start_key = f\"{full_key}:rolling:start\"\n        count_key = f\"{full_key}:rolling:count\"\n        ttl_seconds = max(int(rate.expire // 1000), 1)\n\n        # Multi-step: need a lock to prevent races\n        async with await backend.lock(f\"lock:{full_key}:rolling\", **self.lock_config):\n            window_start = await backend.get(start_key)\n\n            if window_start is None:\n                # First request from this client \u2014 start their window now\n                await backend.multi_set(\n                    {\n                        start_key: str(now_ms),\n                        count_key: str(cost),\n                    },\n                    expire=ttl_seconds,\n                )\n                return 0.0\n\n            window_start_ms = float(window_start)\n            window_end_ms = window_start_ms + rate.expire\n\n            if now_ms &gt;= window_end_ms:\n                # Window expired \u2014 start a fresh one\n                await backend.multi_set(\n                    {\n                        start_key: str(now_ms),\n                        count_key: str(cost),\n                    },\n                    expire=ttl_seconds,\n                )\n                return 0.0\n\n            # Inside the window \u2014 increment and check\n            counter = await backend.increment_with_ttl(count_key, amount=cost, ttl=ttl_seconds)\n\n        if counter &gt; rate.limit:\n            # Over the limit \u2014 tell client when their window resets\n            wait_ms = window_end_ms - now_ms\n            return max(wait_ms, 0.0)\n\n        return 0.0\n\n    async def get_stat(\n        self,\n        key: Stringable,\n        rate: Rate,\n        backend: ThrottleBackend,\n    ) -&gt; StrategyStat:\n        now_ms = time() * 1000\n        full_key = backend.get_key(str(key))\n        start_key = f\"{full_key}:rolling:start\"\n        count_key = f\"{full_key}:rolling:count\"\n\n        window_start_raw, count_raw = await backend.multi_get(start_key, count_key)\n\n        if window_start_raw is None:\n            return StrategyStat(\n                key=key,\n                rate=rate,\n                hits_remaining=float(rate.limit),\n                wait_ms=0.0,\n            )\n\n        window_start_ms = float(window_start_raw)\n        window_end_ms = window_start_ms + rate.expire\n        counter = int(count_raw) if count_raw else 0\n\n        if now_ms &gt;= window_end_ms:\n            # Window expired \u2014 fresh slate\n            return StrategyStat(\n                key=key,\n                rate=rate,\n                hits_remaining=float(rate.limit),\n                wait_ms=0.0,\n            )\n\n        hits_remaining = max(rate.limit - counter, 0)\n        wait_ms = max(window_end_ms - now_ms, 0.0) if counter &gt; rate.limit else 0.0\n\n        return StrategyStat(\n            key=key,\n            rate=rate,\n            hits_remaining=hits_remaining,\n            wait_ms=wait_ms,\n        )\n</code></pre> <p>Use it like any built-in strategy:</p> <pre><code>from traffik import HTTPThrottle\n\nthrottle = HTTPThrottle(\n    \"api:rolling\",\n    rate=\"100/min\",\n    strategy=RollingWindowStrategy(),\n)\n</code></pre>"},{"location":"extending/strategies/#best-practices","title":"Best Practices","text":""},{"location":"extending/strategies/#1-handle-rateunlimited-first","title":"1. Handle <code>rate.unlimited</code> First","text":"<p>This is the zero-overhead fast path. Never skip it:</p> <pre><code>async def __call__(self, key, rate, backend, cost=1):\n    if rate.unlimited:\n        return 0.0\n    # ... your logic\n</code></pre>"},{"location":"extending/strategies/#2-use-dataclassfrozentrue-for-configuration","title":"2. Use <code>@dataclass(frozen=True)</code> for Configuration","text":"<p>Frozen dataclasses prevent accidental mutation and make your strategy safe to share across threads and requests:</p> <pre><code>from dataclasses import dataclass, field\nfrom traffik.types import LockConfig\n\n@dataclass(frozen=True)\nclass MyStrategy:\n    burst_multiplier: float = 1.5\n    lock_config: LockConfig = field(default_factory=dict)\n</code></pre>"},{"location":"extending/strategies/#3-use-backendlock-for-multi-step-operations","title":"3. Use <code>backend.lock()</code> for Multi-Step Operations","text":"<p>Any strategy that reads and then writes needs a lock to prevent race conditions under concurrency. The lock key should be derived from the throttle key:</p> <pre><code>async with await backend.lock(f\"lock:{full_key}:mystrategy\", **self.lock_config):\n    old_value = await backend.get(some_key)\n    new_value = compute(old_value)\n    await backend.set(some_key, new_value, expire=ttl)\n</code></pre>"},{"location":"extending/strategies/#4-use-increment_with_ttl-when-possible","title":"4. Use <code>increment_with_ttl()</code> When Possible","text":"<p>This is an atomic increment-and-set-TTL operation \u2014 much more efficient than <code>get()</code> + <code>increment()</code> + <code>expire()</code> with a lock:</p> <pre><code># Good: single atomic operation\ncounter = await backend.increment_with_ttl(counter_key, amount=cost, ttl=ttl_seconds)\n\n# Less efficient: three operations under a lock\nasync with await backend.lock(...):\n    counter = await backend.increment(counter_key, cost)\n    await backend.expire(counter_key, ttl_seconds)\n</code></pre>"},{"location":"extending/strategies/#5-always-set-ttls","title":"5. Always Set TTLs","text":"<p>Backend keys that never expire are a memory leak. Set TTLs on everything:</p> <pre><code>ttl_seconds = max(int(rate.expire // 1000), 1)  # At least 1 second\nawait backend.set(key, value, expire=ttl_seconds)\n</code></pre>"},{"location":"extending/strategies/#6-return-milliseconds-not-seconds","title":"6. Return Milliseconds, Not Seconds","text":"<p><code>WaitPeriod</code> is in milliseconds. The rate limit window (<code>rate.expire</code>) is also in milliseconds. Don't mix units:</p> <pre><code># Correct: milliseconds\nwait_ms = window_end_ms - now_ms\nreturn max(wait_ms, 0.0)\n\n# Wrong: accidentally returning seconds\nreturn wait_ms / 1000  # This would be almost always 0\n</code></pre>"},{"location":"extending/strategies/#7-return-00-not-none-to-allow","title":"7. Return 0.0 (not None) to Allow","text":"<p>Returning <code>0</code> or <code>0.0</code> means \"allow\". Returning <code>None</code> is not valid \u2014 always return a float.</p>"},{"location":"extending/strategies/#summary-checklist","title":"Summary Checklist","text":"<p>Before shipping your custom strategy:</p> <ul> <li> Handle <code>rate.unlimited</code> at the top with <code>return 0.0</code></li> <li> Use <code>@dataclass(frozen=True)</code> for the class</li> <li> Include <code>lock_config: LockConfig</code> for configurable locking</li> <li> Use <code>backend.lock()</code> for any multi-step read/write sequence</li> <li> Prefer <code>backend.increment_with_ttl()</code> over separate increment + expire</li> <li> Set TTLs on all backend keys</li> <li> Return milliseconds (not seconds)</li> <li> Implement <code>get_stat()</code> for observability</li> <li> Test under concurrency to verify correctness</li> </ul>"},{"location":"integration/","title":"Integration","text":"<p>Traffik doesn't force you into one pattern, use whichever fits your architecture.</p> <p>Every throttle is a plain Python object. You can attach it to a route via FastAPI's dependency injection, wrap a function with a decorator, drop it into ASGI middleware, or call it directly inside your handler logic. All four approaches use the same underlying throttle instance, they're just different entry points.</p>"},{"location":"integration/#the-four-integration-methods","title":"The four integration methods","text":""},{"location":"integration/#dependency-injection","title":"Dependency Injection","text":"<p>Use <code>Depends(throttle)</code> in your route signature or router-level dependencies. FastAPI resolves the throttle automatically on every request. No <code>Request</code> parameter required in most cases.</p> <p>This is the most idiomatic approach for FastAPI and works well for per-route and per-router throttling.</p> <pre><code>@app.get(\"/items\", dependencies=[Depends(throttle)])\nasync def list_items():\n    ...\n</code></pre> <p>Read more about dependency injection</p>"},{"location":"integration/#decorators","title":"Decorators","text":"<p>Apply <code>@throttled(...)</code> directly to a route function. Two variants exist: one for Starlette (needs a <code>Request</code> parameter in the function), and one for FastAPI (no <code>Request</code> needed, FastAPI's DI handles it).</p> <p>Decorators keep the throttle configuration visible at the handler level without modifying the route signature.</p> <pre><code>@app.get(\"/items\")\n@throttled(throttle)\nasync def list_items():\n    ...\n</code></pre> <p>Read more about the decorator approach</p>"},{"location":"integration/#middleware","title":"Middleware","text":"<p>Mount <code>ThrottleMiddleware</code> once and apply throttles globally or to specific paths, methods, or request predicates. Ideal for cross-cutting concerns you want enforced before any route handler runs.</p> <pre><code>app.add_middleware(\n    ThrottleMiddleware,\n    middleware_throttles=[MiddlewareThrottle(throttle, path=\"/api/\")],\n)\n</code></pre> <p>Read more about middleware</p>"},{"location":"integration/#direct-usage","title":"Direct Usage","text":"<p>Call the throttle methods yourself inside a handler. Use <code>await throttle.hit(request)</code> to consume quota, <code>await throttle.check(request)</code> for a non-consuming pre-check, and <code>await throttle.stat(request)</code> to read the current state.</p> <p>This gives you full programmatic control for dynamic cost calculation, multi-step workflows, or conditional throttling logic.</p> <pre><code>@app.post(\"/upload\")\nasync def upload(request: Request):\n    cost = calculate_upload_cost(request)\n    await throttle.hit(request, cost=cost)\n    ...\n</code></pre> <p>Read more about direct usage</p>"},{"location":"integration/#choosing-an-approach","title":"Choosing an approach","text":"Approach Best for Dependency injection Per-route and per-router throttling in FastAPI Decorators Keeping throttle config co-located with the handler Middleware Global or path-based throttling across all routes Direct usage Dynamic costs, conditional logic, multi-step workflows <p>There is no wrong choice. Dependency injection and decorators are interchangeable stylistic preferences. Middleware and direct usage solve different structural problems. You can also combine them, for example, a global middleware throttle plus a tighter per-route dependency.</p>"},{"location":"integration/decorators/","title":"Decorators","text":"<p>The <code>@throttled</code> decorator applies throttling directly to a route function. It keeps the rate limit configuration visible right above the handler, which some developers find easier to scan than a <code>dependencies=[...]</code> list on the route decorator.</p> <p>Traffik ships two versions of <code>@throttled</code>: one for Starlette and one for FastAPI. They behave differently in how the throttle receives the connection object.</p>"},{"location":"integration/decorators/#two-decorator-variants","title":"Two decorator variants","text":"<code>traffik.throttles.throttled</code> <code>traffik.decorators.throttled</code> Works with Starlette and FastAPI FastAPI only Requires <code>Request</code> param Yes \u2014 the route function must declare a <code>Request</code> or <code>WebSocket</code> parameter No \u2014 FastAPI's DI injects the connection automatically Mechanism Inspects function args for an <code>HTTPConnection</code> at call time Wraps the route with a hidden <code>Depends(throttle)</code> OpenAPI impact None None Recommended for Starlette apps; Starlette-compatible code FastAPI apps"},{"location":"integration/decorators/#traffikdecoratorsthrottled-fastapi-version","title":"<code>traffik.decorators.throttled</code>: FastAPI version","text":"<p>Import from <code>traffik.decorators</code>. Your route function does not need to declare a <code>Request</code> parameter. FastAPI resolves the connection internally.</p> <pre><code>from fastapi import FastAPI\nfrom traffik import HTTPThrottle\nfrom traffik.backends.inmemory import InMemoryBackend\nfrom traffik.decorators import throttled  # FastAPI-specific\n\nbackend = InMemoryBackend()\napp = FastAPI(lifespan=backend.lifespan)\n\nthrottle = HTTPThrottle(uid=\"items:read\", rate=\"100/min\")\n\n@app.get(\"/items\")\n@throttled(throttle)\nasync def list_items():\n    return {\"items\": []}\n</code></pre> <p>The decorator order matters: <code>@app.get(...)</code> must be the outermost decorator, <code>@throttled(...)</code> the next one in.</p> <p>Tip</p> <p>Because <code>traffik.decorators.throttled</code> relies on FastAPI's dependency injection, the throttle check happens before any other dependencies in the function signature are resolved. This means a rejected request never triggers database queries or other expensive dependencies.</p>"},{"location":"integration/decorators/#traffikthrottlesthrottled-starlette-version","title":"<code>traffik.throttles.throttled</code>: Starlette version","text":"<p>Import from <code>traffik.throttles</code> (or directly from <code>traffik</code>). The route function must declare a <code>Request</code> or <code>WebSocket</code> parameter. The decorator inspects the function arguments at call time to find the connection object.</p> <pre><code>from starlette.applications import Starlette\nfrom starlette.requests import Request\nfrom starlette.responses import JSONResponse\nfrom starlette.routing import Route\nfrom traffik import HTTPThrottle\nfrom traffik.backends.inmemory import InMemoryBackend\nfrom traffik.throttles import throttled  # Starlette version\n\nbackend = InMemoryBackend()\n\nthrottle = HTTPThrottle(uid=\"items:read\", rate=\"100/min\")\n\n@throttled(throttle)\nasync def list_items(request: Request) -&gt; JSONResponse:\n    return JSONResponse({\"items\": []})\n\napp = Starlette(\n    routes=[Route(\"/items\", list_items, methods=[\"GET\"])],\n    lifespan=backend.lifespan,\n)\n</code></pre> <p>The Starlette decorator requires an <code>HTTPConnection</code> parameter</p> <p>If the decorated function has no <code>Request</code> or <code>WebSocket</code> parameter (positional or keyword), Traffik raises a <code>ValueError</code> at call time. For FastAPI routes without a <code>Request</code> parameter, use <code>traffik.decorators.throttled</code> instead.</p> <p>The Starlette version also works in FastAPI if you prefer it, just make sure your handler declares the <code>Request</code>.</p> <pre><code>from fastapi import FastAPI\nfrom starlette.requests import Request\nfrom traffik import HTTPThrottle\nfrom traffik.backends.inmemory import InMemoryBackend\nfrom traffik.throttles import throttled  # Starlette version, used in FastAPI\n\nbackend = InMemoryBackend()\napp = FastAPI(lifespan=backend.lifespan)\n\nthrottle = HTTPThrottle(uid=\"items:read-sl\", rate=\"100/min\")\n\n@app.get(\"/items\")\n@throttled(throttle)\nasync def list_items(request: Request):  # &lt;-- required for starlette.throttles version\n    return {\"items\": []}\n</code></pre>"},{"location":"integration/decorators/#multiple-throttles-with-throttled","title":"Multiple throttles with <code>@throttled</code>","text":"<p>Pass multiple throttles to <code>@throttled</code>. They are checked sequentially: the first throttle is checked first, and if it rejects the request, the remaining throttles are never consulted.</p> FastAPI decoratorStarlette decorator <pre><code>from fastapi import FastAPI\nfrom traffik import HTTPThrottle\nfrom traffik.backends.inmemory import InMemoryBackend\nfrom traffik.decorators import throttled\n\nbackend = InMemoryBackend()\napp = FastAPI(lifespan=backend.lifespan)\n\nburst_throttle = HTTPThrottle(uid=\"search:burst\", rate=\"10/s\")\nsustained_throttle = HTTPThrottle(uid=\"search:sustained\", rate=\"200/min\")\n\n@app.get(\"/search\")\n@throttled(burst_throttle, sustained_throttle)\nasync def search(q: str):\n    return {\"query\": q, \"results\": []}\n</code></pre> <pre><code>from starlette.applications import Starlette\nfrom starlette.requests import Request\nfrom starlette.responses import JSONResponse\nfrom starlette.routing import Route\nfrom traffik import HTTPThrottle\nfrom traffik.backends.inmemory import InMemoryBackend\nfrom traffik.throttles import throttled\n\nbackend = InMemoryBackend()\n\nburst_throttle = HTTPThrottle(uid=\"search:burst\", rate=\"10/s\")\nsustained_throttle = HTTPThrottle(uid=\"search:sustained\", rate=\"200/min\")\n\n@throttled(burst_throttle, sustained_throttle)\nasync def search(request: Request) -&gt; JSONResponse:\n    q = request.query_params.get(\"q\", \"\")\n    return JSONResponse({\"query\": q, \"results\": []})\n\napp = Starlette(\n    routes=[Route(\"/search\", search, methods=[\"GET\"])],\n    lifespan=backend.lifespan,\n)\n</code></pre> <p>Sequential checking: first failure stops the rest</p> <p>With <code>@throttled(first, second, third)</code>, if <code>first</code> rejects the request, <code>second</code> and <code>third</code> are never evaluated. This is efficient, no unnecessary quota checks, but it also means the order of throttles matters. Put the most restrictive (or cheapest to evaluate) throttle first.</p>"},{"location":"integration/decorators/#websocket-routes","title":"WebSocket routes","text":"<p><code>@throttled</code> works with WebSocket routes too. Use <code>WebSocketThrottle</code> and apply the decorator the same way. This throttles at the connection level, recording one hit when the WebSocket handshake occurs.</p> FastAPI decoratorStarlette decorator <pre><code>from fastapi import FastAPI, WebSocket\nfrom traffik.throttles import WebSocketThrottle\nfrom traffik.backends.inmemory import InMemoryBackend\nfrom traffik.decorators import throttled\n\nbackend = InMemoryBackend()\napp = FastAPI(lifespan=backend.lifespan)\n\nws_throttle = WebSocketThrottle(uid=\"ws:connect\", rate=\"30/min\")\n\n@app.websocket(\"/ws\")\n@throttled(ws_throttle)\nasync def websocket_endpoint(websocket: WebSocket):\n    await websocket.accept()\n    await websocket.send_json({\"status\": \"connected\"})\n    await websocket.close()\n</code></pre> <pre><code>from starlette.applications import Starlette\nfrom starlette.routing import WebSocketRoute\nfrom starlette.websockets import WebSocket\nfrom traffik.throttles import WebSocketThrottle, throttled\nfrom traffik.backends.inmemory import InMemoryBackend\n\nbackend = InMemoryBackend()\n\nws_throttle = WebSocketThrottle(uid=\"ws:connect\", rate=\"30/min\")\n\n@throttled(ws_throttle)\nasync def websocket_endpoint(websocket: WebSocket):\n    await websocket.accept()\n    await websocket.send_json({\"status\": \"connected\"})\n    await websocket.close()\n\napp = Starlette(\n    routes=[WebSocketRoute(\"/ws\", websocket_endpoint)],\n    lifespan=backend.lifespan,\n)\n</code></pre> <p>For per-message throttling inside an established WebSocket connection, see Direct Usage.</p>"},{"location":"integration/decorators/#decorators-vs-dependencies-when-to-prefer-each","title":"Decorators vs. dependencies: when to prefer each","text":"<p>Both approaches produce the same runtime behavior. The choice is largely a matter of style:</p> <ul> <li>Decorators keep the throttle configuration adjacent to the handler. If you scan a file top-to-bottom, the rate limit is immediately visible before you read the function body.</li> <li>Dependencies keep route configuration in the route decorator. Useful when you want all route metadata, including status codes, response models, and dependencies, in one place.</li> </ul> <p>You can mix them freely. A common pattern is to use a router-level dependency for a broad limit and a decorator for a tight per-handler limit:</p> <pre><code>from fastapi import FastAPI, APIRouter, Depends\nfrom traffik import HTTPThrottle\nfrom traffik.backends.inmemory import InMemoryBackend\nfrom traffik.decorators import throttled\n\nbackend = InMemoryBackend()\napp = FastAPI(lifespan=backend.lifespan)\n\nglobal_throttle = HTTPThrottle(uid=\"api:global\", rate=\"1000/min\")\nexport_throttle = HTTPThrottle(uid=\"api:export\", rate=\"5/min\")\n\nrouter = APIRouter(\n    prefix=\"/api/v1\",\n    dependencies=[Depends(global_throttle)],\n)\n\n@router.get(\"/data\")\nasync def get_data():\n    return {\"data\": []}\n\n@router.get(\"/export\")\n@throttled(export_throttle)\nasync def export_data():\n    # hits global_throttle (via router dep) + export_throttle (via decorator)\n    return {\"export\": \"...\"}\n\napp.include_router(router)\n</code></pre>"},{"location":"integration/dependencies/","title":"Dependency Injection","text":"<p>FastAPI's <code>Depends</code> mechanism is the most idiomatic way to attach throttles to routes. Traffik throttle instances are callable and expose a clean <code>__signature__</code> that FastAPI understands natively, slotting in exactly like any other dependency.</p> <p>Recommended approach for FastAPI</p> <p>Dependency injection is the standard FastAPI pattern. It composes cleanly with other dependencies, works at the route and router level, and doesn't require you to change your handler signature.</p> <p>Works with plain Starlette too</p> <p><code>Depends(throttle)</code> is a FastAPI convenience, but the underlying throttle works with any Starlette <code>Request</code>. If you are using plain Starlette (without FastAPI), you can call the throttle directly in your route handler:</p> <pre><code>from starlette.requests import Request\nfrom starlette.responses import JSONResponse\nfrom traffik import HTTPThrottle\n\nthrottle = HTTPThrottle(uid=\"api:items\", rate=\"100/min\")\n\nasync def list_items(request: Request):\n    await throttle.hit(request)  # raises ConnectionThrottled if over limit\n    return JSONResponse({\"items\": []})\n</code></pre> <p>FastAPI's <code>Depends</code> is simply a wrapper around this same call, resolving the <code>Request</code> and calling the throttle for you. In plain Starlette, you manage the call yourself.</p>"},{"location":"integration/dependencies/#basic-usage-no-request-access-needed","title":"Basic usage \u2014 no Request access needed","text":"<p>The simplest setup: pass the throttle as a list dependency on the route decorator. Your handler function doesn't need to declare any extra parameter.</p> <pre><code>from fastapi import FastAPI, Depends\nfrom traffik import HTTPThrottle\nfrom traffik.backends.inmemory import InMemoryBackend\n\nbackend = InMemoryBackend()\napp = FastAPI(lifespan=backend.lifespan)\n\nthrottle = HTTPThrottle(uid=\"api:items\", rate=\"100/min\")\n\n@app.get(\"/items\", dependencies=[Depends(throttle)])\nasync def list_items():\n    return {\"items\": []}\n</code></pre> <p>FastAPI resolves the throttle before executing the handler. If the client exceeds the rate limit, the request is rejected with a <code>429 Too Many Requests</code> response before your handler is ever called.</p>"},{"location":"integration/dependencies/#with-request-access","title":"With <code>Request</code> access","text":"<p>If your handler also needs the request object, for example to read headers or query parameters, declare the throttle as a named parameter. FastAPI injects both the <code>Request</code> and the throttle result.</p> <pre><code>from fastapi import FastAPI, Depends, Request\nfrom traffik import HTTPThrottle\nfrom traffik.backends.inmemory import InMemoryBackend\n\nbackend = InMemoryBackend()\napp = FastAPI(lifespan=backend.lifespan)\n\nthrottle = HTTPThrottle(uid=\"api:search\", rate=\"30/min\")\n\n@app.get(\"/search\")\nasync def search(\n    q: str,\n    request: Request = Depends(throttle),\n):\n    client_ip = request.client.host\n    return {\"query\": q, \"from\": client_ip}\n</code></pre> <p>Note</p> <p><code>Depends(throttle)</code> returns the <code>Request</code> (or <code>WebSocket</code>) object after it has been checked. Naming the parameter <code>request</code> and annotating it with <code>Request = Depends(throttle)</code> gives you the familiar request object without adding any extra function calls.</p>"},{"location":"integration/dependencies/#multiple-throttles-on-one-route","title":"Multiple throttles on one route","text":"<p>Apply several throttles to the same route by adding them to the <code>dependencies</code> list. Each throttle is checked in the order it appears. The first one to reject the request stops processing.</p> <pre><code>from fastapi import FastAPI, Depends\nfrom traffik import HTTPThrottle\nfrom traffik.backends.inmemory import InMemoryBackend\n\nbackend = InMemoryBackend()\napp = FastAPI(lifespan=backend.lifespan)\n\nburst_throttle = HTTPThrottle(uid=\"api:burst\", rate=\"20/s\")\nsustained_throttle = HTTPThrottle(uid=\"api:sustained\", rate=\"500/min\")\n\n@app.post(\n    \"/ingest\",\n    dependencies=[\n        Depends(burst_throttle),\n        Depends(sustained_throttle),\n    ],\n)\nasync def ingest_data(payload: dict):\n    return {\"accepted\": True}\n</code></pre> <p>A common pattern is to combine a tight burst throttle with a looser sustained throttle. The burst throttle prevents sudden spikes; the sustained throttle caps total volume over a longer window.</p>"},{"location":"integration/dependencies/#router-level-dependencies","title":"Router-level dependencies","text":"<p>Add a throttle to an <code>APIRouter</code> and it applies automatically to every route registered on that router. You don't need to repeat <code>Depends(throttle)</code> on each individual route.</p> <pre><code>from fastapi import FastAPI, APIRouter, Depends\nfrom traffik import HTTPThrottle\nfrom traffik.backends.inmemory import InMemoryBackend\n\nbackend = InMemoryBackend()\napp = FastAPI(lifespan=backend.lifespan)\n\napi_throttle = HTTPThrottle(uid=\"api:global\", rate=\"1000/min\")\n\napi_router = APIRouter(\n    prefix=\"/api/v1\",\n    dependencies=[Depends(api_throttle)],\n)\n\n@api_router.get(\"/users\")\nasync def list_users():\n    return {\"users\": []}\n\n@api_router.get(\"/products\")\nasync def list_products():\n    return {\"products\": []}\n\napp.include_router(api_router)\n</code></pre>"},{"location":"integration/dependencies/#layering-router-and-route-throttles","title":"Layering router and route throttles","text":"<p>Router-level and route-level dependencies stack. A request to <code>/api/v1/admin/report</code> in the example below hits both <code>api_throttle</code> (from the router) and <code>report_throttle</code> (from the route).</p> <pre><code>from fastapi import FastAPI, APIRouter, Depends\nfrom traffik import HTTPThrottle\nfrom traffik.backends.inmemory import InMemoryBackend\n\nbackend = InMemoryBackend()\napp = FastAPI(lifespan=backend.lifespan)\n\napi_throttle = HTTPThrottle(uid=\"api:v1\", rate=\"1000/min\")\nreport_throttle = HTTPThrottle(uid=\"api:reports\", rate=\"10/min\")\n\napi_router = APIRouter(\n    prefix=\"/api/v1\",\n    dependencies=[Depends(api_throttle)],\n)\nadmin_router = APIRouter(prefix=\"/admin\")\n\n@admin_router.get(\n    \"/report\",\n    dependencies=[Depends(report_throttle)],\n)\nasync def generate_report():\n    return {\"status\": \"generating\"}\n\napi_router.include_router(admin_router)\napp.include_router(api_router)\n</code></pre> <p>Router-level throttles are the cleanest way to enforce limits on entire API sections</p> <p>Define one throttle per router boundary (e.g., public vs. authenticated vs. admin) rather than repeating dependencies on dozens of individual routes.</p>"},{"location":"integration/dependencies/#websocket-routes","title":"WebSocket routes","text":"<p>The same pattern works for WebSocket routes. Use <code>WebSocketThrottle</code> and declare the dependency the same way.</p> <pre><code>from fastapi import FastAPI, Depends, WebSocket\nfrom traffik.throttles import WebSocketThrottle\nfrom traffik.backends.inmemory import InMemoryBackend\n\nbackend = InMemoryBackend()\napp = FastAPI(lifespan=backend.lifespan)\n\nws_throttle = WebSocketThrottle(uid=\"ws:chat\", rate=\"60/min\")\n\n@app.websocket(\"/ws/chat\")\nasync def chat(websocket: WebSocket = Depends(ws_throttle)):\n    await websocket.accept()\n    while True:\n        data = await websocket.receive_text()\n        await websocket.send_text(f\"Echo: {data}\")\n</code></pre> <p>Note</p> <p>For WebSocket throttles that apply per-message (rather than per-connection), use direct usage inside the message loop instead.</p>"},{"location":"integration/direct-usage/","title":"Direct Usage","text":"<p>Sometimes you need the throttle to behave differently based on runtime information, such as the size of an uploaded file, the number of records in a query, or whether a previous step succeeded. Direct usage lets you call the throttle methods yourself from inside a handler, giving you full programmatic control.</p>"},{"location":"integration/direct-usage/#when-to-use-direct-usage","title":"When to use direct usage","text":"<ul> <li>Dynamic cost \u2014 the cost of a request depends on something you compute at runtime (e.g., payload size, number of items).</li> <li>Multi-step workflows \u2014 you want to consume quota only after a successful operation, or conditionally skip throttling based on an intermediate result.</li> <li>Conditional throttling \u2014 different branches of your handler have different rate limit semantics.</li> <li>Per-message WebSocket throttling \u2014 you want to throttle each message inside an established WebSocket connection, not just the connection attempt.</li> </ul> <p>For simpler cases, dependency injection or decorators are usually less code.</p>"},{"location":"integration/direct-usage/#await-throttlehitconnection-check-and-apply","title":"<code>await throttle.hit(connection)</code>: check and apply","text":"<p><code>hit()</code> is the core method. It records a hit, checks the quota, and rejects the request if the limit is exceeded. Returns the connection object.</p> <pre><code>from fastapi import FastAPI, Request\nfrom traffik import HTTPThrottle\nfrom traffik.backends.inmemory import InMemoryBackend\n\nbackend = InMemoryBackend()\napp = FastAPI(lifespan=backend.lifespan)\n\nthrottle = HTTPThrottle(uid=\"api:data\", rate=\"100/min\")\n\n@app.get(\"/data\")\nasync def get_data(request: Request):\n    await throttle.hit(request)\n    return {\"data\": []}\n</code></pre> <p>If the client is throttled, <code>hit()</code> raises an exception by default (handled by Traffik's default throttled handler, which returns a <code>429</code> response), or uses your custom throttled handler if provided.</p>"},{"location":"integration/direct-usage/#await-throttlehitconnection-costn-custom-cost-per-call","title":"<code>await throttle.hit(connection, cost=N)</code>: custom cost per call","text":"<p>Pass a <code>cost</code> argument to override the throttle's default cost for this specific call. Useful when different invocations of the same endpoint should consume different amounts of quota.</p> <pre><code>from fastapi import FastAPI, Request\nfrom traffik import HTTPThrottle\nfrom traffik.backends.inmemory import InMemoryBackend\n\nbackend = InMemoryBackend()\napp = FastAPI(lifespan=backend.lifespan)\n\n# Rate limit: 1000 \"units\" per minute\nupload_throttle = HTTPThrottle(uid=\"uploads\", rate=\"1000/min\")\n\n@app.post(\"/upload\")\nasync def upload_file(request: Request):\n    body = await request.body()\n    # Charge 1 unit per KB, minimum 1\n    cost = max(1, len(body) // 1024)\n\n    await upload_throttle.hit(request, cost=cost)\n\n    # Process upload...\n    return {\"bytes_received\": len(body)}\n</code></pre> <p>Tip</p> <p>A cost of <code>0</code> is a no-op. Traffik short-circuits immediately without recording anything or checking the backend. You can use this to conditionally skip throttling without an <code>if</code> branch:</p> <pre><code>cost = 0 if request.headers.get(\"X-Internal\") == \"true\" else 1\nawait throttle.hit(request, cost=cost)\n</code></pre>"},{"location":"integration/direct-usage/#await-throttleconnection-context-__call__-alias","title":"<code>await throttle(connection, context={...})</code>: <code>__call__</code> alias","text":"<p>Calling the throttle instance directly is an alias for <code>hit()</code>. The <code>context</code> keyword argument lets you pass extra information that throttle strategies, identifiers, or rules can read.</p> <pre><code>from fastapi import FastAPI, Request\nfrom traffik import HTTPThrottle\nfrom traffik.backends.inmemory import InMemoryBackend\n\nbackend = InMemoryBackend()\napp = FastAPI(lifespan=backend.lifespan)\n\nthrottle = HTTPThrottle(uid=\"api:scoped\", rate=\"200/min\")\n\n@app.get(\"/resource/{tenant_id}\")\nasync def get_resource(tenant_id: str, request: Request):\n    # Pass tenant context so different tenants have separate quota buckets\n    await throttle(request, context={\"scope\": tenant_id})\n    return {\"tenant\": tenant_id}\n</code></pre> <p>The <code>context</code> dict is merged with the throttle's default context (set at construction time). Call-time values take precedence over defaults in case of key conflicts.</p>"},{"location":"integration/direct-usage/#await-throttlecheckconnection-costn-non-consuming-pre-check","title":"<code>await throttle.check(connection, cost=N)</code>: non-consuming pre-check","text":"<p><code>check()</code> inspects the current quota state without consuming any quota. It returns <code>True</code> if there is sufficient quota available for the given cost, <code>False</code> otherwise.</p> <p>Best-effort only</p> <p><code>check()</code> is inherently subject to race conditions (Time-of-Check to Time-of-Use). Between the check and the eventual <code>hit()</code>, another request may consume the remaining quota. Use <code>check()</code> only for fast pre-screening, always follow up with <code>hit()</code> to actually consume quota.</p> <pre><code>from fastapi import FastAPI, Request\nfrom starlette.exceptions import HTTPException\nfrom traffik import HTTPThrottle\nfrom traffik.backends.inmemory import InMemoryBackend\n\nbackend = InMemoryBackend()\napp = FastAPI(lifespan=backend.lifespan)\n\nexport_throttle = HTTPThrottle(uid=\"exports\", rate=\"1000/min\")\n\n@app.post(\"/export\")\nasync def export_data(request: Request):\n    body = await request.body()\n    cost = max(1, len(body) // 512)\n\n    # Pre-check before running the expensive export\n    if not await export_throttle.check(request, cost=cost):\n        raise HTTPException(\n            status_code=429,\n            detail=f\"Rate limit would be exceeded. This request costs {cost} units.\",\n        )\n\n    # Quota looks sufficient \u2014 proceed with the expensive operation\n    result = await run_export(body)\n\n    # Consume quota now that the operation completed\n    await export_throttle.hit(request, cost=cost)\n\n    return result\n</code></pre> <p>When the throttle strategy doesn't support stats (e.g., a custom strategy without a <code>get_stat</code> method), <code>check()</code> returns <code>True</code> optimistically rather than blocking the request.</p>"},{"location":"integration/direct-usage/#await-throttlestatconnection-read-state-without-consuming","title":"<code>await throttle.stat(connection)</code>: read state without consuming","text":"<p><code>stat()</code> returns a <code>StrategyStat</code> object with the current throttle state for the connection. No quota is consumed. Returns <code>None</code> if the strategy doesn't support stats.</p> <p>The <code>StrategyStat</code> object contains:</p> Field Type Description <code>hits_remaining</code> <code>int</code> or <code>float</code> Quota remaining in the current window <code>wait_ms</code> <code>float</code> Milliseconds until quota resets (0 if quota is available) <code>rate</code> <code>Rate</code> The rate this stat is for <code>key</code> <code>str</code> The namespaced throttle key used for this connection <pre><code>from fastapi import FastAPI, Request\nfrom traffik import HTTPThrottle\nfrom traffik.backends.inmemory import InMemoryBackend\n\nbackend = InMemoryBackend()\napp = FastAPI(lifespan=backend.lifespan)\n\nthrottle = HTTPThrottle(uid=\"api:metered\", rate=\"100/min\")\n\n@app.get(\"/status\")\nasync def quota_status(request: Request):\n    stat = await throttle.stat(request)\n\n    if stat is None:\n        return {\"quota\": \"unknown\"}\n\n    return {\n        \"hits_remaining\": stat.hits_remaining,\n        \"retry_after_ms\": stat.wait_ms,\n    }\n</code></pre> <p>A common pattern is to read the stat before consuming quota to include rate limit information in the response:</p> <pre><code>@app.get(\"/data\")\nasync def get_data(request: Request):\n    stat_before = await throttle.stat(request)\n    await throttle.hit(request)\n\n    return {\n        \"data\": [],\n        \"quota_used\": 1,\n        \"quota_remaining\": (stat_before.hits_remaining - 1) if stat_before else None,\n    }\n</code></pre>"},{"location":"integration/direct-usage/#websocket-is_throttledwebsocket-check-state-after-hit","title":"WebSocket: <code>is_throttled(websocket)</code> \u2014 check state after hit","text":"<p>For WebSocket connections, <code>hit()</code> doesn't raise an exception on throttle by default. Instead it sends a <code>rate_limit</code> JSON message to the client and marks the connection state. Use <code>is_throttled()</code> to check whether the last hit was throttled.</p> <pre><code>from fastapi import FastAPI, WebSocket\nfrom traffik.throttles import WebSocketThrottle, is_throttled\nfrom traffik.backends.inmemory import InMemoryBackend\n\nbackend = InMemoryBackend()\napp = FastAPI(lifespan=backend.lifespan)\n\nws_throttle = WebSocketThrottle(uid=\"ws:messages\", rate=\"60/min\")\n\n@app.websocket(\"/ws\")\nasync def websocket_endpoint(websocket: WebSocket):\n    await websocket.accept()\n\n    while True:\n        message = await websocket.receive_text()\n\n        # Record a hit for each message received\n        await ws_throttle.hit(websocket)\n\n        if is_throttled(websocket):\n            # Client was notified by the throttle handler.\n            # Optionally close the connection after repeated violations.\n            await websocket.close(code=1008, reason=\"Rate limit exceeded\")\n            return\n\n        # Process the message normally\n        await websocket.send_text(f\"Echo: {message}\")\n</code></pre> <p>Note</p> <p><code>is_throttled()</code> reads a flag that <code>hit()</code> sets on <code>websocket.state</code>. It reflects the result of the most recent <code>hit()</code> call only. If you call <code>hit()</code> again, the flag is updated.</p>"},{"location":"integration/direct-usage/#putting-it-together-dynamic-cost-with-conditional-throttling","title":"Putting it together: dynamic cost with conditional throttling","text":"<p>This example shows a multi-step pattern: compute cost from the request body, pre-check availability, then consume quota only on success.</p> <pre><code>from fastapi import FastAPI, Request\nfrom starlette.exceptions import HTTPException\nfrom traffik import HTTPThrottle\nfrom traffik.backends.inmemory import InMemoryBackend\n\nbackend = InMemoryBackend()\napp = FastAPI(lifespan=backend.lifespan)\n\n# Each request costs based on number of items processed\n# Total budget: 10,000 items per minute\nbatch_throttle = HTTPThrottle(uid=\"batch:items\", rate=\"10000/min\")\n\n@app.post(\"/batch\")\nasync def process_batch(request: Request):\n    data = await request.json()\n    items = data.get(\"items\", [])\n\n    if not items:\n        return {\"processed\": 0}\n\n    cost = len(items)\n\n    # Non-consuming check first\n    if not await batch_throttle.check(request, cost=cost):\n        stat = await batch_throttle.stat(request)\n        retry_after = int((stat.wait_ms / 1000) + 1) if stat else 60\n        raise HTTPException(\n            status_code=429,\n            detail=f\"Batch of {cost} items would exceed quota.\",\n            headers={\"Retry-After\": str(retry_after)},\n        )\n\n    # Run the actual processing\n    results = await process_items(items)\n\n    # Consume quota \u2014 actual cost may differ if some items failed\n    actual_cost = len(results)\n    await batch_throttle.hit(request, cost=actual_cost)\n\n    return {\"processed\": actual_cost, \"results\": results}\n</code></pre>"},{"location":"integration/middleware/","title":"Middleware","text":"<p><code>ThrottleMiddleware</code> sits in the ASGI stack and intercepts every request before it reaches any route handler. Use it when you want to enforce rate limits globally or based on path/method patterns, without touching individual route definitions.</p> <p>Works with any Starlette application</p> <p><code>ThrottleMiddleware</code> is a standard Starlette middleware and works with any Starlette-based application, not just FastAPI. If you're using plain Starlette or any other ASGI framework built on Starlette, you can add <code>ThrottleMiddleware</code> the same way: <code>app.add_middleware(ThrottleMiddleware, middleware_throttles=[...])</code>.</p> <p>Use middleware for cross-cutting concerns</p> <p>Middleware is the right tool when the throttle logic is independent of which specific route is hit, for example, a global IP-based limit, or a limit on all <code>/api/</code> traffic regardless of endpoint. For per-route or per-handler limits, prefer dependency injection.</p>"},{"location":"integration/middleware/#basic-setup","title":"Basic setup","text":"<p>Wrap your underlying throttle in <code>MiddlewareThrottle</code> and pass it to <code>ThrottleMiddleware</code>. The backend is typically shared via the <code>lifespan</code> context.</p> <pre><code>from fastapi import FastAPI\nfrom traffik import HTTPThrottle\nfrom traffik.backends.inmemory import InMemoryBackend\nfrom traffik.middleware import MiddlewareThrottle, ThrottleMiddleware\n\nbackend = InMemoryBackend()\napp = FastAPI(lifespan=backend.lifespan)\n\nthrottle = HTTPThrottle(uid=\"global\", rate=\"1000/min\")\n\napp.add_middleware(\n    ThrottleMiddleware,\n    middleware_throttles=[\n        MiddlewareThrottle(throttle),\n    ],\n)\n</code></pre> <p>This applies the throttle to every HTTP request. <code>MiddlewareThrottle</code> without any <code>path</code>, <code>methods</code>, or <code>predicate</code> arguments matches all connections.</p>"},{"location":"integration/middleware/#http-and-websocket-throttles-in-middleware","title":"HTTP and WebSocket throttles in middleware","text":"<p><code>ThrottleMiddleware</code> handles both HTTP and WebSocket connections. Pass throttles for each connection type in the same <code>middleware_throttles</code> list. Traffik routes them automatically based on the connection's ASGI scope type.</p> <pre><code>from fastapi import FastAPI\nfrom traffik import HTTPThrottle\nfrom traffik.backends.inmemory import InMemoryBackend\nfrom traffik.middleware import MiddlewareThrottle, ThrottleMiddleware\nfrom traffik.throttles import WebSocketThrottle\n\nbackend = InMemoryBackend()\napp = FastAPI(lifespan=backend.lifespan)\n\nhttp_throttle = HTTPThrottle(uid=\"http:global\", rate=\"500/min\")\nws_throttle = WebSocketThrottle(uid=\"ws:global\", rate=\"60/min\")\n\napp.add_middleware(\n    ThrottleMiddleware,\n    middleware_throttles=[\n        MiddlewareThrottle(http_throttle),\n        MiddlewareThrottle(ws_throttle, path=\"/ws/\"),\n    ],\n)\n</code></pre> <p>HTTP throttles are never applied to WebSocket connections, and vice versa. Traffik categorizes throttles by connection type at startup, so there is no per-request branching overhead.</p>"},{"location":"integration/middleware/#path-filtering","title":"Path filtering","text":"<p>The <code>path</code> argument on <code>MiddlewareThrottle</code> limits the throttle to requests whose URL path matches. Strings are interpreted as prefix matches (compiled to a regex internally). You can also pass a pre-compiled <code>re.Pattern</code>.</p> String prefixCompiled regex <p>A plain string matches any path that starts with that prefix.</p> <pre><code>from traffik import HTTPThrottle\nfrom traffik.middleware import MiddlewareThrottle\n\nthrottle = HTTPThrottle(uid=\"api\", rate=\"300/min\")\n\n# Matches /api/, /api/users, /api/v2/products, etc.\napi_middleware_throttle = MiddlewareThrottle(throttle, path=\"/api/\")\n</code></pre> <p>Pass a <code>re.Pattern</code> for full regex control.</p> <pre><code>import re\nfrom traffik import HTTPThrottle\nfrom traffik.middleware import MiddlewareThrottle\n\nthrottle = HTTPThrottle(uid=\"api:versioned\", rate=\"300/min\")\n\n# Matches /api/v1/..., /api/v2/..., but not /api/legacy/...\npattern = re.compile(r\"^/api/v\\d+/\")\nversioned_middleware_throttle = MiddlewareThrottle(throttle, path=pattern)\n</code></pre> <p>Note</p> <p>When <code>path</code> is a string, it is compiled as a regex pattern. If you use a bare string like <code>\"/api/\"</code>, it matches any path that contains <code>/api/</code> at that position (anchored from the start). Use <code>re.compile(...)</code> explicitly when you need full regex semantics such as anchors or alternation.</p> <p>Wildcard patterns are supported</p> <p>Because <code>MiddlewareThrottle</code>'s <code>path</code> parameter uses <code>ThrottleRule</code> internally, it supports the same wildcard syntax: <code>*</code> matches a single path segment (no <code>/</code>), and <code>**</code> matches any number of segments including <code>/</code>. For example, <code>path=\"/api/*/users\"</code> matches <code>/api/v1/users</code> and <code>/api/v2/users</code> but not <code>/api/v1/admin/users</code>. See Throttle Rules &amp; Wildcards for the full pattern reference.</p>"},{"location":"integration/middleware/#method-filtering","title":"Method filtering","text":"<p>The <code>methods</code> argument restricts the throttle to specific HTTP verbs. WebSocket connections ignore method filtering entirely (they have no HTTP method after the handshake).</p> <pre><code>from traffik import HTTPThrottle\nfrom traffik.middleware import MiddlewareThrottle\n\nwrite_throttle = HTTPThrottle(uid=\"writes\", rate=\"100/min\")\n\n# Only applies to POST, PUT, PATCH, and DELETE requests\nwrite_middleware_throttle = MiddlewareThrottle(\n    write_throttle,\n    methods={\"POST\", \"PUT\", \"PATCH\", \"DELETE\"},\n)\n</code></pre> <p>GET requests to any path pass through this throttle without consuming quota.</p>"},{"location":"integration/middleware/#predicate-filtering","title":"Predicate filtering","text":"<p>For conditions that can't be expressed with path or method alone, provide an async <code>predicate</code> callable. The throttle only applies when the predicate returns <code>True</code>.</p> <pre><code>import typing\nfrom starlette.requests import HTTPConnection\nfrom traffik import HTTPThrottle\nfrom traffik.middleware import MiddlewareThrottle\n\nthrottle = HTTPThrottle(uid=\"premium:api\", rate=\"2000/min\")\n\nasync def is_premium_user(\n    connection: HTTPConnection,\n    context: typing.Optional[typing.Mapping[str, typing.Any]] = None,\n) -&gt; bool:\n    return connection.headers.get(\"X-User-Tier\") == \"premium\"\n\npremium_middleware_throttle = MiddlewareThrottle(\n    throttle,\n    path=\"/api/\",\n    predicate=is_premium_user,\n)\n</code></pre> <p>Keep predicates fast</p> <p>The predicate runs on every matching request before the throttle strategy executes. Avoid blocking I/O or expensive computation. If you need database lookups, consider caching the result in <code>request.state</code> from an earlier middleware.</p>"},{"location":"integration/middleware/#combined-path-methods-predicate","title":"Combined: path + methods + predicate","text":"<p>All three filters are evaluated conjunctively: the throttle only fires when all conditions are met.</p> <pre><code>import typing\nfrom starlette.requests import HTTPConnection\nfrom traffik import HTTPThrottle\nfrom traffik.backends.inmemory import InMemoryBackend\nfrom traffik.middleware import MiddlewareThrottle, ThrottleMiddleware\nfrom fastapi import FastAPI\n\nbackend = InMemoryBackend()\napp = FastAPI(lifespan=backend.lifespan)\n\nupload_throttle = HTTPThrottle(uid=\"uploads:authenticated\", rate=\"50/hour\")\n\nasync def is_authenticated(\n    connection: HTTPConnection,\n    context: typing.Optional[typing.Mapping[str, typing.Any]] = None,\n) -&gt; bool:\n    return \"Authorization\" in connection.headers\n\napp.add_middleware(\n    ThrottleMiddleware,\n    middleware_throttles=[\n        MiddlewareThrottle(\n            upload_throttle,\n            path=\"/api/uploads/\",\n            methods={\"POST\"},\n            predicate=is_authenticated,\n        )\n    ],\n)\n</code></pre>"},{"location":"integration/middleware/#execution-order-within-a-throttle","title":"Execution order within a throttle","text":"<p>For each <code>MiddlewareThrottle</code>, filters are evaluated from cheapest to most expensive:</p> <ol> <li>Methods \u2014 a set membership check, essentially free.</li> <li>Path \u2014 a compiled regex match against the URL string.</li> <li>Predicate \u2014 your async callable, potentially involving I/O.</li> </ol> <p>If any check fails, the remaining checks are skipped and the throttle is bypassed for that request. The throttle strategy (quota consumption) only runs after all filters pass.</p>"},{"location":"integration/middleware/#throttle-sorting","title":"Throttle sorting","text":"<p>When <code>ThrottleMiddleware</code> receives a list of <code>MiddlewareThrottle</code> instances, it sorts them before processing. Sorting controls which throttles run first, which is most useful when you want cheap (low-cost) throttles to reject requests before expensive ones do unnecessary work.</p> <p>The <code>sort</code> parameter accepts:</p> Value Behavior <code>\"cheap_first\"</code> (default) Throttles with lower <code>cost</code> run first <code>\"cheap_last\"</code> Throttles with higher <code>cost</code> run first <code>False</code> or <code>None</code> No sorting; use the order you provided A callable Sorted by your custom key function <pre><code>from fastapi import FastAPI\nfrom traffik import HTTPThrottle\nfrom traffik.backends.inmemory import InMemoryBackend\nfrom traffik.middleware import MiddlewareThrottle, ThrottleMiddleware\n\nbackend = InMemoryBackend()\napp = FastAPI(lifespan=backend.lifespan)\n\ncheap_throttle = HTTPThrottle(uid=\"cheap\", rate=\"1000/min\")\nexpensive_throttle = HTTPThrottle(uid=\"expensive\", rate=\"100/min\")\n\napp.add_middleware(\n    ThrottleMiddleware,\n    middleware_throttles=[\n        MiddlewareThrottle(expensive_throttle, cost=10),\n        MiddlewareThrottle(cheap_throttle, cost=1),\n    ],\n    sort=\"cheap_first\",  # cheap_throttle runs first (default behavior)\n)\n</code></pre> <p>Indeteminable thottle cost is treated as infinite</p> <p>A <code>MiddlewareThrottle</code> with no explicit <code>cost</code> (i.e., <code>cost=None</code>), and the wrapped <code>Throttle</code> uses a dynamic cost function, is treated as having infinite cost and sorted last under <code>\"cheap_first\"</code>. This ensures unconstrained throttles don't block cheap ones from short-circuiting early.</p>"},{"location":"integration/middleware/#custom-sort-key","title":"Custom sort key","text":"<p>Provide a callable that takes a <code>MiddlewareThrottle</code> (or <code>Throttle</code>) and returns any sortable value:</p> <pre><code>app.add_middleware(\n    ThrottleMiddleware,\n    middleware_throttles=[...],\n    sort=lambda t: t.throttle.uid,  # sort alphabetically by throttle UID\n)\n</code></pre>"},{"location":"integration/middleware/#using-middlewarethrottle-as-a-fastapi-dependency","title":"Using <code>MiddlewareThrottle</code> as a FastAPI dependency","text":"<p><code>MiddlewareThrottle</code> implements <code>__call__</code> and exposes a <code>__signature__</code> compatible with FastAPI's dependency injection. This is a niche use case, for example when you have a <code>MiddlewareThrottle</code> instance configured with path and method filters, and you also want to use the same filtered throttle as a route-level dependency.</p> <pre><code>from fastapi import FastAPI, Depends\nfrom traffik import HTTPThrottle\nfrom traffik.backends.inmemory import InMemoryBackend\nfrom traffik.middleware import MiddlewareThrottle\n\nbackend = InMemoryBackend()\napp = FastAPI(lifespan=backend.lifespan)\n\nthrottle = HTTPThrottle(uid=\"api:write\", rate=\"50/min\")\nwrite_middleware_throttle = MiddlewareThrottle(\n    throttle,\n    methods={\"POST\", \"PUT\", \"DELETE\"},\n)\n\n# Used as a dependency \u2014 the path/method filters still apply\n@app.post(\"/resource\", dependencies=[Depends(write_middleware_throttle)])\nasync def create_resource(payload: dict):\n    return {\"created\": True}\n</code></pre> <p>Note</p> <p>When used as a dependency, <code>MiddlewareThrottle</code> applies its path and method filters using the actual request. This means you get the same filtered behavior as in middleware, but scoped to a specific route. This is rarely needed, prefer plain <code>Depends(throttle)</code> for route-level throttling.</p>"},{"location":"integration/middleware/#complete-middleware-example","title":"Complete middleware example","text":"<pre><code>import re\nimport typing\nfrom fastapi import FastAPI\nfrom starlette.requests import HTTPConnection\nfrom traffik import HTTPThrottle\nfrom traffik.backends.inmemory import InMemoryBackend\nfrom traffik.middleware import MiddlewareThrottle, ThrottleMiddleware\nfrom traffik.throttles import WebSocketThrottle\n\nbackend = InMemoryBackend()\napp = FastAPI(lifespan=backend.lifespan)\n\n# Global IP-based limit for all traffic\nglobal_throttle = HTTPThrottle(uid=\"global\", rate=\"2000/min\")\n\n# Stricter limit for write operations on the API\nwrite_throttle = HTTPThrottle(uid=\"api:writes\", rate=\"200/min\")\n\n# Per-connection limit for WebSocket\nws_throttle = WebSocketThrottle(uid=\"ws:connections\", rate=\"30/min\")\n\nasync def is_write_method(\n    connection: HTTPConnection,\n    context: typing.Optional[typing.Mapping[str, typing.Any]] = None,\n) -&gt; bool:\n    return connection.scope.get(\"method\", \"\") in {\"POST\", \"PUT\", \"PATCH\", \"DELETE\"}\n\napp.add_middleware(\n    ThrottleMiddleware,\n    middleware_throttles=[\n        MiddlewareThrottle(global_throttle, cost=1),\n        MiddlewareThrottle(\n            write_throttle,\n            path=re.compile(r\"^/api/\"),\n            predicate=is_write_method,\n            cost=5,\n        ),\n        MiddlewareThrottle(ws_throttle, path=\"/ws/\"),\n    ],\n    sort=\"cheap_first\",\n)\n\n@app.get(\"/api/data\")\nasync def get_data():\n    return {\"data\": []}\n\n@app.post(\"/api/data\")\nasync def create_data(payload: dict):\n    return {\"created\": True}\n</code></pre>"}]}